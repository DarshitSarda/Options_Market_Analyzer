{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3781836b-c75d-42f0-9aba-d1c015b379c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NSE OPTIONS ANALYZER - PCR & MAX PAIN CALCULATOR\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Output directory: C:\\Users\\sarda\\Desktop\\nse_options_analysis\n",
      "\n",
      "STEP 1: Determining target expiries from RELIANCE...\n",
      "âœ“ Using RELIANCE expiries: ['25-Nov-2025', '30-Dec-2025', '27-Jan-2026']\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Processing Equities (PCR + Max Pain)...\n",
      "======================================================================\n",
      "Note: Symbols without options data will be skipped silently\n",
      "\n",
      "  Progress: 215/215 | âœ“ Processed: 205 | âŠ˜ Skipped: 10\n",
      "\n",
      "âœ“ Completed: 205 symbols with options data\n",
      "âŠ˜ Skipped: 10 symbols (no options data or errors)\n",
      "\n",
      "Generating master equity summary files...\n",
      "  âœ“ Equities_Analysis_25-Nov-2025.csv (205 symbols)\n",
      "  âœ“ Equities_Analysis_30-Dec-2025.csv (204 symbols)\n",
      "  âœ“ Equities_Analysis_27-Jan-2026.csv (197 symbols)\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Processing Indices (PCR + Max Pain)...\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Processing NIFTY...\n",
      "  âœ“ NIFTY summary saved (18 expiries)\n",
      "ğŸ“Š Processing BANKNIFTY...\n",
      "  âœ“ BANKNIFTY summary saved (6 expiries)\n",
      "\n",
      "======================================================================\n",
      "âœ… ANALYSIS COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ All results saved to: C:\\Users\\sarda\\Desktop\\nse_options_analysis\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "  â€¢ Equities processed: 205/215\n",
      "  â€¢ Equities skipped: 10 (no options data)\n",
      "  â€¢ Indices processed: 2/2\n",
      "  â€¢ Master equity files: 3 expiries\n",
      "  â€¢ Target expiries: 25-Nov-2025, 30-Dec-2025, 27-Jan-2026\n",
      "\n",
      "ğŸ“‹ Generated Files:\n",
      "  â”œâ”€ Equities_Analysis_<expiry>.csv (master PCR + Max Pain)\n",
      "  â”œâ”€ <SYMBOL>/ (individual equity folders)\n",
      "  â”‚   â””â”€ <SYMBOL>_<expiry>.csv (option chain data)\n",
      "  â””â”€ index_options/\n",
      "      â”œâ”€ NIFTY/\n",
      "      â”‚   â”œâ”€ NIFTY_<expiry>.csv (all expiries)\n",
      "      â”‚   â””â”€ NIFTY_Analysis_Summary.csv\n",
      "      â””â”€ BANKNIFTY/\n",
      "          â”œâ”€ BANKNIFTY_<expiry>.csv (all expiries)\n",
      "          â””â”€ BANKNIFTY_Analysis_Summary.csv\n",
      "\n",
      "ğŸ’¡ Each file contains:\n",
      "  âœ“ Option chain data (Strike, OI, LTP, IV, Bid/Ask)\n",
      "  âœ“ PCR (Put-Call Ratio) = Put OI / Call OI\n",
      "  âœ“ Max Pain Strike (minimum loss for option writers)\n",
      "\n",
      "======================================================================\n",
      "ğŸ“„ Processing log saved: processing_log.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "NSE Options Analyzer - Combined PCR & Max Pain Calculator\n",
    "\n",
    "Features:\n",
    "1. Scrapes NSE option chain data for equities and indices\n",
    "2. Calculates PCR (Put-Call Ratio) for all symbols\n",
    "3. Calculates Max Pain for each option chain\n",
    "4. Generates comprehensive summary reports\n",
    "5. Robust error handling for symbols without options data\n",
    "\n",
    "Output: Desktop/nse_options_analysis/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "# Output directory (new folder name)\n",
    "BASE_DIR = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"nse_options_analysis\")\n",
    "\n",
    "# NSE URLs\n",
    "WARMUP_URL = \"https://www.nseindia.com/option-chain\"\n",
    "EQUITY_OC_URL = \"https://www.nseindia.com/api/option-chain-equities?symbol={symbol}\"\n",
    "INDEX_OC_URL = \"https://www.nseindia.com/api/option-chain-indices?symbol={symbol}\"\n",
    "INDEX_OC_URL_ALT = \"https://www.nseindia.com/api/option-chain-indices/?symbol={symbol}\"\n",
    "\n",
    "# Full equities ticker list\n",
    "RAW_TICKERS = [\n",
    "    'NSE:360ONE', 'NSE:ABB', 'NSE:ADANIENSOL', 'NSE:ADANIENT', 'NSE:ADANIGREEN', 'NSE:ADANIPORTS',\n",
    "    'NSE:ATGL', 'NSE:ABCAPITAL', 'NSE:ABFRL', 'NSE:ALKEM', 'NSE:AMBER', 'NSE:AMBUJACEM',\n",
    "    'NSE:ANGELONE', 'NSE:APLAPOLLO', 'NSE:APOLLOHOSP', 'NSE:ASHOKLEY', 'NSE:ASIANPAINT',\n",
    "    'NSE:ASTRAL', 'NSE:AUBANK', 'NSE:AUROPHARMA', 'NSE:DMART', 'NSE:AXISBANK', 'NSE:BAJAJ-AUTO',\n",
    "    'NSE:BAJFINANCE', 'NSE:BAJAJFINSV', 'NSE:BANDHANBNK', 'NSE:BANKBARODA', 'NSE:BANKINDIA',\n",
    "    'NSE:BDL', 'NSE:BEL', 'NSE:BHARATFORG', 'NSE:BHEL', 'NSE:BPCL', 'NSE:BHARTIARTL',\n",
    "    'NSE:BIOCON', 'NSE:BLUESTARCO', 'NSE:BOSCHLTD', 'NSE:BRITANNIA', 'NSE:BSE', 'NSE:CANBK',\n",
    "    'NSE:CDSL', 'NSE:CESC', 'NSE:CGPOWER', 'NSE:CHOLAFIN', 'NSE:CIPLA', 'NSE:COALINDIA',\n",
    "    'NSE:COFORGE', 'NSE:COLPAL', 'NSE:CAMS', 'NSE:CONCOR', 'NSE:CROMPTON', 'NSE:CUMMINSIND',\n",
    "    'NSE:CYIENT', 'NSE:DABUR', 'NSE:DALBHARAT', 'NSE:DELHIVERY', 'NSE:DIVISLAB', 'NSE:DIXON',\n",
    "    'NSE:DLF', 'NSE:DRREDDY', 'NSE:EICHERMOT', 'NSE:ETERNAL', 'NSE:EXIDEIND', 'NSE:FORTIS',\n",
    "    'NSE:NYKAA', 'NSE:GAIL', 'NSE:GLENMARK', 'NSE:GMRAIRPORT', 'NSE:GODREJCP', 'NSE:GODREJPROP',\n",
    "    'NSE:GRANULES', 'NSE:GRASIM', 'NSE:HAVELLS', 'NSE:HCLTECH', 'NSE:HDFCAMC', 'NSE:HDFCBANK',\n",
    "    'NSE:HDFCLIFE', 'NSE:HEROMOTOCO', 'NSE:HFCL', 'NSE:HINDALCO', 'NSE:HAL', 'NSE:HINDPETRO',\n",
    "    'NSE:HINDUNILVR', 'NSE:HINDZINC', 'NSE:HUDCO', 'NSE:ICICIBANK', 'NSE:ICICIGI', 'NSE:ICICIPRULI',\n",
    "    'NSE:IDEA', 'NSE:IDFCFIRSTB', 'NSE:IIFL', 'NSE:INDIANB', 'NSE:IEX', 'NSE:IOC', 'NSE:IRCTC',\n",
    "    'NSE:IRFC', 'NSE:IREDA', 'NSE:IGL', 'NSE:INDUSTOWER', 'NSE:INDUSINDBK', 'NSE:NAUKRI',\n",
    "    'NSE:INFY', 'NSE:INOXWIND', 'NSE:INDIGO', 'NSE:IRB', 'NSE:ITC', 'NSE:JSL', 'NSE:JINDALSTEL',\n",
    "    'NSE:JIOFIN', 'NSE:JSWENERGY', 'NSE:JSWSTEEL', 'NSE:JUBLFOOD', 'NSE:KALYANKJIL', 'NSE:KAYNES',\n",
    "    'NSE:KEI', 'NSE:KFINTECH', 'NSE:KOTAKBANK', 'NSE:KPITTECH', 'NSE:LTF', 'NSE:LT', 'NSE:LAURUSLABS',\n",
    "    'NSE:LICHSGFIN', 'NSE:LICI', 'NSE:LTIM', 'NSE:LUPIN', 'NSE:LODHA', 'NSE:MANAPPURAM',\n",
    "    'NSE:MANKIND', 'NSE:MARICO', 'NSE:MARUTI', 'NSE:MFSL', 'NSE:MAXHEALTH', 'NSE:MAZDOCK',\n",
    "    'NSE:MOTHERSON', 'NSE:MPHASIS', 'NSE:MCX', 'NSE:MUTHOOTFIN', 'NSE:NATIONALUM', 'NSE:NBCC',\n",
    "    'NSE:NCC', 'NSE:NESTLEIND', 'NSE:NHPC', 'NSE:NMDC', 'NSE:NTPC', 'NSE:NUVAMA',\n",
    "    'NSE:OBEROIRLTY', 'NSE:ONGC', 'NSE:OIL', 'NSE:PAYTM', 'NSE:OFSS', 'NSE:PAGEIND', 'NSE:PATANJALI',\n",
    "    'NSE:POLICYBZR', 'NSE:PERSISTENT', 'NSE:PETRONET', 'NSE:PGEL', 'NSE:PIIND', 'NSE:PIDILITIND',\n",
    "    'NSE:PPLPHARMA', 'NSE:PNBHOUSING', 'NSE:POLYCAB', 'NSE:POONAWALLA', 'NSE:PFC', 'NSE:POWERGRID',\n",
    "    'NSE:PRESTIGE', 'NSE:PNB', 'NSE:RVNL', 'NSE:RBLBANK', 'NSE:RELIANCE', 'NSE:RECLTD', 'NSE:SBICARD',\n",
    "    'NSE:SBILIFE', 'NSE:SHREECEM', 'NSE:SHRIRAMFIN', 'NSE:SIEMENS', 'NSE:SJVN', 'NSE:SOLARINDS',\n",
    "    'NSE:SONACOMS', 'NSE:SRF', 'NSE:SBIN', 'NSE:SAIL', 'NSE:SUNPHARMA', 'NSE:SUPREMEIND', 'NSE:SUZLON',\n",
    "    'NSE:SYNGENE', 'NSE:TATACHEM', 'NSE:TCS', 'NSE:TATACONSUM', 'NSE:TATAELXSI', 'NSE:TATAMOTORS',\n",
    "    'NSE:TATAPOWER', 'NSE:TATASTEEL', 'NSE:TATATECH', 'NSE:TECHM', 'NSE:FEDERALBNK', 'NSE:INDHOTEL',\n",
    "    'NSE:PHOENIXLTD', 'NSE:TITAGARH', 'NSE:TITAN', 'NSE:TORNTPHARM', 'NSE:TORNTPOWER', 'NSE:TRENT',\n",
    "    'NSE:TIINDIA', 'NSE:TVSMOTOR', 'NSE:ULTRACEMCO', 'NSE:UNIONBANK', 'NSE:UNITDSPR', 'NSE:UNOMINDA',\n",
    "    'NSE:UPL', 'NSE:VBL', 'NSE:VEDL', 'NSE:VOLTAS', 'NSE:WIPRO', 'NSE:YESBANK', 'NSE:ZYDUSLIFE'\n",
    "]\n",
    "\n",
    "# Indices to process\n",
    "INDEX_LIST = [\"NIFTY\", \"BANKNIFTY\"]\n",
    "INDEX_SYMBOLS = set(INDEX_LIST)\n",
    "\n",
    "# Analysis settings\n",
    "ENABLE_PLOTS = False  # Set to True to generate Max Pain plots\n",
    "VERBOSE_ERRORS = False  # Set to True to see detailed error traces\n",
    "\n",
    "# ==========================\n",
    "# HTTP HELPERS\n",
    "# ==========================\n",
    "def new_session():\n",
    "    \"\"\"Create a new session with proper NSE headers\"\"\"\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/115.0.0.0 Safari/537.36\"),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://www.nseindia.com/option-chain\",\n",
    "        \"Host\": \"www.nseindia.com\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "    })\n",
    "    return s\n",
    "\n",
    "def warm_cookies(session, retries=5, sleep_min=1.0, sleep_max=2.0):\n",
    "    \"\"\"\n",
    "    Warm up session cookies by visiting NSE homepage.\n",
    "    This is CRITICAL - NSE requires valid cookies before API access.\n",
    "    \"\"\"\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            # Visit main page first\n",
    "            resp = session.get(WARMUP_URL, timeout=15, allow_redirects=True)\n",
    "            \n",
    "            if resp.status_code == 200:\n",
    "                # Also visit the derivatives page for better cookie setup\n",
    "                time.sleep(random.uniform(0.3, 0.8))\n",
    "                session.get(\"https://www.nseindia.com/get-quotes/derivatives\", timeout=15)\n",
    "                return True\n",
    "            elif resp.status_code in [301, 302, 307, 308]:\n",
    "                # Follow redirects manually if needed\n",
    "                if 'Location' in resp.headers:\n",
    "                    session.get(resp.headers['Location'], timeout=15)\n",
    "                    return True\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  â± Warmup attempt {i+1}/{retries}: Timeout, retrying...\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"  ğŸŒ Warmup attempt {i+1}/{retries}: Connection error, retrying...\")\n",
    "        except Exception as e:\n",
    "            if VERBOSE_ERRORS:\n",
    "                print(f\"  âš  Warmup attempt {i+1}/{retries}: {type(e).__name__}\")\n",
    "        \n",
    "        # Exponential backoff\n",
    "        wait_time = random.uniform(sleep_min, sleep_max) * (i + 1)\n",
    "        time.sleep(wait_time)\n",
    "    \n",
    "    return False\n",
    "\n",
    "def fetch_option_chain(session, symbol, is_index=False, retries=3, backoff=1.5):\n",
    "    \"\"\"\n",
    "    Fetch NSE option-chain JSON for the given symbol.\n",
    "    Returns the parsed JSON data or None if symbol has no options data.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    if is_index:\n",
    "        urls = [INDEX_OC_URL.format(symbol=symbol), INDEX_OC_URL_ALT.format(symbol=symbol)]\n",
    "    else:\n",
    "        urls = [EQUITY_OC_URL.format(symbol=symbol)]\n",
    "\n",
    "    for url in urls:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                resp = session.get(url, timeout=15)\n",
    "                \n",
    "                if resp.status_code == 200:\n",
    "                    try:\n",
    "                        data = resp.json()\n",
    "                        # Check if response has valid option data\n",
    "                        if not data.get(\"records\") or not data[\"records\"].get(\"data\"):\n",
    "                            # No options data available for this symbol\n",
    "                            return None\n",
    "                        return data\n",
    "                    except Exception:\n",
    "                        # Non-JSON response, likely blocked\n",
    "                        warm_cookies(session)\n",
    "                        time.sleep(backoff * (attempt + 1))\n",
    "                        continue\n",
    "                        \n",
    "                elif resp.status_code == 404:\n",
    "                    # Symbol doesn't have options data\n",
    "                    return None\n",
    "                    \n",
    "                elif resp.status_code in (429, 503, 403):\n",
    "                    # Rate-limited/blocked - backoff and retry\n",
    "                    time.sleep(backoff * (attempt + 1))\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    # Other HTTP errors\n",
    "                    time.sleep(backoff * (attempt + 1))\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "            except Exception as e:\n",
    "                if VERBOSE_ERRORS:\n",
    "                    print(f\"  Fetch attempt {attempt+1} failed for {symbol}: {e}\")\n",
    "                time.sleep(backoff * (attempt + 1))\n",
    "    \n",
    "    # All retries exhausted - symbol likely has no options or network issue\n",
    "    return None\n",
    "\n",
    "# ==========================\n",
    "# DATA PROCESSING HELPERS\n",
    "# ==========================\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def strip_prefix(t):\n",
    "    \"\"\"Remove 'NSE:' prefix from ticker\"\"\"\n",
    "    return t.split(\":\", 1)[1] if \":\" in t else t\n",
    "\n",
    "def parse_rows_for_expiry(data, expiry):\n",
    "    \"\"\"Extract option chain rows for a specific expiry date\"\"\"\n",
    "    rows = []\n",
    "    for rec in data.get(\"records\", {}).get(\"data\", []):\n",
    "        if rec.get(\"expiryDate\") == expiry:\n",
    "            strike = rec.get(\"strikePrice\")\n",
    "            ce = rec.get(\"CE\", {}) or {}\n",
    "            pe = rec.get(\"PE\", {}) or {}\n",
    "            rows.append({\n",
    "                \"Strike\": strike,\n",
    "                \"Expiry\": expiry,\n",
    "                \"Call_OI\": ce.get(\"openInterest\", 0),\n",
    "                \"Call_LTP\": ce.get(\"lastPrice\", None),\n",
    "                \"Put_OI\": pe.get(\"openInterest\", 0),\n",
    "                \"Put_LTP\": pe.get(\"lastPrice\", None),\n",
    "                \"CE_BidQty\": ce.get(\"bidQty\"),\n",
    "                \"CE_BidPrice\": ce.get(\"bidprice\"),\n",
    "                \"CE_AskQty\": ce.get(\"askQty\"),\n",
    "                \"CE_AskPrice\": ce.get(\"askPrice\"),\n",
    "                \"PE_BidQty\": pe.get(\"bidQty\"),\n",
    "                \"PE_BidPrice\": pe.get(\"bidprice\"),\n",
    "                \"PE_AskQty\": pe.get(\"askQty\"),\n",
    "                \"PE_AskPrice\": pe.get(\"askPrice\"),\n",
    "                \"CE_IV\": ce.get(\"impliedVolatility\"),\n",
    "                \"PE_IV\": pe.get(\"impliedVolatility\"),\n",
    "                \"Underlying\": ce.get(\"underlying\") or pe.get(\"underlying\"),\n",
    "                \"UnderlyingValue\": ce.get(\"underlyingValue\") or pe.get(\"underlyingValue\"),\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def compute_pcr(rows):\n",
    "    \"\"\"\n",
    "    Calculate Put-Call Ratio (PCR)\n",
    "    PCR = Total Put OI / Total Call OI\n",
    "    \"\"\"\n",
    "    total_call_oi = sum((r.get(\"Call_OI\") or 0) for r in rows)\n",
    "    total_put_oi = sum((r.get(\"Put_OI\") or 0) for r in rows)\n",
    "    pcr = (total_put_oi / total_call_oi) if total_call_oi else None\n",
    "    return total_call_oi, total_put_oi, (round(pcr, 6) if pcr is not None else None)\n",
    "\n",
    "# ==========================\n",
    "# MAX PAIN CALCULATION\n",
    "# ==========================\n",
    "def calculate_max_pain(df: pd.DataFrame) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Calculate Max Pain strike price for an option chain.\n",
    "    \n",
    "    Max Pain Theory: The strike price at which option writers (sellers) \n",
    "    experience minimum total loss at expiration.\n",
    "    \n",
    "    Returns dict with: max_pain_strike, min_loss, or None if calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure required columns exist\n",
    "        required = ['Strike', 'Call_OI', 'Put_OI']\n",
    "        if not all(col in df.columns for col in required):\n",
    "            return None\n",
    "        \n",
    "        # Clean data\n",
    "        df = df.copy()\n",
    "        df = df.dropna(subset=required)\n",
    "        if df.empty:\n",
    "            return None\n",
    "        \n",
    "        strikes = df['Strike'].values\n",
    "        call_oi = df['Call_OI'].values\n",
    "        put_oi = df['Put_OI'].values\n",
    "        \n",
    "        # Skip if all OI is zero\n",
    "        if call_oi.sum() == 0 and put_oi.sum() == 0:\n",
    "            return None\n",
    "        \n",
    "        total_losses = []\n",
    "        # Calculate total loss at each potential expiry price\n",
    "        for expiry_strike in strikes:\n",
    "            # Call writers lose when price > strike: (price - strike) Ã— call_oi\n",
    "            call_loss = ((expiry_strike - strikes).clip(min=0) * call_oi).sum()\n",
    "            # Put writers lose when price < strike: (strike - price) Ã— put_oi\n",
    "            put_loss = ((strikes - expiry_strike).clip(min=0) * put_oi).sum()\n",
    "            total_losses.append(call_loss + put_loss)\n",
    "        \n",
    "        # Find strike with minimum loss\n",
    "        min_loss_idx = total_losses.index(min(total_losses))\n",
    "        max_pain_strike = int(strikes[min_loss_idx])\n",
    "        min_loss = total_losses[min_loss_idx]\n",
    "        \n",
    "        return {\n",
    "            'max_pain_strike': max_pain_strike,\n",
    "            'min_loss': min_loss\n",
    "        }\n",
    "    except Exception as e:\n",
    "        if VERBOSE_ERRORS:\n",
    "            print(f\"  âš  Max Pain calculation error: {e}\")\n",
    "        return None\n",
    "\n",
    "def plot_max_pain(df: pd.DataFrame, title: str, save_path: str):\n",
    "    \"\"\"Generate and save Max Pain visualization\"\"\"\n",
    "    try:\n",
    "        strikes = df['Strike'].values\n",
    "        call_oi = df['Call_OI'].values\n",
    "        put_oi = df['Put_OI'].values\n",
    "        \n",
    "        total_losses = []\n",
    "        for expiry_strike in strikes:\n",
    "            call_loss = ((expiry_strike - strikes).clip(min=0) * call_oi).sum()\n",
    "            put_loss = ((strikes - expiry_strike).clip(min=0) * put_oi).sum()\n",
    "            total_losses.append(call_loss + put_loss)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(strikes, total_losses, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel(\"Strike Price\")\n",
    "        plt.ylabel(\"Total Loss (to Option Writers)\")\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        max_pain_idx = total_losses.index(min(total_losses))\n",
    "        max_pain = int(strikes[max_pain_idx])\n",
    "        plt.axvline(max_pain, color='red', linestyle='--', linewidth=2, \n",
    "                    label=f\"Max Pain = {max_pain}\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=150)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        if VERBOSE_ERRORS:\n",
    "            print(f\"  âš  Plot generation error: {e}\")\n",
    "\n",
    "# ==========================\n",
    "# EQUITY PROCESSING\n",
    "# ==========================\n",
    "def process_symbol(session, raw_symbol, target_expiries, out_base):\n",
    "    \"\"\"\n",
    "    Process a single equity symbol:\n",
    "    1. Fetch option chain data\n",
    "    2. Save per-expiry CSVs\n",
    "    3. Calculate PCR and Max Pain for each expiry\n",
    "    \n",
    "    Returns list of results or empty list if symbol has no options\n",
    "    \"\"\"\n",
    "    symbol = strip_prefix(raw_symbol).upper()\n",
    "    is_index = symbol in INDEX_SYMBOLS\n",
    "\n",
    "    # First attempt\n",
    "    data = fetch_option_chain(session, symbol, is_index=is_index)\n",
    "    \n",
    "    # If failed, try once more with cookie refresh\n",
    "    if data is None:\n",
    "        warm_cookies(session)\n",
    "        time.sleep(random.uniform(0.5, 1.0))\n",
    "        data = fetch_option_chain(session, symbol, is_index=is_index)\n",
    "    \n",
    "    # If still no data, symbol doesn't have options\n",
    "    if data is None:\n",
    "        return []  # Return empty list instead of raising error\n",
    "\n",
    "    found_expiries = set(data.get(\"records\", {}).get(\"expiryDates\", []))\n",
    "    if not found_expiries:\n",
    "        return []\n",
    "    \n",
    "    out_dir = os.path.join(out_base, symbol)\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    pcr_maxpain_out = []\n",
    "    \n",
    "    for expiry in target_expiries:\n",
    "        if expiry not in found_expiries:\n",
    "            continue\n",
    "        \n",
    "        rows = parse_rows_for_expiry(data, expiry)\n",
    "        if not rows:\n",
    "            continue\n",
    "        \n",
    "        df = pd.DataFrame(rows).sort_values(\"Strike\").reset_index(drop=True)\n",
    "        \n",
    "        # Save option chain CSV\n",
    "        fname = f\"{symbol}_{expiry}.csv\".replace(\"/\", \"-\")\n",
    "        out_path = os.path.join(out_dir, fname)\n",
    "        try:\n",
    "            df.to_csv(out_path, index=False)\n",
    "        except Exception as e:\n",
    "            if VERBOSE_ERRORS:\n",
    "                print(f\"[WARN] Error writing {out_path}: {e}\")\n",
    "        \n",
    "        # Calculate PCR\n",
    "        tcoi, tpoi, pcr = compute_pcr(rows)\n",
    "        \n",
    "        # Calculate Max Pain\n",
    "        max_pain_result = calculate_max_pain(df)\n",
    "        max_pain_strike = max_pain_result['max_pain_strike'] if max_pain_result else None\n",
    "        \n",
    "        # Generate Max Pain plot if enabled\n",
    "        if ENABLE_PLOTS and max_pain_result:\n",
    "            plot_path = os.path.join(out_dir, f\"{symbol}_{expiry}_MaxPain.png\".replace(\"/\", \"-\"))\n",
    "            plot_title = f\"{symbol} - {expiry} Max Pain Analysis\"\n",
    "            plot_max_pain(df, plot_title, plot_path)\n",
    "        \n",
    "        pcr_maxpain_out.append({\n",
    "            \"Symbol\": symbol,\n",
    "            \"Expiry\": expiry,\n",
    "            \"Total_Call_OI\": tcoi,\n",
    "            \"Total_Put_OI\": tpoi,\n",
    "            \"PCR\": pcr,\n",
    "            \"Max_Pain_Strike\": max_pain_strike\n",
    "        })\n",
    "    \n",
    "    return pcr_maxpain_out\n",
    "\n",
    "def process_symbol_threadsafe(raw_symbol, target_expiries, out_base):\n",
    "    \"\"\"Thread-safe wrapper for processing symbols\"\"\"\n",
    "    s = new_session()\n",
    "    warm_cookies(s)\n",
    "    time.sleep(random.uniform(0.2, 0.7))\n",
    "    \n",
    "    try:\n",
    "        return process_symbol(s, raw_symbol, target_expiries, out_base)\n",
    "    except Exception as e:\n",
    "        symbol = strip_prefix(raw_symbol)\n",
    "        print(f\"  âš  {symbol}: No options data available or fetch error\")\n",
    "        if VERBOSE_ERRORS:\n",
    "            traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# ==========================\n",
    "# INDEX PROCESSING\n",
    "# ==========================\n",
    "def process_index_all_expiries(session, symbol, out_base):\n",
    "    \"\"\"\n",
    "    Process all expiries for an index:\n",
    "    1. Fetch option chain data\n",
    "    2. Save per-expiry CSVs\n",
    "    3. Calculate PCR and Max Pain for each expiry\n",
    "    4. Generate master summary file\n",
    "    \"\"\"\n",
    "    symbol = symbol.upper()\n",
    "    data = fetch_option_chain(session, symbol, is_index=True)\n",
    "    \n",
    "    if data is None:\n",
    "        raise RuntimeError(f\"No option data available for index {symbol}\")\n",
    "\n",
    "    expiries = data.get(\"records\", {}).get(\"expiryDates\", [])\n",
    "    if not expiries:\n",
    "        raise RuntimeError(f\"No expiries found for index {symbol}\")\n",
    "\n",
    "    out_dir = os.path.join(out_base, symbol)\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    pcr_maxpain_rows = []\n",
    "    \n",
    "    for expiry in expiries:\n",
    "        rows = parse_rows_for_expiry(data, expiry)\n",
    "        \n",
    "        if not rows:\n",
    "            # Create empty CSV for transparency\n",
    "            df_empty = pd.DataFrame(columns=[\n",
    "                \"Strike\", \"Expiry\", \"Call_OI\", \"Call_LTP\", \"Put_OI\", \"Put_LTP\",\n",
    "                \"CE_BidQty\", \"CE_BidPrice\", \"CE_AskQty\", \"CE_AskPrice\",\n",
    "                \"PE_BidQty\", \"PE_BidPrice\", \"PE_AskQty\", \"PE_AskPrice\",\n",
    "                \"CE_IV\", \"PE_IV\", \"Underlying\", \"UnderlyingValue\"\n",
    "            ])\n",
    "            out_path = os.path.join(out_dir, f\"{symbol}_{expiry}.csv\".replace(\"/\", \"-\"))\n",
    "            try:\n",
    "                df_empty.to_csv(out_path, index=False)\n",
    "            except Exception as e:\n",
    "                if VERBOSE_ERRORS:\n",
    "                    print(f\"[WARN] Error writing empty CSV {out_path}: {e}\")\n",
    "            \n",
    "            pcr_maxpain_rows.append({\n",
    "                \"Expiry\": expiry,\n",
    "                \"Total_Call_OI\": 0,\n",
    "                \"Total_Put_OI\": 0,\n",
    "                \"PCR\": None,\n",
    "                \"Max_Pain_Strike\": None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(rows).sort_values(\"Strike\").reset_index(drop=True)\n",
    "        \n",
    "        # Save option chain CSV\n",
    "        out_path = os.path.join(out_dir, f\"{symbol}_{expiry}.csv\".replace(\"/\", \"-\"))\n",
    "        try:\n",
    "            df.to_csv(out_path, index=False)\n",
    "        except Exception as e:\n",
    "            if VERBOSE_ERRORS:\n",
    "                print(f\"[WARN] Error writing {out_path}: {e}\")\n",
    "\n",
    "        # Calculate PCR\n",
    "        tcoi, tpoi, pcr = compute_pcr(rows)\n",
    "        \n",
    "        # Calculate Max Pain\n",
    "        max_pain_result = calculate_max_pain(df)\n",
    "        max_pain_strike = max_pain_result['max_pain_strike'] if max_pain_result else None\n",
    "        \n",
    "        # Generate Max Pain plot if enabled\n",
    "        if ENABLE_PLOTS and max_pain_result:\n",
    "            plot_path = os.path.join(out_dir, f\"{symbol}_{expiry}_MaxPain.png\".replace(\"/\", \"-\"))\n",
    "            plot_title = f\"{symbol} - {expiry} Max Pain Analysis\"\n",
    "            plot_max_pain(df, plot_title, plot_path)\n",
    "        \n",
    "        pcr_maxpain_rows.append({\n",
    "            \"Expiry\": expiry,\n",
    "            \"Total_Call_OI\": tcoi,\n",
    "            \"Total_Put_OI\": tpoi,\n",
    "            \"PCR\": pcr,\n",
    "            \"Max_Pain_Strike\": max_pain_strike\n",
    "        })\n",
    "\n",
    "    # Save master summary file for index\n",
    "    master_df = pd.DataFrame(pcr_maxpain_rows, \n",
    "                            columns=[\"Expiry\", \"Total_Call_OI\", \"Total_Put_OI\", \"PCR\", \"Max_Pain_Strike\"])\n",
    "    master_name = f\"{symbol}_Analysis_Summary.csv\"\n",
    "    master_path = os.path.join(out_dir, master_name)\n",
    "    try:\n",
    "        master_df.sort_values(\"Expiry\", inplace=True)\n",
    "        master_df.to_csv(master_path, index=False)\n",
    "        print(f\"  âœ“ {symbol} summary saved ({len(master_df)} expiries)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Writing index master file {master_path}: {e}\")\n",
    "\n",
    "    return pcr_maxpain_rows\n",
    "\n",
    "# ==========================\n",
    "# MAIN EXECUTION\n",
    "# ==========================\n",
    "def main():\n",
    "    \"\"\"Main execution flow\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"NSE OPTIONS ANALYZER - PCR & MAX PAIN CALCULATOR\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    ensure_dir(BASE_DIR)\n",
    "    print(f\"\\nğŸ“ Output directory: {os.path.abspath(BASE_DIR)}\\n\")\n",
    "\n",
    "    equities_out_base = BASE_DIR\n",
    "    indices_base = os.path.join(BASE_DIR, \"index_options\")\n",
    "    ensure_dir(indices_base)\n",
    "\n",
    "    # Initialize session\n",
    "    session = new_session()\n",
    "    if not warm_cookies(session):\n",
    "        print(\"[WARN] Warmup did not return 200 â€” proceeding anyway...\\n\")\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 1: Determine target expiries from RELIANCE\n",
    "    # ==========================================\n",
    "    print(\"STEP 1: Determining target expiries from RELIANCE...\")\n",
    "    try:\n",
    "        base_symbol = \"RELIANCE\"\n",
    "        reliance_data = fetch_option_chain(session, base_symbol, is_index=False)\n",
    "        \n",
    "        if reliance_data is None:\n",
    "            print(\"[ERROR] RELIANCE has no option data. Using fallback...\")\n",
    "            # Fallback: Try another liquid stock\n",
    "            for fallback in [\"SBIN\", \"HDFCBANK\", \"INFY\", \"TCS\"]:\n",
    "                print(f\"  Trying {fallback}...\")\n",
    "                reliance_data = fetch_option_chain(session, fallback, is_index=False)\n",
    "                if reliance_data:\n",
    "                    base_symbol = fallback\n",
    "                    break\n",
    "        \n",
    "        if reliance_data is None:\n",
    "            raise RuntimeError(\"Cannot determine target expiries from any fallback symbol\")\n",
    "        \n",
    "        target_expiries = reliance_data.get(\"records\", {}).get(\"expiryDates\", [])\n",
    "        if len(target_expiries) < 3:\n",
    "            raise RuntimeError(f\"Could not find 3 expiries for {base_symbol}. Found: {target_expiries}\")\n",
    "        target_expiries = target_expiries[:3]\n",
    "        print(f\"âœ“ Using {base_symbol} expiries: {target_expiries}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL] Cannot determine target expiries: {e}\")\n",
    "        if VERBOSE_ERRORS:\n",
    "            traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 2: Process all equities (multithreaded)\n",
    "    # ==========================================\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 2: Processing Equities (PCR + Max Pain)...\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Note: Symbols without options data will be skipped silently\\n\")\n",
    "    \n",
    "    max_workers = 6  # Reduced to avoid rate limiting\n",
    "    futures = []\n",
    "    results = {exp: [] for exp in target_expiries}\n",
    "    skipped_count = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for raw in RAW_TICKERS:\n",
    "            futures.append(executor.submit(process_symbol_threadsafe, raw, target_expiries, equities_out_base))\n",
    "        \n",
    "        completed = 0\n",
    "        total = len(futures)\n",
    "        for fut in as_completed(futures):\n",
    "            completed += 1\n",
    "            try:\n",
    "                sym_results = fut.result()\n",
    "                if sym_results:\n",
    "                    processed_count += 1\n",
    "                    for row in sym_results:\n",
    "                        exp = row.get(\"Expiry\")\n",
    "                        if exp in results:\n",
    "                            results[exp].append(row)\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "            except Exception as e:\n",
    "                skipped_count += 1\n",
    "                if VERBOSE_ERRORS:\n",
    "                    print(f\"\\n[ERROR] Worker raised: {e}\")\n",
    "                    traceback.print_exc()\n",
    "            \n",
    "            # Progress indicator\n",
    "            print(f\"  Progress: {completed}/{total} | âœ“ Processed: {processed_count} | âŠ˜ Skipped: {skipped_count}\", end='\\r')\n",
    "    \n",
    "    print(f\"\\n\\nâœ“ Completed: {processed_count} symbols with options data\")\n",
    "    print(f\"âŠ˜ Skipped: {skipped_count} symbols (no options data or errors)\\n\")\n",
    "\n",
    "    # Write master PCR + Max Pain files for equities (3 files)\n",
    "    print(\"Generating master equity summary files...\")\n",
    "    for exp in target_expiries:\n",
    "        rows = results.get(exp, [])\n",
    "        if not rows:\n",
    "            print(f\"  âš  No data for expiry {exp}\")\n",
    "            continue\n",
    "            \n",
    "        df = pd.DataFrame(rows, columns=[\"Symbol\", \"Expiry\", \"Total_Call_OI\", \"Total_Put_OI\", \"PCR\", \"Max_Pain_Strike\"])\n",
    "        df.sort_values(\"Symbol\", inplace=True)\n",
    "        master_name = f\"Equities_Analysis_{exp}.csv\".replace(\"/\", \"-\")\n",
    "        master_path = os.path.join(BASE_DIR, master_name)\n",
    "        try:\n",
    "            df.to_csv(master_path, index=False)\n",
    "            print(f\"  âœ“ {master_name} ({len(df)} symbols)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Writing {master_path}: {e}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # STEP 3: Process indices (all expiries)\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 3: Processing Indices (PCR + Max Pain)...\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    index_success = 0\n",
    "    index_failed = 0\n",
    "    \n",
    "    for idx in INDEX_LIST:\n",
    "        try:\n",
    "            print(f\"ğŸ“Š Processing {idx}...\")\n",
    "            s = new_session()\n",
    "            warm_cookies(s)\n",
    "            process_index_all_expiries(s, idx, indices_base)\n",
    "            index_success += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Failed to process {idx}: {e}\")\n",
    "            index_failed += 1\n",
    "            if VERBOSE_ERRORS:\n",
    "                traceback.print_exc()\n",
    "\n",
    "    # ==========================================\n",
    "    # FINAL SUMMARY\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nğŸ“‚ All results saved to: {os.path.abspath(BASE_DIR)}\")\n",
    "    print(\"\\nğŸ“Š Summary:\")\n",
    "    print(f\"  â€¢ Equities processed: {processed_count}/{len(RAW_TICKERS)}\")\n",
    "    print(f\"  â€¢ Equities skipped: {skipped_count} (no options data)\")\n",
    "    print(f\"  â€¢ Indices processed: {index_success}/{len(INDEX_LIST)}\")\n",
    "    if index_failed > 0:\n",
    "        print(f\"  â€¢ Indices failed: {index_failed}\")\n",
    "    print(f\"  â€¢ Master equity files: {len(target_expiries)} expiries\")\n",
    "    print(f\"  â€¢ Target expiries: {', '.join(target_expiries)}\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Generated Files:\")\n",
    "    print(\"  â”œâ”€ Equities_Analysis_<expiry>.csv (master PCR + Max Pain)\")\n",
    "    print(\"  â”œâ”€ <SYMBOL>/ (individual equity folders)\")\n",
    "    print(\"  â”‚   â””â”€ <SYMBOL>_<expiry>.csv (option chain data)\")\n",
    "    print(\"  â””â”€ index_options/\")\n",
    "    print(\"      â”œâ”€ NIFTY/\")\n",
    "    print(\"      â”‚   â”œâ”€ NIFTY_<expiry>.csv (all expiries)\")\n",
    "    print(\"      â”‚   â””â”€ NIFTY_Analysis_Summary.csv\")\n",
    "    print(\"      â””â”€ BANKNIFTY/\")\n",
    "    print(\"          â”œâ”€ BANKNIFTY_<expiry>.csv (all expiries)\")\n",
    "    print(\"          â””â”€ BANKNIFTY_Analysis_Summary.csv\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Each file contains:\")\n",
    "    print(\"  âœ“ Option chain data (Strike, OI, LTP, IV, Bid/Ask)\")\n",
    "    print(\"  âœ“ PCR (Put-Call Ratio) = Put OI / Call OI\")\n",
    "    print(\"  âœ“ Max Pain Strike (minimum loss for option writers)\")\n",
    "    \n",
    "    if ENABLE_PLOTS:\n",
    "        print(\"  âœ“ Max Pain visualization plots (.png)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # Save processing log\n",
    "    log_path = os.path.join(BASE_DIR, \"processing_log.txt\")\n",
    "    try:\n",
    "        with open(log_path, 'w') as f:\n",
    "            f.write(\"NSE Options Analysis - Processing Log\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Total tickers attempted: {len(RAW_TICKERS)}\\n\")\n",
    "            f.write(f\"Successfully processed: {processed_count}\\n\")\n",
    "            f.write(f\"Skipped (no options): {skipped_count}\\n\")\n",
    "            f.write(f\"Target expiries: {', '.join(target_expiries)}\\n\")\n",
    "            f.write(f\"\\nIndices processed: {index_success}/{len(INDEX_LIST)}\\n\")\n",
    "            if index_failed > 0:\n",
    "                f.write(f\"Indices failed: {index_failed}\\n\")\n",
    "        print(f\"ğŸ“„ Processing log saved: processing_log.txt\")\n",
    "    except Exception as e:\n",
    "        if VERBOSE_ERRORS:\n",
    "            print(f\"[WARN] Could not save log: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâš  Process interrupted by user\")\n",
    "        sys.exit(0)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[FATAL] Unhandled exception in main(): {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0b76e-c0fe-45a5-9811-3144bb2fa1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9494db-528f-4fae-9eed-ea200bbd4554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing analysis directories...\n",
      "âœ… Data found at: C:\\Users\\sarda\\Desktop\\nse_options_analysis\n",
      "âœ… Output will be saved to: C:\\Users\\sarda\\Desktop\\nse_options_analysis_pro\n",
      "\n",
      "ğŸ“… Available expiries: 25-Nov-2025, 27-Jan-2026, 30-Dec-2025\n",
      "\n",
      "======================================================================\n",
      "âœ… READY TO ANALYZE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“š Available functions:\n",
      "  â€¢ generate_market_report(expiry) - Full market analysis\n",
      "  â€¢ analyze_stock(symbol, expiry) - Detailed stock analysis\n",
      "  â€¢ analyze_indices() - NIFTY & BANKNIFTY analysis\n",
      "\n",
      "ğŸ’¡ Example usage:\n",
      "  df = generate_market_report('25-Nov-2025')\n",
      "  analyze_stock('RELIANCE', '25-Nov-2025')\n",
      "  analyze_indices()\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "NSE Options Advanced Analyzer & Signal Generator (Jupyter Compatible)\n",
    "FIXED VERSION - All warnings and errors resolved\n",
    "\n",
    "Run this in Cell 2 after your data collection script.\n",
    "\n",
    "Fixes:\n",
    "1. SettingWithCopyWarning resolved (df_clean['distance'])\n",
    "2. TypeError in index analysis resolved (None values)\n",
    "3. Improved error handling\n",
    "4. Better data validation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suppress pandas warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "BASE_DIR = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"nse_options_analysis\")\n",
    "OUTPUT_DIR = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"nse_options_analysis_pro\")\n",
    "HISTORICAL_DIR = os.path.join(OUTPUT_DIR, \"historical_tracking\")\n",
    "SIGNALS_DIR = os.path.join(OUTPUT_DIR, \"trade_signals\")\n",
    "REPORTS_DIR = os.path.join(OUTPUT_DIR, \"reports\")\n",
    "\n",
    "# Analysis thresholds\n",
    "PCR_EXTREME_BULLISH = 1.5\n",
    "PCR_EXTREME_BEARISH = 0.7\n",
    "PCR_NEUTRAL_LOW = 0.9\n",
    "PCR_NEUTRAL_HIGH = 1.1\n",
    "\n",
    "MAX_PAIN_DISTANCE_THRESHOLD = 5.0\n",
    "MAX_PAIN_STRONG_THRESHOLD = 8.0\n",
    "\n",
    "IV_PERCENTILE_HIGH = 75\n",
    "IV_PERCENTILE_LOW = 25\n",
    "\n",
    "# ==========================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def get_today_str():\n",
    "    \"\"\"Get today's date as string\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def safe_float(val, default=None):\n",
    "    \"\"\"Safely convert to float\"\"\"\n",
    "    try:\n",
    "        if val is None or pd.isna(val):\n",
    "            return default\n",
    "        return float(val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "# ==========================\n",
    "# DATA LOADING\n",
    "# ==========================\n",
    "def load_equity_analysis(expiry: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load equity analysis CSV for a specific expiry\"\"\"\n",
    "    file_path = os.path.join(BASE_DIR, f\"Equities_Analysis_{expiry}.csv\".replace(\"/\", \"-\"))\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš  File not found: {file_path}\")\n",
    "        return None\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def load_index_analysis(index_name: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load index analysis summary CSV\"\"\"\n",
    "    file_path = os.path.join(BASE_DIR, \"index_options\", index_name, f\"{index_name}_Analysis_Summary.csv\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš  File not found: {file_path}\")\n",
    "        return None\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def load_option_chain(symbol: str, expiry: str, is_index=False) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load detailed option chain for a symbol\"\"\"\n",
    "    if is_index:\n",
    "        file_path = os.path.join(BASE_DIR, \"index_options\", symbol, f\"{symbol}_{expiry}.csv\".replace(\"/\", \"-\"))\n",
    "    else:\n",
    "        file_path = os.path.join(BASE_DIR, symbol, f\"{symbol}_{expiry}.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def get_underlying_price(df: pd.DataFrame) -> Optional[float]:\n",
    "    \"\"\"Extract underlying price (CMP) from option chain\"\"\"\n",
    "    if 'UnderlyingValue' in df.columns:\n",
    "        price = df['UnderlyingValue'].dropna().iloc[0] if not df['UnderlyingValue'].dropna().empty else None\n",
    "        return safe_float(price)\n",
    "    return None\n",
    "\n",
    "# ==========================\n",
    "# OI ANALYSIS\n",
    "# ==========================\n",
    "def analyze_oi_levels(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze OI distribution to find support/resistance levels\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    max_call_oi_strike = df.loc[df['Call_OI'].idxmax(), 'Strike'] if df['Call_OI'].sum() > 0 else None\n",
    "    max_put_oi_strike = df.loc[df['Put_OI'].idxmax(), 'Strike'] if df['Put_OI'].sum() > 0 else None\n",
    "    \n",
    "    top_call_strikes = df.nlargest(3, 'Call_OI')[['Strike', 'Call_OI']].to_dict('records')\n",
    "    top_put_strikes = df.nlargest(3, 'Put_OI')[['Strike', 'Put_OI']].to_dict('records')\n",
    "    \n",
    "    avg_call_oi = df['Call_OI'].mean()\n",
    "    avg_put_oi = df['Put_OI'].mean()\n",
    "    \n",
    "    strong_resistance = df[df['Call_OI'] > 2 * avg_call_oi]['Strike'].tolist()\n",
    "    strong_support = df[df['Put_OI'] > 2 * avg_put_oi]['Strike'].tolist()\n",
    "    \n",
    "    return {\n",
    "        'max_call_oi_strike': max_call_oi_strike,\n",
    "        'max_put_oi_strike': max_put_oi_strike,\n",
    "        'top_call_strikes': top_call_strikes,\n",
    "        'top_put_strikes': top_put_strikes,\n",
    "        'strong_resistance_zones': strong_resistance,\n",
    "        'strong_support_zones': strong_support,\n",
    "        'total_call_oi': df['Call_OI'].sum(),\n",
    "        'total_put_oi': df['Put_OI'].sum()\n",
    "    }\n",
    "\n",
    "# ==========================\n",
    "# IV ANALYSIS (FIXED)\n",
    "# ==========================\n",
    "def analyze_iv(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Analyze Implied Volatility patterns - FIXED VERSION\"\"\"\n",
    "    if df.empty or 'CE_IV' not in df.columns or 'PE_IV' not in df.columns:\n",
    "        return {}\n",
    "    \n",
    "    # FIX: Create explicit copy to avoid SettingWithCopyWarning\n",
    "    df_clean = df.dropna(subset=['CE_IV', 'PE_IV']).copy()\n",
    "    if df_clean.empty:\n",
    "        return {}\n",
    "    \n",
    "    avg_call_iv = df_clean['CE_IV'].mean()\n",
    "    avg_put_iv = df_clean['PE_IV'].mean()\n",
    "    iv_skew = avg_put_iv - avg_call_iv\n",
    "    \n",
    "    # Find ATM IV\n",
    "    underlying = get_underlying_price(df)\n",
    "    if underlying:\n",
    "        # FIX: Use .loc to properly assign values\n",
    "        df_clean.loc[:, 'distance'] = abs(df_clean['Strike'] - underlying)\n",
    "        atm_row = df_clean.loc[df_clean['distance'].idxmin()]\n",
    "        atm_call_iv = safe_float(atm_row['CE_IV'])\n",
    "        atm_put_iv = safe_float(atm_row['PE_IV'])\n",
    "    else:\n",
    "        atm_call_iv = None\n",
    "        atm_put_iv = None\n",
    "    \n",
    "    return {\n",
    "        'avg_call_iv': avg_call_iv,\n",
    "        'avg_put_iv': avg_put_iv,\n",
    "        'iv_skew': iv_skew,\n",
    "        'atm_call_iv': atm_call_iv,\n",
    "        'atm_put_iv': atm_put_iv,\n",
    "        'max_call_iv': df_clean['CE_IV'].max(),\n",
    "        'min_call_iv': df_clean['CE_IV'].min(),\n",
    "        'max_put_iv': df_clean['PE_IV'].max(),\n",
    "        'min_put_iv': df_clean['PE_IV'].min()\n",
    "    }\n",
    "\n",
    "# ==========================\n",
    "# SIGNAL GENERATION\n",
    "# ==========================\n",
    "def calculate_max_pain_signal(cmp: float, max_pain: float, pcr: float) -> Dict:\n",
    "    \"\"\"Generate signal based on CMP vs Max Pain divergence\"\"\"\n",
    "    if not cmp or not max_pain or not pcr:\n",
    "        return {'signal': 'NEUTRAL', 'strength': 0, 'reason': 'Insufficient data', 'distance_pct': 0}\n",
    "    \n",
    "    distance_pct = ((max_pain - cmp) / cmp) * 100\n",
    "    abs_distance = abs(distance_pct)\n",
    "    \n",
    "    if abs_distance < MAX_PAIN_DISTANCE_THRESHOLD:\n",
    "        strength = 1\n",
    "    elif abs_distance < MAX_PAIN_STRONG_THRESHOLD:\n",
    "        strength = 2\n",
    "    else:\n",
    "        strength = 3\n",
    "    \n",
    "    if distance_pct > 0:\n",
    "        if pcr > 1.0:\n",
    "            signal = 'BULLISH'\n",
    "            reason = f'CMP below Max Pain ({distance_pct:.2f}%), PCR={pcr:.2f} (bearish sentiment â†’ contrarian bullish)'\n",
    "        else:\n",
    "            signal = 'WEAK_BULLISH'\n",
    "            reason = f'CMP below Max Pain ({distance_pct:.2f}%) but PCR={pcr:.2f} (not confirming)'\n",
    "    else:\n",
    "        if pcr < 1.0:\n",
    "            signal = 'BEARISH'\n",
    "            reason = f'CMP above Max Pain ({abs(distance_pct):.2f}%), PCR={pcr:.2f} (bullish sentiment â†’ contrarian bearish)'\n",
    "        else:\n",
    "            signal = 'WEAK_BEARISH'\n",
    "            reason = f'CMP above Max Pain ({abs(distance_pct):.2f}%) but PCR={pcr:.2f} (not confirming)'\n",
    "    \n",
    "    return {\n",
    "        'signal': signal,\n",
    "        'strength': strength,\n",
    "        'distance_pct': distance_pct,\n",
    "        'reason': reason\n",
    "    }\n",
    "\n",
    "def calculate_pcr_signal(pcr: float) -> Dict:\n",
    "    \"\"\"Generate signal based on PCR extremes\"\"\"\n",
    "    if not pcr:\n",
    "        return {'signal': 'NEUTRAL', 'strength': 0, 'reason': 'No PCR data'}\n",
    "    \n",
    "    if pcr >= PCR_EXTREME_BULLISH:\n",
    "        strength = 3 if pcr > 1.8 else 2\n",
    "        return {\n",
    "            'signal': 'BULLISH',\n",
    "            'strength': strength,\n",
    "            'reason': f'Extreme bearish sentiment (PCR={pcr:.2f}) â†’ Contrarian bullish'\n",
    "        }\n",
    "    elif pcr <= PCR_EXTREME_BEARISH:\n",
    "        strength = 3 if pcr < 0.5 else 2\n",
    "        return {\n",
    "            'signal': 'BEARISH',\n",
    "            'strength': strength,\n",
    "            'reason': f'Extreme bullish sentiment (PCR={pcr:.2f}) â†’ Contrarian bearish'\n",
    "        }\n",
    "    elif PCR_NEUTRAL_LOW <= pcr <= PCR_NEUTRAL_HIGH:\n",
    "        return {\n",
    "            'signal': 'NEUTRAL',\n",
    "            'strength': 0,\n",
    "            'reason': f'PCR in neutral zone ({pcr:.2f})'\n",
    "        }\n",
    "    elif pcr > PCR_NEUTRAL_HIGH:\n",
    "        return {\n",
    "            'signal': 'WEAK_BULLISH',\n",
    "            'strength': 1,\n",
    "            'reason': f'Slightly bearish sentiment (PCR={pcr:.2f})'\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'signal': 'WEAK_BEARISH',\n",
    "            'strength': 1,\n",
    "            'reason': f'Slightly bullish sentiment (PCR={pcr:.2f})'\n",
    "        }\n",
    "\n",
    "def calculate_oi_signal(cmp: float, oi_analysis: Dict) -> Dict:\n",
    "    \"\"\"Generate signal based on OI distribution\"\"\"\n",
    "    if not cmp or not oi_analysis:\n",
    "        return {'signal': 'NEUTRAL', 'strength': 0, 'reason': 'No OI data'}\n",
    "    \n",
    "    resistance = oi_analysis.get('max_call_oi_strike')\n",
    "    support = oi_analysis.get('max_put_oi_strike')\n",
    "    \n",
    "    if not resistance or not support:\n",
    "        return {'signal': 'NEUTRAL', 'strength': 0, 'reason': 'Insufficient OI data'}\n",
    "    \n",
    "    range_size = resistance - support\n",
    "    if range_size <= 0:\n",
    "        return {'signal': 'NEUTRAL', 'strength': 0, 'reason': 'Invalid S/R levels'}\n",
    "    \n",
    "    position_pct = ((cmp - support) / range_size) * 100\n",
    "    \n",
    "    if position_pct < 25:\n",
    "        signal = 'BULLISH'\n",
    "        reason = f'Near support ({support:.0f}), resistance at {resistance:.0f}'\n",
    "        strength = 2\n",
    "    elif position_pct > 75:\n",
    "        signal = 'BEARISH'\n",
    "        reason = f'Near resistance ({resistance:.0f}), support at {support:.0f}'\n",
    "        strength = 2\n",
    "    else:\n",
    "        signal = 'NEUTRAL'\n",
    "        reason = f'Mid-range between support {support:.0f} and resistance {resistance:.0f}'\n",
    "        strength = 0\n",
    "    \n",
    "    return {\n",
    "        'signal': signal,\n",
    "        'strength': strength,\n",
    "        'reason': reason,\n",
    "        'support': support,\n",
    "        'resistance': resistance,\n",
    "        'position_pct': position_pct\n",
    "    }\n",
    "\n",
    "def calculate_iv_signal(iv_analysis: Dict) -> Dict:\n",
    "    \"\"\"Generate trading strategy based on IV levels\"\"\"\n",
    "    if not iv_analysis or 'atm_call_iv' not in iv_analysis:\n",
    "        return {'signal': 'NEUTRAL', 'strength': 0, 'reason': 'No IV data', 'atm_iv': None, 'iv_skew': 0}\n",
    "    \n",
    "    atm_call = iv_analysis.get('atm_call_iv', 0) or 0\n",
    "    atm_put = iv_analysis.get('atm_put_iv', 0) or 0\n",
    "    atm_iv = (atm_call + atm_put) / 2 if (atm_call or atm_put) else 0\n",
    "    iv_skew = iv_analysis.get('iv_skew', 0)\n",
    "    \n",
    "    if not atm_iv:\n",
    "        return {'signal': 'NEUTRAL', 'strength': 0, 'reason': 'No ATM IV data', 'atm_iv': None, 'iv_skew': 0}\n",
    "    \n",
    "    if atm_iv > 30:\n",
    "        signal = 'SELL_OPTIONS'\n",
    "        strength = 2 if atm_iv > 40 else 1\n",
    "        reason = f'High IV ({atm_iv:.1f}%) â†’ Favor premium selling'\n",
    "    elif atm_iv < 15:\n",
    "        signal = 'BUY_OPTIONS'\n",
    "        strength = 2 if atm_iv < 10 else 1\n",
    "        reason = f'Low IV ({atm_iv:.1f}%) â†’ Favor option buying'\n",
    "    else:\n",
    "        signal = 'NEUTRAL'\n",
    "        strength = 0\n",
    "        reason = f'Moderate IV ({atm_iv:.1f}%)'\n",
    "    \n",
    "    skew_note = ''\n",
    "    if abs(iv_skew) > 3:\n",
    "        if iv_skew > 0:\n",
    "            skew_note = ' | Put IV > Call IV (bearish skew)'\n",
    "        else:\n",
    "            skew_note = ' | Call IV > Put IV (bullish skew)'\n",
    "    \n",
    "    return {\n",
    "        'signal': signal,\n",
    "        'strength': strength,\n",
    "        'reason': reason + skew_note,\n",
    "        'atm_iv': atm_iv,\n",
    "        'iv_skew': iv_skew\n",
    "    }\n",
    "\n",
    "# ==========================\n",
    "# COMBINED SIGNAL SCORING\n",
    "# ==========================\n",
    "def generate_combined_signal(symbol: str, expiry: str, is_index=False) -> Optional[Dict]:\n",
    "    \"\"\"Combine all signals to generate final trade recommendation\"\"\"\n",
    "    df = load_option_chain(symbol, expiry, is_index)\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    \n",
    "    cmp = get_underlying_price(df)\n",
    "    if not cmp:\n",
    "        return None\n",
    "    \n",
    "    if is_index:\n",
    "        summary = load_index_analysis(symbol)\n",
    "        if summary is None:\n",
    "            return None\n",
    "        row = summary[summary['Expiry'] == expiry]\n",
    "    else:\n",
    "        summary = load_equity_analysis(expiry)\n",
    "        if summary is None:\n",
    "            return None\n",
    "        row = summary[summary['Symbol'] == symbol]\n",
    "    \n",
    "    if row.empty:\n",
    "        return None\n",
    "    \n",
    "    pcr = safe_float(row.iloc[0]['PCR'])\n",
    "    max_pain = safe_float(row.iloc[0]['Max_Pain_Strike'])\n",
    "    \n",
    "    # Validate essential data\n",
    "    if not pcr or not max_pain:\n",
    "        return None\n",
    "    \n",
    "    max_pain_sig = calculate_max_pain_signal(cmp, max_pain, pcr)\n",
    "    pcr_sig = calculate_pcr_signal(pcr)\n",
    "    \n",
    "    oi_analysis = analyze_oi_levels(df)\n",
    "    oi_sig = calculate_oi_signal(cmp, oi_analysis)\n",
    "    \n",
    "    iv_analysis = analyze_iv(df)\n",
    "    iv_sig = calculate_iv_signal(iv_analysis)\n",
    "    \n",
    "    signal_map = {'BULLISH': 3, 'WEAK_BULLISH': 1, 'NEUTRAL': 0, 'WEAK_BEARISH': -1, 'BEARISH': -3}\n",
    "    \n",
    "    max_pain_score = signal_map.get(max_pain_sig['signal'], 0) * max_pain_sig['strength']\n",
    "    pcr_score = signal_map.get(pcr_sig['signal'], 0) * pcr_sig['strength']\n",
    "    oi_score = signal_map.get(oi_sig['signal'], 0) * oi_sig['strength']\n",
    "    \n",
    "    total_score = max_pain_score + pcr_score + oi_score\n",
    "    max_possible = 3 * 3 + 3 * 3 + 2 * 3\n",
    "    \n",
    "    confidence_pct = (abs(total_score) / max_possible) * 100\n",
    "    \n",
    "    if total_score >= 6:\n",
    "        final_signal = 'STRONG_BULLISH'\n",
    "    elif total_score >= 3:\n",
    "        final_signal = 'BULLISH'\n",
    "    elif total_score >= 1:\n",
    "        final_signal = 'WEAK_BULLISH'\n",
    "    elif total_score <= -6:\n",
    "        final_signal = 'STRONG_BEARISH'\n",
    "    elif total_score <= -3:\n",
    "        final_signal = 'BEARISH'\n",
    "    elif total_score <= -1:\n",
    "        final_signal = 'WEAK_BEARISH'\n",
    "    else:\n",
    "        final_signal = 'NEUTRAL'\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'expiry': expiry,\n",
    "        'cmp': cmp,\n",
    "        'max_pain': max_pain,\n",
    "        'pcr': pcr,\n",
    "        'final_signal': final_signal,\n",
    "        'confidence_pct': confidence_pct,\n",
    "        'total_score': total_score,\n",
    "        'max_pain_signal': max_pain_sig,\n",
    "        'pcr_signal': pcr_sig,\n",
    "        'oi_signal': oi_sig,\n",
    "        'iv_signal': iv_sig,\n",
    "        'oi_analysis': oi_analysis,\n",
    "        'iv_analysis': iv_analysis\n",
    "    }\n",
    "\n",
    "# ==========================\n",
    "# STRIKE RECOMMENDATION\n",
    "# ==========================\n",
    "def recommend_strikes(signal_data: Dict, df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Recommend specific strikes to trade\"\"\"\n",
    "    if not signal_data or df is None or df.empty:\n",
    "        return {}\n",
    "    \n",
    "    cmp = signal_data['cmp']\n",
    "    final_signal = signal_data['final_signal']\n",
    "    iv_signal = signal_data['iv_signal']['signal']\n",
    "    \n",
    "    recommendations = []\n",
    "    df = df.dropna(subset=['Strike', 'Call_LTP', 'Put_LTP'])\n",
    "    \n",
    "    if 'BULLISH' in final_signal:\n",
    "        if iv_signal == 'BUY_OPTIONS':\n",
    "            atm_calls = df[df['Strike'] >= cmp].nsmallest(3, 'Strike')\n",
    "            for _, row in atm_calls.iterrows():\n",
    "                strike = row['Strike']\n",
    "                premium = safe_float(row['Call_LTP'], 0)\n",
    "                if premium > 0:\n",
    "                    risk = premium\n",
    "                    potential_reward = (strike * 0.05)\n",
    "                    rr_ratio = potential_reward / risk if risk > 0 else 0\n",
    "                    recommendations.append({\n",
    "                        'type': 'BUY_CALL',\n",
    "                        'strike': strike,\n",
    "                        'premium': premium,\n",
    "                        'risk': risk,\n",
    "                        'potential_reward': potential_reward,\n",
    "                        'rr_ratio': rr_ratio,\n",
    "                        'description': f'Buy {strike} Call @ â‚¹{premium:.2f}'\n",
    "                    })\n",
    "        else:\n",
    "            otm_puts = df[df['Strike'] < cmp].nlargest(3, 'Strike')\n",
    "            for _, row in otm_puts.iterrows():\n",
    "                strike = row['Strike']\n",
    "                premium = safe_float(row['Put_LTP'], 0)\n",
    "                if premium > 0:\n",
    "                    risk = strike * 0.1\n",
    "                    reward = premium\n",
    "                    rr_ratio = reward / risk if risk > 0 else 0\n",
    "                    recommendations.append({\n",
    "                        'type': 'SELL_PUT',\n",
    "                        'strike': strike,\n",
    "                        'premium': premium,\n",
    "                        'reward': reward,\n",
    "                        'risk': risk,\n",
    "                        'rr_ratio': rr_ratio,\n",
    "                        'description': f'Sell {strike} Put @ â‚¹{premium:.2f}'\n",
    "                    })\n",
    "    \n",
    "    elif 'BEARISH' in final_signal:\n",
    "        if iv_signal == 'BUY_OPTIONS':\n",
    "            atm_puts = df[df['Strike'] <= cmp].nlargest(3, 'Strike')\n",
    "            for _, row in atm_puts.iterrows():\n",
    "                strike = row['Strike']\n",
    "                premium = safe_float(row['Put_LTP'], 0)\n",
    "                if premium > 0:\n",
    "                    risk = premium\n",
    "                    potential_reward = (strike * 0.05)\n",
    "                    rr_ratio = potential_reward / risk if risk > 0 else 0\n",
    "                    recommendations.append({\n",
    "                        'type': 'BUY_PUT',\n",
    "                        'strike': strike,\n",
    "                        'premium': premium,\n",
    "                        'risk': risk,\n",
    "                        'potential_reward': potential_reward,\n",
    "                        'rr_ratio': rr_ratio,\n",
    "                        'description': f'Buy {strike} Put @ â‚¹{premium:.2f}'\n",
    "                    })\n",
    "        else:\n",
    "            otm_calls = df[df['Strike'] > cmp].nsmallest(3, 'Strike')\n",
    "            for _, row in otm_calls.iterrows():\n",
    "                strike = row['Strike']\n",
    "                premium = safe_float(row['Call_LTP'], 0)\n",
    "                if premium > 0:\n",
    "                    risk = strike * 0.1\n",
    "                    reward = premium\n",
    "                    rr_ratio = reward / risk if risk > 0 else 0\n",
    "                    recommendations.append({\n",
    "                        'type': 'SELL_CALL',\n",
    "                        'strike': strike,\n",
    "                        'premium': premium,\n",
    "                        'reward': reward,\n",
    "                        'risk': risk,\n",
    "                        'rr_ratio': rr_ratio,\n",
    "                        'description': f'Sell {strike} Call @ â‚¹{premium:.2f}'\n",
    "                    })\n",
    "    \n",
    "    recommendations.sort(key=lambda x: x.get('rr_ratio', 0), reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'top_trades': recommendations[:3] if recommendations else [],\n",
    "        'total_opportunities': len(recommendations)\n",
    "    }\n",
    "\n",
    "# ==========================\n",
    "# HISTORICAL TRACKING\n",
    "# ==========================\n",
    "def append_to_historical(symbol: str, expiry: str, signal_data: Dict, is_index=False):\n",
    "    \"\"\"Append today's data to historical CSV\"\"\"\n",
    "    if not signal_data:\n",
    "        return\n",
    "    \n",
    "    today = get_today_str()\n",
    "    \n",
    "    row = {\n",
    "        'Date': today,\n",
    "        'Symbol': symbol,\n",
    "        'Expiry': expiry,\n",
    "        'CMP': signal_data['cmp'],\n",
    "        'Max_Pain': signal_data['max_pain'],\n",
    "        'PCR': signal_data['pcr'],\n",
    "        'Final_Signal': signal_data['final_signal'],\n",
    "        'Confidence_pct': signal_data['confidence_pct'],\n",
    "        'Max_Pain_Distance_pct': signal_data['max_pain_signal']['distance_pct'],\n",
    "        'ATM_IV': signal_data['iv_analysis'].get('atm_call_iv'),\n",
    "        'IV_Skew': signal_data['iv_analysis'].get('iv_skew'),\n",
    "        'Call_OI_Total': signal_data['oi_analysis'].get('total_call_oi'),\n",
    "        'Put_OI_Total': signal_data['oi_analysis'].get('total_put_oi'),\n",
    "        'Support': signal_data['oi_signal'].get('support'),\n",
    "        'Resistance': signal_data['oi_signal'].get('resistance')\n",
    "    }\n",
    "    \n",
    "    hist_file = os.path.join(HISTORICAL_DIR, f\"{symbol}_history.csv\")\n",
    "    \n",
    "    if os.path.exists(hist_file):\n",
    "        hist_df = pd.read_csv(hist_file)\n",
    "        if not ((hist_df['Date'] == today) & (hist_df['Expiry'] == expiry)).any():\n",
    "            hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n",
    "            hist_df.to_csv(hist_file, index=False)\n",
    "    else:\n",
    "        pd.DataFrame([row]).to_csv(hist_file, index=False)\n",
    "\n",
    "# ==========================\n",
    "# MAIN ANALYSIS FUNCTIONS\n",
    "# ==========================\n",
    "def generate_market_report(expiry: str):\n",
    "    \"\"\"Generate comprehensive market report\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MARKET ANALYSIS REPORT - {expiry}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    summary = load_equity_analysis(expiry)\n",
    "    if summary is None:\n",
    "        print(\"âŒ No data available for this expiry\")\n",
    "        return\n",
    "    \n",
    "    all_signals = []\n",
    "    \n",
    "    print(f\"Analyzing {len(summary)} stocks...\\n\")\n",
    "    \n",
    "    for idx, row in summary.iterrows():\n",
    "        symbol = row['Symbol']\n",
    "        signal_data = generate_combined_signal(symbol, expiry, is_index=False)\n",
    "        \n",
    "        if signal_data:\n",
    "            all_signals.append(signal_data)\n",
    "            append_to_historical(symbol, expiry, signal_data, is_index=False)\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Analyzed: {idx + 1}/{len(summary)}\", end='\\r')\n",
    "    \n",
    "    print(f\"\\n\")\n",
    "    \n",
    "    signals_df = pd.DataFrame([{\n",
    "        'Symbol': s['symbol'],\n",
    "        'CMP': s['cmp'],\n",
    "        'Max_Pain': s['max_pain'],\n",
    "        'PCR': s['pcr'],\n",
    "        'Signal': s['final_signal'],\n",
    "        'Confidence': f\"{s['confidence_pct']:.1f}%\",\n",
    "        'Score': s['total_score']\n",
    "    } for s in all_signals])\n",
    "    \n",
    "    report_file = os.path.join(REPORTS_DIR, f\"Market_Report_{expiry}_{get_today_str()}.csv\".replace(\"/\", \"-\"))\n",
    "    signals_df.to_csv(report_file, index=False)\n",
    "    \n",
    "    print(f\"ğŸ“Š MARKET SUMMARY:\")\n",
    "    print(f\"Total stocks analyzed: {len(signals_df)}\\n\")\n",
    "    \n",
    "    signal_counts = signals_df['Signal'].value_counts()\n",
    "    for sig, count in signal_counts.items():\n",
    "        print(f\"  {sig}: {count}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ TOP BULLISH OPPORTUNITIES:\")\n",
    "    bullish = signals_df[signals_df['Signal'].str.contains('BULLISH')].nlargest(10, 'Score')\n",
    "    print(bullish[['Symbol', 'CMP', 'Max_Pain', 'PCR', 'Signal', 'Confidence']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nğŸ¯ TOP BEARISH OPPORTUNITIES:\")\n",
    "    bearish = signals_df[signals_df['Signal'].str.contains('BEARISH')].nsmallest(10, 'Score')\n",
    "    print(bearish[['Symbol', 'CMP', 'Max_Pain', 'PCR', 'Signal', 'Confidence']].to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Full report saved: {report_file}\\n\")\n",
    "    \n",
    "    return signals_df\n",
    "\n",
    "def analyze_stock(symbol: str, expiry: str):\n",
    "    \"\"\"Detailed analysis for a single stock\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DETAILED ANALYSIS: {symbol} - {expiry}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    is_index = symbol in ['NIFTY', 'BANKNIFTY']\n",
    "    signal_data = generate_combined_signal(symbol, expiry, is_index)\n",
    "    \n",
    "    if not signal_data:\n",
    "        print(\"âŒ No data available\")\n",
    "        return\n",
    "    \n",
    "    df = load_option_chain(symbol, expiry, is_index)\n",
    "    \n",
    "    # FIX: Safely format with None checks\n",
    "    cmp = signal_data['cmp'] or 0\n",
    "    max_pain = signal_data['max_pain'] or 0\n",
    "    pcr = signal_data['pcr'] or 0\n",
    "    distance = signal_data['max_pain_signal']['distance_pct'] or 0\n",
    "    \n",
    "    print(f\"ğŸ“ CMP: â‚¹{cmp:.2f}\")\n",
    "    print(f\"ğŸ¯ Max Pain: â‚¹{max_pain:.2f}\")\n",
    "    print(f\"ğŸ“Š PCR: {pcr:.4f}\")\n",
    "    print(f\"ğŸ“ˆ Distance from Max Pain: {distance:.2f}%\\n\")\n",
    "    \n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(\"SIGNAL BREAKDOWN:\")\n",
    "    print(f\"{'â”€'*70}\\n\")\n",
    "    \n",
    "    print(f\"1ï¸âƒ£ MAX PAIN: {signal_data['max_pain_signal']['signal']} ({'â­' * signal_data['max_pain_signal']['strength']})\")\n",
    "    print(f\"   {signal_data['max_pain_signal']['reason']}\\n\")\n",
    "    \n",
    "    print(f\"2ï¸âƒ£ PCR: {signal_data['pcr_signal']['signal']} ({'â­' * signal_data['pcr_signal']['strength']})\")\n",
    "    print(f\"   {signal_data['pcr_signal']['reason']}\\n\")\n",
    "    \n",
    "    print(f\"3ï¸âƒ£ OI: {signal_data['oi_signal']['signal']} ({'â­' * signal_data['oi_signal']['strength']})\")\n",
    "    print(f\"   {signal_data['oi_signal']['reason']}\")\n",
    "    if 'support' in signal_data['oi_signal']:\n",
    "        support = signal_data['oi_signal'].get('support', 0) or 0\n",
    "        resistance = signal_data['oi_signal'].get('resistance', 0) or 0\n",
    "        print(f\"   Support: â‚¹{support:.2f} | Resistance: â‚¹{resistance:.2f}\\n\")\n",
    "    \n",
    "    print(f\"4ï¸âƒ£ IV: {signal_data['iv_signal']['signal']} ({'â­' * signal_data['iv_signal']['strength']})\")\n",
    "    print(f\"   {signal_data['iv_signal']['reason']}\")\n",
    "    if signal_data['iv_signal'].get('atm_iv'):\n",
    "        atm_iv = signal_data['iv_signal']['atm_iv'] or 0\n",
    "        print(f\"   ATM IV: {atm_iv:.2f}%\\n\")\n",
    "    \n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(f\"ğŸ¯ FINAL SIGNAL: {signal_data['final_signal']}\")\n",
    "    print(f\"ğŸ“Š Confidence: {signal_data['confidence_pct']:.1f}%\")\n",
    "    print(f\"ğŸ’¯ Composite Score: {signal_data['total_score']}\")\n",
    "    print(f\"{'â”€'*70}\\n\")\n",
    "    \n",
    "    # Strike recommendations\n",
    "    if df is not None and not df.empty:\n",
    "        strikes = recommend_strikes(signal_data, df)\n",
    "        \n",
    "        if strikes and strikes.get('top_trades'):\n",
    "            print(f\"{'='*70}\")\n",
    "            print(\"RECOMMENDED TRADES:\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "            \n",
    "            for i, trade in enumerate(strikes['top_trades'], 1):\n",
    "                print(f\"{i}. {trade['description']}\")\n",
    "                print(f\"   Type: {trade['type']}\")\n",
    "                print(f\"   Premium: â‚¹{trade.get('premium', 0):.2f}\")\n",
    "                if 'risk' in trade and 'potential_reward' in trade:\n",
    "                    print(f\"   Risk: â‚¹{trade['risk']:.2f}\")\n",
    "                    print(f\"   Potential Reward: â‚¹{trade.get('potential_reward', 0):.2f}\")\n",
    "                    print(f\"   Risk-Reward: 1:{trade.get('rr_ratio', 0):.2f}\")\n",
    "                print()\n",
    "    \n",
    "    append_to_historical(symbol, expiry, signal_data, is_index)\n",
    "    \n",
    "    return signal_data\n",
    "\n",
    "def analyze_indices():\n",
    "    \"\"\"Analyze NIFTY and BANKNIFTY - FIXED VERSION\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"INDEX ANALYSIS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for index in ['NIFTY', 'BANKNIFTY']:\n",
    "        summary = load_index_analysis(index)\n",
    "        if summary is None:\n",
    "            print(f\"âŒ No data for {index}\\n\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"{'â”€'*70}\")\n",
    "        print(f\"{index}\")\n",
    "        print(f\"{'â”€'*70}\\n\")\n",
    "        \n",
    "        index_results = []\n",
    "        \n",
    "        for _, row in summary.head(3).iterrows():\n",
    "            expiry = row['Expiry']\n",
    "            signal_data = generate_combined_signal(index, expiry, is_index=True)\n",
    "            \n",
    "            if signal_data:\n",
    "                # FIX: Safely handle None values\n",
    "                cmp = signal_data.get('cmp') or 0\n",
    "                max_pain = signal_data.get('max_pain') or 0\n",
    "                pcr = signal_data.get('pcr') or 0\n",
    "                confidence = signal_data.get('confidence_pct', 0)\n",
    "                \n",
    "                print(f\"ğŸ“… {expiry}\")\n",
    "                print(f\"   CMP: â‚¹{cmp:.2f} | Max Pain: â‚¹{max_pain:.2f} | PCR: {pcr:.4f}\")\n",
    "                print(f\"   Signal: {signal_data['final_signal']} ({confidence:.1f}% confidence)\")\n",
    "                print(f\"   {signal_data['max_pain_signal']['reason']}\\n\")\n",
    "                \n",
    "                append_to_historical(index, expiry, signal_data, is_index=True)\n",
    "                index_results.append(signal_data)\n",
    "        \n",
    "        results[index] = index_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ==========================\n",
    "# INITIALIZE DIRECTORIES\n",
    "# ==========================\n",
    "print(\"Initializing analysis directories...\")\n",
    "ensure_dir(OUTPUT_DIR)\n",
    "ensure_dir(HISTORICAL_DIR)\n",
    "ensure_dir(SIGNALS_DIR)\n",
    "ensure_dir(REPORTS_DIR)\n",
    "\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    print(f\"\\nâŒ Error: Base data not found at {BASE_DIR}\")\n",
    "    print(\"Please run the data collection script first!\")\n",
    "else:\n",
    "    print(f\"âœ… Data found at: {BASE_DIR}\")\n",
    "    print(f\"âœ… Output will be saved to: {OUTPUT_DIR}\")\n",
    "    \n",
    "    equity_files = [f for f in os.listdir(BASE_DIR) if f.startswith('Equities_Analysis_')]\n",
    "    if equity_files:\n",
    "        expiries = [f.replace('Equities_Analysis_', '').replace('.csv', '') for f in equity_files]\n",
    "        print(f\"\\nğŸ“… Available expiries: {', '.join(expiries)}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… READY TO ANALYZE!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nğŸ“š Available functions:\")\n",
    "        print(\"  â€¢ generate_market_report(expiry) - Full market analysis\")\n",
    "        print(\"  â€¢ analyze_stock(symbol, expiry) - Detailed stock analysis\")\n",
    "        print(\"  â€¢ analyze_indices() - NIFTY & BANKNIFTY analysis\")\n",
    "        print(\"\\nğŸ’¡ Example usage:\")\n",
    "        print(f\"  df = generate_market_report('{expiries[0]}')\")\n",
    "        print(f\"  analyze_stock('RELIANCE', '{expiries[0]}')\")\n",
    "        print(\"  analyze_indices()\")\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    else:\n",
    "        print(\"\\nâŒ No equity analysis files found!\")\n",
    "        print(\"Please run the data collection script first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c50072-1735-49de-8677-10dbb08f6828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… Available expiries: ['25-Nov-2025', '27-Jan-2026', '30-Dec-2025']\n",
      "\n",
      "ğŸ” Analyzing: 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "MARKET ANALYSIS REPORT - 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "Analyzing 205 stocks...\n",
      "\n",
      "  Analyzed: 10/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 20/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 40/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 60/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 90/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 150/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 160/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 190/205\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed: 200/205\n",
      "\n",
      "ğŸ“Š MARKET SUMMARY:\n",
      "Total stocks analyzed: 205\n",
      "\n",
      "  STRONG_BEARISH: 92\n",
      "  BEARISH: 52\n",
      "  WEAK_BEARISH: 31\n",
      "  WEAK_BULLISH: 19\n",
      "  NEUTRAL: 8\n",
      "  STRONG_BULLISH: 2\n",
      "  BULLISH: 1\n",
      "\n",
      "ğŸ¯ TOP BULLISH OPPORTUNITIES:\n",
      "   Symbol     CMP  Max_Pain      PCR         Signal Confidence\n",
      "   CONCOR  522.00     540.0 0.755448 STRONG_BULLISH      25.0%\n",
      " EXIDEIND  377.50     390.0 0.705869 STRONG_BULLISH      25.0%\n",
      " PETRONET  277.70     290.0 1.123096        BULLISH      16.7%\n",
      "AMBUJACEM  558.25     570.0 0.673039   WEAK_BULLISH       4.2%\n",
      "     CAMS 3834.60    3900.0 0.533747   WEAK_BULLISH       4.2%\n",
      "  CGPOWER  732.00     740.0 0.656610   WEAK_BULLISH       4.2%\n",
      "   COLPAL 2184.00    2200.0 0.624022   WEAK_BULLISH       4.2%\n",
      " GLENMARK 1814.00    1840.0 0.685088   WEAK_BULLISH       4.2%\n",
      "  HDFCAMC 5481.00    5500.0 0.627669   WEAK_BULLISH       4.2%\n",
      "      IEX  139.45     145.0 0.581362   WEAK_BULLISH       4.2%\n",
      "\n",
      "ğŸ¯ TOP BEARISH OPPORTUNITIES:\n",
      "    Symbol     CMP  Max_Pain      PCR         Signal Confidence\n",
      "ADANIENSOL  989.85     960.0 0.544516 STRONG_BEARISH      62.5%\n",
      "    ASTRAL 1557.10    1520.0 0.667271 STRONG_BEARISH      62.5%\n",
      "AUROPHARMA 1165.00    1140.0 0.688213 STRONG_BEARISH      62.5%\n",
      "       BEL  426.80     420.0 0.619583 STRONG_BEARISH      62.5%\n",
      "   COFORGE 1797.80    1780.0 0.502689 STRONG_BEARISH      62.5%\n",
      "ICICIPRULI  623.60     605.0 0.688662 STRONG_BEARISH      62.5%\n",
      "      IDEA   10.19      10.0 0.559602 STRONG_BEARISH      62.5%\n",
      "  JSWSTEEL 1186.70    1170.0 0.584208 STRONG_BEARISH      62.5%\n",
      "     NYKAA  261.60     255.0 0.647982 STRONG_BEARISH      62.5%\n",
      "PERSISTENT 6034.00    5900.0 0.675182 STRONG_BEARISH      62.5%\n",
      "\n",
      "ğŸ’¾ Full report saved: C:\\Users\\sarda\\Desktop\\nse_options_analysis_pro\\reports\\Market_Report_25-Nov-2025_2025-11-11.csv\n",
      "\n",
      "\n",
      "ğŸ“ˆ QUICK STATS:\n",
      "Total stocks analyzed: 205\n",
      "Average PCR: 0.6118\n",
      "Bullish signals: 22\n",
      "Bearish signals: 175\n",
      "Neutral signals: 8\n",
      "\n",
      "ğŸ¯ High confidence trades (>60%): 14\n",
      "\n",
      "Top 5 High Confidence Opportunities:\n",
      "    Symbol         Signal Confidence      PCR  Score\n",
      "ADANIENSOL STRONG_BEARISH      62.5% 0.544516    -15\n",
      "    ASTRAL STRONG_BEARISH      62.5% 0.667271    -15\n",
      "AUROPHARMA STRONG_BEARISH      62.5% 0.688213    -15\n",
      "       BEL STRONG_BEARISH      62.5% 0.619583    -15\n",
      "   COFORGE STRONG_BEARISH      62.5% 0.502689    -15\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Quick Market Overview\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"nse_options_analysis\")\n",
    "equity_files = [f for f in os.listdir(BASE_DIR) if f.startswith('Equities_Analysis_')]\n",
    "expiries = [f.replace('Equities_Analysis_', '').replace('.csv', '') for f in equity_files]\n",
    "\n",
    "print(f\"ğŸ“… Available expiries: {expiries}\")\n",
    "\n",
    "# Analyze nearest expiry\n",
    "if expiries:\n",
    "    expiry = expiries[0]\n",
    "    print(f\"\\nğŸ” Analyzing: {expiry}\")\n",
    "    print(\"=\" * 70)\n",
    "    market_df = generate_market_report(expiry)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    if market_df is not None and not market_df.empty:\n",
    "        print(\"\\nğŸ“ˆ QUICK STATS:\")\n",
    "        print(f\"Total stocks analyzed: {len(market_df)}\")\n",
    "        print(f\"Average PCR: {market_df['PCR'].mean():.4f}\")\n",
    "        print(f\"Bullish signals: {len(market_df[market_df['Signal'].str.contains('BULLISH')])}\")\n",
    "        print(f\"Bearish signals: {len(market_df[market_df['Signal'].str.contains('BEARISH')])}\")\n",
    "        print(f\"Neutral signals: {len(market_df[market_df['Signal'] == 'NEUTRAL'])}\")\n",
    "        \n",
    "        # High confidence trades (>60%)\n",
    "        market_df['Confidence_num'] = market_df['Confidence'].str.rstrip('%').astype(float)\n",
    "        high_conf = market_df[market_df['Confidence_num'] > 60]\n",
    "        print(f\"\\nğŸ¯ High confidence trades (>60%): {len(high_conf)}\")\n",
    "        \n",
    "        if not high_conf.empty:\n",
    "            print(\"\\nTop 5 High Confidence Opportunities:\")\n",
    "            print(high_conf.nlargest(5, 'Confidence_num')[['Symbol', 'Signal', 'Confidence', 'PCR', 'Score']].to_string(index=False))\n",
    "else:\n",
    "    print(\"âŒ No expiries found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f57dc6a-8a7b-41f9-8118-0379088c02e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š INDEX ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "INDEX ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "NIFTY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“… 02-Dec-2025\n",
      "   CMP: â‚¹25694.95 | Max Pain: â‚¹25700.00 | PCR: 0.6821\n",
      "   Signal: BEARISH (20.8% confidence)\n",
      "   CMP below Max Pain (0.02%) but PCR=0.68 (not confirming)\n",
      "\n",
      "ğŸ“… 09-Dec-2025\n",
      "   CMP: â‚¹25694.95 | Max Pain: â‚¹25650.00 | PCR: 1.2064\n",
      "   Signal: STRONG_BEARISH (25.0% confidence)\n",
      "   CMP above Max Pain (0.17%) but PCR=1.21 (not confirming)\n",
      "\n",
      "ğŸ“… 11-Nov-2025\n",
      "   CMP: â‚¹25694.95 | Max Pain: â‚¹25650.00 | PCR: 1.1074\n",
      "   Signal: STRONG_BEARISH (25.0% confidence)\n",
      "   CMP above Max Pain (0.17%) but PCR=1.11 (not confirming)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "BANKNIFTY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“… 25-Nov-2025\n",
      "   CMP: â‚¹58138.15 | Max Pain: â‚¹58000.00 | PCR: 0.9205\n",
      "   Signal: BEARISH (12.5% confidence)\n",
      "   CMP above Max Pain (0.24%), PCR=0.92 (bullish sentiment â†’ contrarian bearish)\n",
      "\n",
      "ğŸ“… 27-Jan-2026\n",
      "   CMP: â‚¹58138.15 | Max Pain: â‚¹58000.00 | PCR: 4.0371\n",
      "   Signal: WEAK_BULLISH (8.3% confidence)\n",
      "   CMP above Max Pain (0.24%) but PCR=4.04 (not confirming)\n",
      "\n",
      "\n",
      "ğŸ“‹ INDEX SUMMARY:\n",
      "\n",
      "NIFTY:\n",
      "  â€¢ 02-Dec-2025: BEARISH (Confidence: 20.8%, PCR: 0.6821)\n",
      "  â€¢ 09-Dec-2025: STRONG_BEARISH (Confidence: 25.0%, PCR: 1.2064)\n",
      "  â€¢ 11-Nov-2025: STRONG_BEARISH (Confidence: 25.0%, PCR: 1.1074)\n",
      "\n",
      "BANKNIFTY:\n",
      "  â€¢ 25-Nov-2025: BEARISH (Confidence: 12.5%, PCR: 0.9205)\n",
      "  â€¢ 27-Jan-2026: WEAK_BULLISH (Confidence: 8.3%, PCR: 4.0371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\1148196810.py:554: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  hist_df = pd.concat([hist_df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Analyze NIFTY & BANKNIFTY\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“Š INDEX ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    index_results = analyze_indices()\n",
    "    \n",
    "    # Summary of index signals\n",
    "    print(\"\\nğŸ“‹ INDEX SUMMARY:\")\n",
    "    for index_name, results in index_results.items():\n",
    "        if results:\n",
    "            print(f\"\\n{index_name}:\")\n",
    "            for result in results:\n",
    "                expiry = result['expiry']\n",
    "                signal = result['final_signal']\n",
    "                conf = result['confidence_pct']\n",
    "                pcr = result['pcr']\n",
    "                print(f\"  â€¢ {expiry}: {signal} (Confidence: {conf:.1f}%, PCR: {pcr:.4f})\")\n",
    "        else:\n",
    "            print(f\"\\n{index_name}: No data available\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error analyzing indices: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if index data exists in Desktop/nse_options_analysis/index_options/\")\n",
    "    print(\"2. Ensure Cell 1 (data collection) completed successfully\")\n",
    "    print(\"3. Try running Cell 1 again to fetch fresh data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896454bb-08db-43a0-bf07-2d45e68ce9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ” DETAILED STOCK ANALYSIS - Expiry: 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DETAILED ANALYSIS: RELIANCE - 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ CMP: â‚¹1493.80\n",
      "ğŸ¯ Max Pain: â‚¹1500.00\n",
      "ğŸ“Š PCR: 0.6263\n",
      "ğŸ“ˆ Distance from Max Pain: 0.42%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SIGNAL BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "1ï¸âƒ£ MAX PAIN: WEAK_BULLISH (â­)\n",
      "   CMP below Max Pain (0.42%) but PCR=0.63 (not confirming)\n",
      "\n",
      "2ï¸âƒ£ PCR: BEARISH (â­â­)\n",
      "   Extreme bullish sentiment (PCR=0.63) â†’ Contrarian bearish\n",
      "\n",
      "3ï¸âƒ£ OI: NEUTRAL ()\n",
      "   Invalid S/R levels\n",
      "4ï¸âƒ£ IV: NEUTRAL ()\n",
      "   Moderate IV (17.8%) | Put IV > Call IV (bearish skew)\n",
      "   ATM IV: 17.80%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ¯ FINAL SIGNAL: BEARISH\n",
      "ğŸ“Š Confidence: 20.8%\n",
      "ğŸ’¯ Composite Score: -5\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDED TRADES:\n",
      "======================================================================\n",
      "\n",
      "1. Sell 1500 Call @ â‚¹21.30\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹21.30\n",
      "\n",
      "2. Sell 1510 Call @ â‚¹15.90\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹15.90\n",
      "\n",
      "3. Sell 1520 Call @ â‚¹11.60\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹11.60\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DETAILED ANALYSIS: INFY - 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ CMP: â‚¹1531.10\n",
      "ğŸ¯ Max Pain: â‚¹1500.00\n",
      "ğŸ“Š PCR: 0.6345\n",
      "ğŸ“ˆ Distance from Max Pain: -2.03%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SIGNAL BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "1ï¸âƒ£ MAX PAIN: BEARISH (â­)\n",
      "   CMP above Max Pain (2.03%), PCR=0.63 (bullish sentiment â†’ contrarian bearish)\n",
      "\n",
      "2ï¸âƒ£ PCR: BEARISH (â­â­)\n",
      "   Extreme bullish sentiment (PCR=0.63) â†’ Contrarian bearish\n",
      "\n",
      "3ï¸âƒ£ OI: NEUTRAL ()\n",
      "   Mid-range between support 1500 and resistance 1600\n",
      "   Support: â‚¹1500.00 | Resistance: â‚¹1600.00\n",
      "\n",
      "4ï¸âƒ£ IV: NEUTRAL ()\n",
      "   Moderate IV (25.5%) | Put IV > Call IV (bearish skew)\n",
      "   ATM IV: 25.54%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ¯ FINAL SIGNAL: STRONG_BEARISH\n",
      "ğŸ“Š Confidence: 37.5%\n",
      "ğŸ’¯ Composite Score: -9\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDED TRADES:\n",
      "======================================================================\n",
      "\n",
      "1. Sell 1540 Call @ â‚¹17.10\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹17.10\n",
      "\n",
      "2. Sell 1560 Call @ â‚¹12.05\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹12.05\n",
      "\n",
      "3. Sell 1580 Call @ â‚¹8.40\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹8.40\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DETAILED ANALYSIS: HDFCBANK - 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ CMP: â‚¹992.00\n",
      "ğŸ¯ Max Pain: â‚¹995.00\n",
      "ğŸ“Š PCR: 0.6903\n",
      "ğŸ“ˆ Distance from Max Pain: 0.30%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SIGNAL BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "1ï¸âƒ£ MAX PAIN: WEAK_BULLISH (â­)\n",
      "   CMP below Max Pain (0.30%) but PCR=0.69 (not confirming)\n",
      "\n",
      "2ï¸âƒ£ PCR: BEARISH (â­â­)\n",
      "   Extreme bullish sentiment (PCR=0.69) â†’ Contrarian bearish\n",
      "\n",
      "3ï¸âƒ£ OI: NEUTRAL ()\n",
      "   Invalid S/R levels\n",
      "4ï¸âƒ£ IV: NEUTRAL ()\n",
      "   Moderate IV (16.9%)\n",
      "   ATM IV: 16.95%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ¯ FINAL SIGNAL: BEARISH\n",
      "ğŸ“Š Confidence: 20.8%\n",
      "ğŸ’¯ Composite Score: -5\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDED TRADES:\n",
      "======================================================================\n",
      "\n",
      "1. Sell 995 Call @ â‚¹13.10\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹13.10\n",
      "\n",
      "2. Sell 1000 Call @ â‚¹10.90\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹10.90\n",
      "\n",
      "3. Sell 1005 Call @ â‚¹8.90\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹8.90\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DETAILED ANALYSIS: TCS - 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ CMP: â‚¹3049.00\n",
      "ğŸ¯ Max Pain: â‚¹3080.00\n",
      "ğŸ“Š PCR: 0.6754\n",
      "ğŸ“ˆ Distance from Max Pain: 1.02%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SIGNAL BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "1ï¸âƒ£ MAX PAIN: WEAK_BULLISH (â­)\n",
      "   CMP below Max Pain (1.02%) but PCR=0.68 (not confirming)\n",
      "\n",
      "2ï¸âƒ£ PCR: BEARISH (â­â­)\n",
      "   Extreme bullish sentiment (PCR=0.68) â†’ Contrarian bearish\n",
      "\n",
      "3ï¸âƒ£ OI: NEUTRAL ()\n",
      "   Mid-range between support 3000 and resistance 3100\n",
      "   Support: â‚¹3000.00 | Resistance: â‚¹3100.00\n",
      "\n",
      "4ï¸âƒ£ IV: NEUTRAL ()\n",
      "   Moderate IV (19.4%) | Put IV > Call IV (bearish skew)\n",
      "   ATM IV: 19.38%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ¯ FINAL SIGNAL: BEARISH\n",
      "ğŸ“Š Confidence: 20.8%\n",
      "ğŸ’¯ Composite Score: -5\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDED TRADES:\n",
      "======================================================================\n",
      "\n",
      "1. Sell 3060 Call @ â‚¹47.90\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹47.90\n",
      "\n",
      "2. Sell 3080 Call @ â‚¹38.00\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹38.00\n",
      "\n",
      "3. Sell 3100 Call @ â‚¹30.20\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹30.20\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DETAILED ANALYSIS: SBIN - 25-Nov-2025\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ CMP: â‚¹954.00\n",
      "ğŸ¯ Max Pain: â‚¹945.00\n",
      "ğŸ“Š PCR: 0.7716\n",
      "ğŸ“ˆ Distance from Max Pain: -0.94%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SIGNAL BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "1ï¸âƒ£ MAX PAIN: BEARISH (â­)\n",
      "   CMP above Max Pain (0.94%), PCR=0.77 (bullish sentiment â†’ contrarian bearish)\n",
      "\n",
      "2ï¸âƒ£ PCR: WEAK_BEARISH (â­)\n",
      "   Slightly bullish sentiment (PCR=0.77)\n",
      "\n",
      "3ï¸âƒ£ OI: BEARISH (â­â­)\n",
      "   Near resistance (970), support at 900\n",
      "   Support: â‚¹900.00 | Resistance: â‚¹970.00\n",
      "\n",
      "4ï¸âƒ£ IV: NEUTRAL ()\n",
      "   Moderate IV (18.1%) | Put IV > Call IV (bearish skew)\n",
      "   ATM IV: 18.08%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ğŸ¯ FINAL SIGNAL: STRONG_BEARISH\n",
      "ğŸ“Š Confidence: 41.7%\n",
      "ğŸ’¯ Composite Score: -10\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDED TRADES:\n",
      "======================================================================\n",
      "\n",
      "1. Sell 955 Call @ â‚¹14.90\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹14.90\n",
      "\n",
      "2. Sell 960 Call @ â‚¹12.30\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹12.30\n",
      "\n",
      "3. Sell 965 Call @ â‚¹9.80\n",
      "   Type: SELL_CALL\n",
      "   Premium: â‚¹9.80\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š COMPARISON TABLE\n",
      "======================================================================\n",
      "  Symbol         Signal Confidence  Score      PCR    CMP  Max_Pain  Distance_%\n",
      "RELIANCE        BEARISH      20.8%     -5 0.626302 1493.8    1500.0    0.415049\n",
      "    INFY STRONG_BEARISH      37.5%     -9 0.634547 1531.1    1500.0   -2.031219\n",
      "HDFCBANK        BEARISH      20.8%     -5 0.690321  992.0     995.0    0.302419\n",
      "     TCS        BEARISH      20.8%     -5 0.675368 3049.0    3080.0    1.016727\n",
      "    SBIN STRONG_BEARISH      41.7%    -10 0.771640  954.0     945.0   -0.943396\n",
      "\n",
      "ğŸ’¡ INTERPRETATION:\n",
      "  â€¢ Score > 6: Strong directional signal\n",
      "  â€¢ Score 3-6: Moderate signal\n",
      "  â€¢ Score -3 to 3: Weak/Neutral\n",
      "  â€¢ Confidence > 60%: Reliable signal\n",
      "  â€¢ PCR > 1.5: Extreme bearishness (contrarian bullish)\n",
      "  â€¢ PCR < 0.7: Extreme bullishness (contrarian bearish)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Analyze specific stocks with detailed breakdown\n",
    "stocks_to_analyze = ['RELIANCE', 'INFY', 'HDFCBANK', 'TCS', 'SBIN']\n",
    "expiry = expiries[0]  # Use nearest expiry\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ğŸ” DETAILED STOCK ANALYSIS - Expiry: {expiry}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "for stock in stocks_to_analyze:\n",
    "    try:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        result = analyze_stock(stock, expiry)\n",
    "        \n",
    "        if result:\n",
    "            analysis_results.append({\n",
    "                'Symbol': stock,\n",
    "                'Signal': result['final_signal'],\n",
    "                'Confidence': f\"{result['confidence_pct']:.1f}%\",\n",
    "                'Score': result['total_score'],\n",
    "                'PCR': result['pcr'],\n",
    "                'CMP': result['cmp'],\n",
    "                'Max_Pain': result['max_pain'],\n",
    "                'Distance_%': result['max_pain_signal']['distance_pct']\n",
    "            })\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš  Could not analyze {stock}: {e}\\n\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "if analysis_results:\n",
    "    import pandas as pd\n",
    "    comparison_df = pd.DataFrame(analysis_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š COMPARISON TABLE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "    print(\"  â€¢ Score > 6: Strong directional signal\")\n",
    "    print(\"  â€¢ Score 3-6: Moderate signal\")\n",
    "    print(\"  â€¢ Score -3 to 3: Weak/Neutral\")\n",
    "    print(\"  â€¢ Confidence > 60%: Reliable signal\")\n",
    "    print(\"  â€¢ PCR > 1.5: Extreme bearishness (contrarian bullish)\")\n",
    "    print(\"  â€¢ PCR < 0.7: Extreme bullishness (contrarian bearish)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3261a-5acc-4b6c-be8a-33bdca7ce03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95edb1db-a2d7-40f6-a088-a2a11d0b973e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ“Š DAILY HISTORICAL DATA BUILDER\n",
      "======================================================================\n",
      "\n",
      "ğŸ“… Date: 2025-11-11\n",
      "ğŸ’¾ Database: C:\\Users\\sarda\\Desktop\\nse_options_historical_db\n",
      "\n",
      "\n",
      "â° IMPORTANT: Run this cell EVERY DAY after market close!\n",
      "   Recommended time: After 4:00 PM IST\n",
      "\n",
      "\n",
      "ğŸ”„ Starting daily data collection...\n",
      "\n",
      "\n",
      "ğŸ“… Processing expiry: 25-Nov-2025\n",
      "  âœ“ Appended 360ONE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for 360ONE 25-Nov-2025\n",
      "  âœ“ Appended ABB 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ABB 25-Nov-2025\n",
      "  âœ“ Appended ABCAPITAL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ABCAPITAL 25-Nov-2025\n",
      "  âœ“ Appended ADANIENSOL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIENSOL 25-Nov-2025\n",
      "  âœ“ Appended ADANIENT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIENT 25-Nov-2025\n",
      "  âœ“ Appended ADANIGREEN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIGREEN 25-Nov-2025\n",
      "  âœ“ Appended ADANIPORTS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIPORTS 25-Nov-2025\n",
      "  âœ“ Appended ALKEM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ALKEM 25-Nov-2025\n",
      "  âœ“ Appended AMBER 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AMBER 25-Nov-2025\n",
      "  âœ“ Appended AMBUJACEM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AMBUJACEM 25-Nov-2025\n",
      "  âœ“ Appended ANGELONE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ANGELONE 25-Nov-2025\n",
      "  âœ“ Appended APLAPOLLO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for APLAPOLLO 25-Nov-2025\n",
      "  âœ“ Appended APOLLOHOSP 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for APOLLOHOSP 25-Nov-2025\n",
      "  âœ“ Appended ASHOKLEY 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASHOKLEY 25-Nov-2025\n",
      "  âœ“ Appended ASIANPAINT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASIANPAINT 25-Nov-2025\n",
      "  âœ“ Appended ASTRAL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASTRAL 25-Nov-2025\n",
      "  âœ“ Appended AUBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AUBANK 25-Nov-2025\n",
      "  âœ“ Appended AUROPHARMA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AUROPHARMA 25-Nov-2025\n",
      "  âœ“ Appended AXISBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AXISBANK 25-Nov-2025\n",
      "  âœ“ Appended BAJAJ-AUTO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJAJ-AUTO 25-Nov-2025\n",
      "  âœ“ Appended BAJAJFINSV 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJAJFINSV 25-Nov-2025\n",
      "  âœ“ Appended BAJFINANCE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJFINANCE 25-Nov-2025\n",
      "  âœ“ Appended BANDHANBNK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANDHANBNK 25-Nov-2025\n",
      "  âœ“ Appended BANKBARODA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKBARODA 25-Nov-2025\n",
      "  âœ“ Appended BANKINDIA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKINDIA 25-Nov-2025\n",
      "  âœ“ Appended BDL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BDL 25-Nov-2025\n",
      "  âœ“ Appended BEL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BEL 25-Nov-2025\n",
      "  âœ“ Appended BHARATFORG 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHARATFORG 25-Nov-2025\n",
      "  âœ“ Appended BHARTIARTL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHARTIARTL 25-Nov-2025\n",
      "  âœ“ Appended BHEL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHEL 25-Nov-2025\n",
      "  âœ“ Appended BIOCON 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BIOCON 25-Nov-2025\n",
      "  âœ“ Appended BLUESTARCO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BLUESTARCO 25-Nov-2025\n",
      "  âœ“ Appended BOSCHLTD 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BOSCHLTD 25-Nov-2025\n",
      "  âœ“ Appended BPCL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BPCL 25-Nov-2025\n",
      "  âœ“ Appended BRITANNIA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BRITANNIA 25-Nov-2025\n",
      "  âœ“ Appended BSE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BSE 25-Nov-2025\n",
      "  âœ“ Appended CAMS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CAMS 25-Nov-2025\n",
      "  âœ“ Appended CANBK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CANBK 25-Nov-2025\n",
      "  âœ“ Appended CDSL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CDSL 25-Nov-2025\n",
      "  âœ“ Appended CGPOWER 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CGPOWER 25-Nov-2025\n",
      "  âœ“ Appended CHOLAFIN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CHOLAFIN 25-Nov-2025\n",
      "  âœ“ Appended CIPLA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CIPLA 25-Nov-2025\n",
      "  âœ“ Appended COALINDIA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for COALINDIA 25-Nov-2025\n",
      "  âœ“ Appended COFORGE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for COFORGE 25-Nov-2025\n",
      "  âœ“ Appended COLPAL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for COLPAL 25-Nov-2025\n",
      "  âœ“ Appended CONCOR 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CONCOR 25-Nov-2025\n",
      "  âœ“ Appended CROMPTON 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CROMPTON 25-Nov-2025\n",
      "  âœ“ Appended CUMMINSIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CUMMINSIND 25-Nov-2025\n",
      "  âœ“ Appended CYIENT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CYIENT 25-Nov-2025\n",
      "  âœ“ Appended DABUR 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DABUR 25-Nov-2025\n",
      "  âœ“ Appended DALBHARAT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DALBHARAT 25-Nov-2025\n",
      "  âœ“ Appended DELHIVERY 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DELHIVERY 25-Nov-2025\n",
      "  âœ“ Appended DIVISLAB 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DIVISLAB 25-Nov-2025\n",
      "  âœ“ Appended DIXON 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DIXON 25-Nov-2025\n",
      "  âœ“ Appended DLF 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DLF 25-Nov-2025\n",
      "  âœ“ Appended DMART 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DMART 25-Nov-2025\n",
      "  âœ“ Appended DRREDDY 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DRREDDY 25-Nov-2025\n",
      "  âœ“ Appended EICHERMOT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for EICHERMOT 25-Nov-2025\n",
      "  âœ“ Appended ETERNAL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ETERNAL 25-Nov-2025\n",
      "  âœ“ Appended EXIDEIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for EXIDEIND 25-Nov-2025\n",
      "  âœ“ Appended FEDERALBNK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for FEDERALBNK 25-Nov-2025\n",
      "  âœ“ Appended FORTIS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for FORTIS 25-Nov-2025\n",
      "  âœ“ Appended GAIL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GAIL 25-Nov-2025\n",
      "  âœ“ Appended GLENMARK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GLENMARK 25-Nov-2025\n",
      "  âœ“ Appended GMRAIRPORT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GMRAIRPORT 25-Nov-2025\n",
      "  âœ“ Appended GODREJCP 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GODREJCP 25-Nov-2025\n",
      "  âœ“ Appended GODREJPROP 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GODREJPROP 25-Nov-2025\n",
      "  âœ“ Appended GRASIM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GRASIM 25-Nov-2025\n",
      "  âœ“ Appended HAL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HAL 25-Nov-2025\n",
      "  âœ“ Appended HAVELLS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HAVELLS 25-Nov-2025\n",
      "  âœ“ Appended HCLTECH 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HCLTECH 25-Nov-2025\n",
      "  âœ“ Appended HDFCAMC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCAMC 25-Nov-2025\n",
      "  âœ“ Appended HDFCBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCBANK 25-Nov-2025\n",
      "  âœ“ Appended HDFCLIFE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCLIFE 25-Nov-2025\n",
      "  âœ“ Appended HEROMOTOCO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HEROMOTOCO 25-Nov-2025\n",
      "  âœ“ Appended HFCL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HFCL 25-Nov-2025\n",
      "  âœ“ Appended HINDALCO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDALCO 25-Nov-2025\n",
      "  âœ“ Appended HINDPETRO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDPETRO 25-Nov-2025\n",
      "  âœ“ Appended HINDUNILVR 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDUNILVR 25-Nov-2025\n",
      "  âœ“ Appended HINDZINC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDZINC 25-Nov-2025\n",
      "  âœ“ Appended HUDCO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HUDCO 25-Nov-2025\n",
      "  âœ“ Appended ICICIBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIBANK 25-Nov-2025\n",
      "  âœ“ Appended ICICIGI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIGI 25-Nov-2025\n",
      "  âœ“ Appended ICICIPRULI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIPRULI 25-Nov-2025\n",
      "  âœ“ Appended IDEA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IDEA 25-Nov-2025\n",
      "  âœ“ Appended IDFCFIRSTB 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IDFCFIRSTB 25-Nov-2025\n",
      "  âœ“ Appended IEX 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IEX 25-Nov-2025\n",
      "  âœ“ Appended IGL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IGL 25-Nov-2025\n",
      "  âœ“ Appended IIFL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IIFL 25-Nov-2025\n",
      "  âœ“ Appended INDHOTEL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDHOTEL 25-Nov-2025\n",
      "  âœ“ Appended INDIANB 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDIANB 25-Nov-2025\n",
      "  âœ“ Appended INDIGO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDIGO 25-Nov-2025\n",
      "  âœ“ Appended INDUSINDBK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDUSINDBK 25-Nov-2025\n",
      "  âœ“ Appended INDUSTOWER 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDUSTOWER 25-Nov-2025\n",
      "  âœ“ Appended INFY 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INFY 25-Nov-2025\n",
      "  âœ“ Appended INOXWIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INOXWIND 25-Nov-2025\n",
      "  âœ“ Appended IOC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IOC 25-Nov-2025\n",
      "  âœ“ Appended IRCTC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IRCTC 25-Nov-2025\n",
      "  âœ“ Appended IREDA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IREDA 25-Nov-2025\n",
      "  âœ“ Appended IRFC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IRFC 25-Nov-2025\n",
      "  âœ“ Appended ITC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ITC 25-Nov-2025\n",
      "  âœ“ Appended JINDALSTEL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JINDALSTEL 25-Nov-2025\n",
      "  âœ“ Appended JIOFIN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JIOFIN 25-Nov-2025\n",
      "  âœ“ Appended JSWENERGY 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JSWENERGY 25-Nov-2025\n",
      "  âœ“ Appended JSWSTEEL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JSWSTEEL 25-Nov-2025\n",
      "  âœ“ Appended JUBLFOOD 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JUBLFOOD 25-Nov-2025\n",
      "  âœ“ Appended KALYANKJIL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KALYANKJIL 25-Nov-2025\n",
      "  âœ“ Appended KAYNES 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KAYNES 25-Nov-2025\n",
      "  âœ“ Appended KEI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KEI 25-Nov-2025\n",
      "  âœ“ Appended KFINTECH 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KFINTECH 25-Nov-2025\n",
      "  âœ“ Appended KOTAKBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KOTAKBANK 25-Nov-2025\n",
      "  âœ“ Appended KPITTECH 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KPITTECH 25-Nov-2025\n",
      "  âœ“ Appended LAURUSLABS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LAURUSLABS 25-Nov-2025\n",
      "  âœ“ Appended LICHSGFIN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LICHSGFIN 25-Nov-2025\n",
      "  âœ“ Appended LICI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LICI 25-Nov-2025\n",
      "  âœ“ Appended LODHA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LODHA 25-Nov-2025\n",
      "  âœ“ Appended LT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LT 25-Nov-2025\n",
      "  âœ“ Appended LTF 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LTF 25-Nov-2025\n",
      "  âœ“ Appended LTIM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LTIM 25-Nov-2025\n",
      "  âœ“ Appended LUPIN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LUPIN 25-Nov-2025\n",
      "  âœ“ Appended MANAPPURAM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MANAPPURAM 25-Nov-2025\n",
      "  âœ“ Appended MANKIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MANKIND 25-Nov-2025\n",
      "  âœ“ Appended MARICO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MARICO 25-Nov-2025\n",
      "  âœ“ Appended MARUTI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MARUTI 25-Nov-2025\n",
      "  âœ“ Appended MAXHEALTH 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MAXHEALTH 25-Nov-2025\n",
      "  âœ“ Appended MAZDOCK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MAZDOCK 25-Nov-2025\n",
      "  âœ“ Appended MCX 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MCX 25-Nov-2025\n",
      "  âœ“ Appended MFSL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MFSL 25-Nov-2025\n",
      "  âœ“ Appended MOTHERSON 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MOTHERSON 25-Nov-2025\n",
      "  âœ“ Appended MPHASIS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MPHASIS 25-Nov-2025\n",
      "  âœ“ Appended MUTHOOTFIN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MUTHOOTFIN 25-Nov-2025\n",
      "  âœ“ Appended NATIONALUM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NATIONALUM 25-Nov-2025\n",
      "  âœ“ Appended NAUKRI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NAUKRI 25-Nov-2025\n",
      "  âœ“ Appended NBCC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NBCC 25-Nov-2025\n",
      "  âœ“ Appended NCC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NCC 25-Nov-2025\n",
      "  âœ“ Appended NESTLEIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NESTLEIND 25-Nov-2025\n",
      "  âœ“ Appended NHPC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NHPC 25-Nov-2025\n",
      "  âœ“ Appended NMDC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NMDC 25-Nov-2025\n",
      "  âœ“ Appended NTPC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NTPC 25-Nov-2025\n",
      "  âœ“ Appended NUVAMA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NUVAMA 25-Nov-2025\n",
      "  âœ“ Appended NYKAA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NYKAA 25-Nov-2025\n",
      "  âœ“ Appended OBEROIRLTY 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for OBEROIRLTY 25-Nov-2025\n",
      "  âœ“ Appended OFSS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for OFSS 25-Nov-2025\n",
      "  âœ“ Appended OIL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for OIL 25-Nov-2025\n",
      "  âœ“ Appended ONGC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ONGC 25-Nov-2025\n",
      "  âœ“ Appended PAGEIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PAGEIND 25-Nov-2025\n",
      "  âœ“ Appended PATANJALI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PATANJALI 25-Nov-2025\n",
      "  âœ“ Appended PAYTM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PAYTM 25-Nov-2025\n",
      "  âœ“ Appended PERSISTENT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PERSISTENT 25-Nov-2025\n",
      "  âœ“ Appended PETRONET 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PETRONET 25-Nov-2025\n",
      "  âœ“ Appended PFC 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PFC 25-Nov-2025\n",
      "  âœ“ Appended PGEL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PGEL 25-Nov-2025\n",
      "  âœ“ Appended PHOENIXLTD 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PHOENIXLTD 25-Nov-2025\n",
      "  âœ“ Appended PIDILITIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PIDILITIND 25-Nov-2025\n",
      "  âœ“ Appended PIIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PIIND 25-Nov-2025\n",
      "  âœ“ Appended PNB 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PNB 25-Nov-2025\n",
      "  âœ“ Appended PNBHOUSING 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PNBHOUSING 25-Nov-2025\n",
      "  âœ“ Appended POLICYBZR 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for POLICYBZR 25-Nov-2025\n",
      "  âœ“ Appended POLYCAB 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for POLYCAB 25-Nov-2025\n",
      "  âœ“ Appended POWERGRID 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for POWERGRID 25-Nov-2025\n",
      "  âœ“ Appended PPLPHARMA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PPLPHARMA 25-Nov-2025\n",
      "  âœ“ Appended PRESTIGE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PRESTIGE 25-Nov-2025\n",
      "  âœ“ Appended RBLBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RBLBANK 25-Nov-2025\n",
      "  âœ“ Appended RECLTD 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RECLTD 25-Nov-2025\n",
      "  âœ“ Appended RELIANCE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RELIANCE 25-Nov-2025\n",
      "  âœ“ Appended RVNL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RVNL 25-Nov-2025\n",
      "  âœ“ Appended SAIL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SAIL 25-Nov-2025\n",
      "  âœ“ Appended SBICARD 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBICARD 25-Nov-2025\n",
      "  âœ“ Appended SBILIFE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBILIFE 25-Nov-2025\n",
      "  âœ“ Appended SBIN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBIN 25-Nov-2025\n",
      "  âœ“ Appended SHREECEM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SHREECEM 25-Nov-2025\n",
      "  âœ“ Appended SHRIRAMFIN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SHRIRAMFIN 25-Nov-2025\n",
      "  âœ“ Appended SIEMENS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SIEMENS 25-Nov-2025\n",
      "  âœ“ Appended SOLARINDS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SOLARINDS 25-Nov-2025\n",
      "  âœ“ Appended SONACOMS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SONACOMS 25-Nov-2025\n",
      "  âœ“ Appended SRF 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SRF 25-Nov-2025\n",
      "  âœ“ Appended SUNPHARMA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUNPHARMA 25-Nov-2025\n",
      "  âœ“ Appended SUPREMEIND 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUPREMEIND 25-Nov-2025\n",
      "  âœ“ Appended SUZLON 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUZLON 25-Nov-2025\n",
      "  âœ“ Appended SYNGENE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SYNGENE 25-Nov-2025\n",
      "  âœ“ Appended TATACONSUM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATACONSUM 25-Nov-2025\n",
      "  âœ“ Appended TATAELXSI 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATAELXSI 25-Nov-2025\n",
      "  âœ“ Appended TATAPOWER 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATAPOWER 25-Nov-2025\n",
      "  âœ“ Appended TATASTEEL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATASTEEL 25-Nov-2025\n",
      "  âœ“ Appended TATATECH 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATATECH 25-Nov-2025\n",
      "  âœ“ Appended TCS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TCS 25-Nov-2025\n",
      "  âœ“ Appended TECHM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TECHM 25-Nov-2025\n",
      "  âœ“ Appended TIINDIA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TIINDIA 25-Nov-2025\n",
      "  âœ“ Appended TITAGARH 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TITAGARH 25-Nov-2025\n",
      "  âœ“ Appended TITAN 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TITAN 25-Nov-2025\n",
      "  âœ“ Appended TORNTPHARM 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TORNTPHARM 25-Nov-2025\n",
      "  âœ“ Appended TORNTPOWER 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TORNTPOWER 25-Nov-2025\n",
      "  âœ“ Appended TRENT 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TRENT 25-Nov-2025\n",
      "  âœ“ Appended TVSMOTOR 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TVSMOTOR 25-Nov-2025\n",
      "  âœ“ Appended ULTRACEMCO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ULTRACEMCO 25-Nov-2025\n",
      "  âœ“ Appended UNIONBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNIONBANK 25-Nov-2025\n",
      "  âœ“ Appended UNITDSPR 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNITDSPR 25-Nov-2025\n",
      "  âœ“ Appended UNOMINDA 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNOMINDA 25-Nov-2025\n",
      "  âœ“ Appended UPL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UPL 25-Nov-2025\n",
      "  âœ“ Appended VBL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for VBL 25-Nov-2025\n",
      "  âœ“ Appended VEDL 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for VEDL 25-Nov-2025\n",
      "  âœ“ Appended VOLTAS 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for VOLTAS 25-Nov-2025\n",
      "  âœ“ Appended WIPRO 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for WIPRO 25-Nov-2025\n",
      "  âœ“ Appended YESBANK 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for YESBANK 25-Nov-2025\n",
      "  âœ“ Appended ZYDUSLIFE 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ZYDUSLIFE 25-Nov-2025\n",
      "\n",
      "ğŸ“… Processing expiry: 27-Jan-2026\n",
      "  âœ“ Appended 360ONE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for 360ONE 27-Jan-2026\n",
      "  âœ“ Appended ABB 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ABB 27-Jan-2026\n",
      "  âœ“ Appended ABCAPITAL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ABCAPITAL 27-Jan-2026\n",
      "  âœ“ Appended ADANIENSOL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIENSOL 27-Jan-2026\n",
      "  âœ“ Appended ADANIENT 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIENT 27-Jan-2026\n",
      "  âœ“ Appended ADANIGREEN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIGREEN 27-Jan-2026\n",
      "  âœ“ Appended ADANIPORTS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIPORTS 27-Jan-2026\n",
      "  âœ“ Appended AMBER 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for AMBER 27-Jan-2026\n",
      "  âœ“ Appended AMBUJACEM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for AMBUJACEM 27-Jan-2026\n",
      "  âœ“ Appended ANGELONE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ANGELONE 27-Jan-2026\n",
      "  âœ“ Appended APLAPOLLO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for APLAPOLLO 27-Jan-2026\n",
      "  âœ“ Appended APOLLOHOSP 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for APOLLOHOSP 27-Jan-2026\n",
      "  âœ“ Appended ASHOKLEY 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASHOKLEY 27-Jan-2026\n",
      "  âœ“ Appended ASIANPAINT 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASIANPAINT 27-Jan-2026\n",
      "  âœ“ Appended ASTRAL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASTRAL 27-Jan-2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Appended AUBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for AUBANK 27-Jan-2026\n",
      "  âœ“ Appended AUROPHARMA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for AUROPHARMA 27-Jan-2026\n",
      "  âœ“ Appended AXISBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for AXISBANK 27-Jan-2026\n",
      "  âœ“ Appended BAJAJ-AUTO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJAJ-AUTO 27-Jan-2026\n",
      "  âœ“ Appended BAJAJFINSV 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJAJFINSV 27-Jan-2026\n",
      "  âœ“ Appended BAJFINANCE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJFINANCE 27-Jan-2026\n",
      "  âœ“ Appended BANDHANBNK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANDHANBNK 27-Jan-2026\n",
      "  âœ“ Appended BANKBARODA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKBARODA 27-Jan-2026\n",
      "  âœ“ Appended BANKINDIA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKINDIA 27-Jan-2026\n",
      "  âœ“ Appended BDL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BDL 27-Jan-2026\n",
      "  âœ“ Appended BEL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BEL 27-Jan-2026\n",
      "  âœ“ Appended BHARATFORG 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHARATFORG 27-Jan-2026\n",
      "  âœ“ Appended BHARTIARTL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHARTIARTL 27-Jan-2026\n",
      "  âœ“ Appended BHEL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHEL 27-Jan-2026\n",
      "  âœ“ Appended BIOCON 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BIOCON 27-Jan-2026\n",
      "  âœ“ Appended BOSCHLTD 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BOSCHLTD 27-Jan-2026\n",
      "  âœ“ Appended BPCL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BPCL 27-Jan-2026\n",
      "  âœ“ Appended BRITANNIA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BRITANNIA 27-Jan-2026\n",
      "  âœ“ Appended BSE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BSE 27-Jan-2026\n",
      "  âœ“ Appended CAMS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CAMS 27-Jan-2026\n",
      "  âœ“ Appended CANBK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CANBK 27-Jan-2026\n",
      "  âœ“ Appended CDSL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CDSL 27-Jan-2026\n",
      "  âœ“ Appended CGPOWER 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CGPOWER 27-Jan-2026\n",
      "  âœ“ Appended CHOLAFIN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CHOLAFIN 27-Jan-2026\n",
      "  âœ“ Appended CIPLA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CIPLA 27-Jan-2026\n",
      "  âœ“ Appended COALINDIA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for COALINDIA 27-Jan-2026\n",
      "  âœ“ Appended COFORGE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for COFORGE 27-Jan-2026\n",
      "  âœ“ Appended COLPAL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for COLPAL 27-Jan-2026\n",
      "  âœ“ Appended CONCOR 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CONCOR 27-Jan-2026\n",
      "  âœ“ Appended CROMPTON 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CROMPTON 27-Jan-2026\n",
      "  âœ“ Appended CUMMINSIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for CUMMINSIND 27-Jan-2026\n",
      "  âœ“ Appended DABUR 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DABUR 27-Jan-2026\n",
      "  âœ“ Appended DALBHARAT 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DALBHARAT 27-Jan-2026\n",
      "  âœ“ Appended DELHIVERY 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DELHIVERY 27-Jan-2026\n",
      "  âœ“ Appended DIVISLAB 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DIVISLAB 27-Jan-2026\n",
      "  âœ“ Appended DIXON 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DIXON 27-Jan-2026\n",
      "  âœ“ Appended DLF 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DLF 27-Jan-2026\n",
      "  âœ“ Appended DMART 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DMART 27-Jan-2026\n",
      "  âœ“ Appended DRREDDY 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for DRREDDY 27-Jan-2026\n",
      "  âœ“ Appended EICHERMOT 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for EICHERMOT 27-Jan-2026\n",
      "  âœ“ Appended ETERNAL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ETERNAL 27-Jan-2026\n",
      "  âœ“ Appended EXIDEIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for EXIDEIND 27-Jan-2026\n",
      "  âœ“ Appended FEDERALBNK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for FEDERALBNK 27-Jan-2026\n",
      "  âœ“ Appended FORTIS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for FORTIS 27-Jan-2026\n",
      "  âœ“ Appended GAIL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for GAIL 27-Jan-2026\n",
      "  âœ“ Appended GLENMARK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for GLENMARK 27-Jan-2026\n",
      "  âœ“ Appended GMRAIRPORT 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for GMRAIRPORT 27-Jan-2026\n",
      "  âœ“ Appended GODREJCP 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for GODREJCP 27-Jan-2026\n",
      "  âœ“ Appended GODREJPROP 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for GODREJPROP 27-Jan-2026\n",
      "  âœ“ Appended GRASIM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for GRASIM 27-Jan-2026\n",
      "  âœ“ Appended HAL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HAL 27-Jan-2026\n",
      "  âœ“ Appended HAVELLS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HAVELLS 27-Jan-2026\n",
      "  âœ“ Appended HCLTECH 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HCLTECH 27-Jan-2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Appended HDFCAMC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCAMC 27-Jan-2026\n",
      "  âœ“ Appended HDFCBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCBANK 27-Jan-2026\n",
      "  âœ“ Appended HDFCLIFE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCLIFE 27-Jan-2026\n",
      "  âœ“ Appended HEROMOTOCO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HEROMOTOCO 27-Jan-2026\n",
      "  âœ“ Appended HINDALCO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDALCO 27-Jan-2026\n",
      "  âœ“ Appended HINDPETRO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDPETRO 27-Jan-2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Appended HINDUNILVR 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDUNILVR 27-Jan-2026\n",
      "  âœ“ Appended HINDZINC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDZINC 27-Jan-2026\n",
      "  âœ“ Appended HUDCO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for HUDCO 27-Jan-2026\n",
      "  âœ“ Appended ICICIBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIBANK 27-Jan-2026\n",
      "  âœ“ Appended ICICIGI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIGI 27-Jan-2026\n",
      "  âœ“ Appended ICICIPRULI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIPRULI 27-Jan-2026\n",
      "  âœ“ Appended IDEA 27-Jan-2026 to master DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Saved aggregate metrics for IDEA 27-Jan-2026\n",
      "  âœ“ Appended IDFCFIRSTB 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for IDFCFIRSTB 27-Jan-2026\n",
      "  âœ“ Appended IEX 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for IEX 27-Jan-2026\n",
      "  âœ“ Appended IIFL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for IIFL 27-Jan-2026\n",
      "  âœ“ Appended INDHOTEL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDHOTEL 27-Jan-2026\n",
      "  âœ“ Appended INDIANB 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDIANB 27-Jan-2026\n",
      "  âœ“ Appended INDIGO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDIGO 27-Jan-2026\n",
      "  âœ“ Appended INDUSINDBK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDUSINDBK 27-Jan-2026\n",
      "  âœ“ Appended INDUSTOWER 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDUSTOWER 27-Jan-2026\n",
      "  âœ“ Appended INFY 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for INFY 27-Jan-2026\n",
      "  âœ“ Appended INOXWIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for INOXWIND 27-Jan-2026\n",
      "  âœ“ Appended IOC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for IOC 27-Jan-2026\n",
      "  âœ“ Appended IRCTC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for IRCTC 27-Jan-2026\n",
      "  âœ“ Appended IREDA 27-Jan-2026 to master DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Saved aggregate metrics for IREDA 27-Jan-2026\n",
      "  âœ“ Appended IRFC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for IRFC 27-Jan-2026\n",
      "  âœ“ Appended ITC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ITC 27-Jan-2026\n",
      "  âœ“ Appended JINDALSTEL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for JINDALSTEL 27-Jan-2026\n",
      "  âœ“ Appended JIOFIN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for JIOFIN 27-Jan-2026\n",
      "  âœ“ Appended JSWENERGY 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for JSWENERGY 27-Jan-2026\n",
      "  âœ“ Appended JSWSTEEL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for JSWSTEEL 27-Jan-2026\n",
      "  âœ“ Appended JUBLFOOD 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for JUBLFOOD 27-Jan-2026\n",
      "  âœ“ Appended KALYANKJIL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for KALYANKJIL 27-Jan-2026\n",
      "  âœ“ Appended KAYNES 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for KAYNES 27-Jan-2026\n",
      "  âœ“ Appended KEI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for KEI 27-Jan-2026\n",
      "  âœ“ Appended KFINTECH 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for KFINTECH 27-Jan-2026\n",
      "  âœ“ Appended KOTAKBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for KOTAKBANK 27-Jan-2026\n",
      "  âœ“ Appended KPITTECH 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for KPITTECH 27-Jan-2026\n",
      "  âœ“ Appended LAURUSLABS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LAURUSLABS 27-Jan-2026\n",
      "  âœ“ Appended LICHSGFIN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LICHSGFIN 27-Jan-2026\n",
      "  âœ“ Appended LICI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LICI 27-Jan-2026\n",
      "  âœ“ Appended LODHA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LODHA 27-Jan-2026\n",
      "  âœ“ Appended LT 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LT 27-Jan-2026\n",
      "  âœ“ Appended LTF 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LTF 27-Jan-2026\n",
      "  âœ“ Appended LTIM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LTIM 27-Jan-2026\n",
      "  âœ“ Appended LUPIN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for LUPIN 27-Jan-2026\n",
      "  âœ“ Appended MANAPPURAM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MANAPPURAM 27-Jan-2026\n",
      "  âœ“ Appended MANKIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MANKIND 27-Jan-2026\n",
      "  âœ“ Appended MARICO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MARICO 27-Jan-2026\n",
      "  âœ“ Appended MARUTI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MARUTI 27-Jan-2026\n",
      "  âœ“ Appended MAXHEALTH 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MAXHEALTH 27-Jan-2026\n",
      "  âœ“ Appended MAZDOCK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MAZDOCK 27-Jan-2026\n",
      "  âœ“ Appended MCX 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MCX 27-Jan-2026\n",
      "  âœ“ Appended MFSL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MFSL 27-Jan-2026\n",
      "  âœ“ Appended MOTHERSON 27-Jan-2026 to master DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Saved aggregate metrics for MOTHERSON 27-Jan-2026\n",
      "  âœ“ Appended MPHASIS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MPHASIS 27-Jan-2026\n",
      "  âœ“ Appended MUTHOOTFIN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for MUTHOOTFIN 27-Jan-2026\n",
      "  âœ“ Appended NATIONALUM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NATIONALUM 27-Jan-2026\n",
      "  âœ“ Appended NAUKRI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NAUKRI 27-Jan-2026\n",
      "  âœ“ Appended NBCC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NBCC 27-Jan-2026\n",
      "  âœ“ Appended NESTLEIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NESTLEIND 27-Jan-2026\n",
      "  âœ“ Appended NHPC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NHPC 27-Jan-2026\n",
      "  âœ“ Appended NMDC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NMDC 27-Jan-2026\n",
      "  âœ“ Appended NTPC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NTPC 27-Jan-2026\n",
      "  âœ“ Appended NUVAMA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NUVAMA 27-Jan-2026\n",
      "  âœ“ Appended NYKAA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for NYKAA 27-Jan-2026\n",
      "  âœ“ Appended OBEROIRLTY 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for OBEROIRLTY 27-Jan-2026\n",
      "  âœ“ Appended OFSS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for OFSS 27-Jan-2026\n",
      "  âœ“ Appended OIL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for OIL 27-Jan-2026\n",
      "  âœ“ Appended ONGC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ONGC 27-Jan-2026\n",
      "  âœ“ Appended PAGEIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PAGEIND 27-Jan-2026\n",
      "  âœ“ Appended PATANJALI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PATANJALI 27-Jan-2026\n",
      "  âœ“ Appended PAYTM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PAYTM 27-Jan-2026\n",
      "  âœ“ Appended PERSISTENT 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PERSISTENT 27-Jan-2026\n",
      "  âœ“ Appended PETRONET 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PETRONET 27-Jan-2026\n",
      "  âœ“ Appended PFC 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PFC 27-Jan-2026\n",
      "  âœ“ Appended PGEL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PGEL 27-Jan-2026\n",
      "  âœ“ Appended PHOENIXLTD 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PHOENIXLTD 27-Jan-2026\n",
      "  âœ“ Appended PIDILITIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PIDILITIND 27-Jan-2026\n",
      "  âœ“ Appended PIIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PIIND 27-Jan-2026\n",
      "  âœ“ Appended PNB 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PNB 27-Jan-2026\n",
      "  âœ“ Appended PNBHOUSING 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PNBHOUSING 27-Jan-2026\n",
      "  âœ“ Appended POLICYBZR 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for POLICYBZR 27-Jan-2026\n",
      "  âœ“ Appended POWERGRID 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for POWERGRID 27-Jan-2026\n",
      "  âœ“ Appended PPLPHARMA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PPLPHARMA 27-Jan-2026\n",
      "  âœ“ Appended PRESTIGE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for PRESTIGE 27-Jan-2026\n",
      "  âœ“ Appended RBLBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for RBLBANK 27-Jan-2026\n",
      "  âœ“ Appended RECLTD 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for RECLTD 27-Jan-2026\n",
      "  âœ“ Appended RELIANCE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for RELIANCE 27-Jan-2026\n",
      "  âœ“ Appended RVNL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for RVNL 27-Jan-2026\n",
      "  âœ“ Appended SAIL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SAIL 27-Jan-2026\n",
      "  âœ“ Appended SBICARD 27-Jan-2026 to master DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Saved aggregate metrics for SBICARD 27-Jan-2026\n",
      "  âœ“ Appended SBILIFE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBILIFE 27-Jan-2026\n",
      "  âœ“ Appended SBIN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBIN 27-Jan-2026\n",
      "  âœ“ Appended SHREECEM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SHREECEM 27-Jan-2026\n",
      "  âœ“ Appended SHRIRAMFIN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SHRIRAMFIN 27-Jan-2026\n",
      "  âœ“ Appended SIEMENS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SIEMENS 27-Jan-2026\n",
      "  âœ“ Appended SOLARINDS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SOLARINDS 27-Jan-2026\n",
      "  âœ“ Appended SONACOMS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SONACOMS 27-Jan-2026\n",
      "  âœ“ Appended SRF 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SRF 27-Jan-2026\n",
      "  âœ“ Appended SUNPHARMA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUNPHARMA 27-Jan-2026\n",
      "  âœ“ Appended SUPREMEIND 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUPREMEIND 27-Jan-2026\n",
      "  âœ“ Appended SUZLON 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUZLON 27-Jan-2026\n",
      "  âœ“ Appended SYNGENE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for SYNGENE 27-Jan-2026\n",
      "  âœ“ Appended TATACONSUM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATACONSUM 27-Jan-2026\n",
      "  âœ“ Appended TATAELXSI 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATAELXSI 27-Jan-2026\n",
      "  âœ“ Appended TATAPOWER 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATAPOWER 27-Jan-2026\n",
      "  âœ“ Appended TATASTEEL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATASTEEL 27-Jan-2026\n",
      "  âœ“ Appended TATATECH 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATATECH 27-Jan-2026\n",
      "  âœ“ Appended TCS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TCS 27-Jan-2026\n",
      "  âœ“ Appended TECHM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TECHM 27-Jan-2026\n",
      "  âœ“ Appended TIINDIA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TIINDIA 27-Jan-2026\n",
      "  âœ“ Appended TITAN 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TITAN 27-Jan-2026\n",
      "  âœ“ Appended TORNTPHARM 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TORNTPHARM 27-Jan-2026\n",
      "  âœ“ Appended TORNTPOWER 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TORNTPOWER 27-Jan-2026\n",
      "  âœ“ Appended TRENT 27-Jan-2026 to master DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Saved aggregate metrics for TRENT 27-Jan-2026\n",
      "  âœ“ Appended TVSMOTOR 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for TVSMOTOR 27-Jan-2026\n",
      "  âœ“ Appended ULTRACEMCO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ULTRACEMCO 27-Jan-2026\n",
      "  âœ“ Appended UNIONBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNIONBANK 27-Jan-2026\n",
      "  âœ“ Appended UNITDSPR 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNITDSPR 27-Jan-2026\n",
      "  âœ“ Appended UNOMINDA 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNOMINDA 27-Jan-2026\n",
      "  âœ“ Appended UPL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for UPL 27-Jan-2026\n",
      "  âœ“ Appended VBL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for VBL 27-Jan-2026\n",
      "  âœ“ Appended VEDL 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for VEDL 27-Jan-2026\n",
      "  âœ“ Appended VOLTAS 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for VOLTAS 27-Jan-2026\n",
      "  âœ“ Appended WIPRO 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for WIPRO 27-Jan-2026\n",
      "  âœ“ Appended YESBANK 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for YESBANK 27-Jan-2026\n",
      "  âœ“ Appended ZYDUSLIFE 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for ZYDUSLIFE 27-Jan-2026\n",
      "\n",
      "ğŸ“… Processing expiry: 30-Dec-2025\n",
      "  âœ“ Appended 360ONE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for 360ONE 30-Dec-2025\n",
      "  âœ“ Appended ABB 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ABB 30-Dec-2025\n",
      "  âœ“ Appended ABCAPITAL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ABCAPITAL 30-Dec-2025\n",
      "  âœ“ Appended ADANIENSOL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIENSOL 30-Dec-2025\n",
      "  âœ“ Appended ADANIENT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIENT 30-Dec-2025\n",
      "  âœ“ Appended ADANIGREEN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIGREEN 30-Dec-2025\n",
      "  âœ“ Appended ADANIPORTS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ADANIPORTS 30-Dec-2025\n",
      "  âœ“ Appended ALKEM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ALKEM 30-Dec-2025\n",
      "  âœ“ Appended AMBER 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AMBER 30-Dec-2025\n",
      "  âœ“ Appended AMBUJACEM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AMBUJACEM 30-Dec-2025\n",
      "  âœ“ Appended ANGELONE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ANGELONE 30-Dec-2025\n",
      "  âœ“ Appended APLAPOLLO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for APLAPOLLO 30-Dec-2025\n",
      "  âœ“ Appended APOLLOHOSP 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for APOLLOHOSP 30-Dec-2025\n",
      "  âœ“ Appended ASHOKLEY 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASHOKLEY 30-Dec-2025\n",
      "  âœ“ Appended ASIANPAINT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASIANPAINT 30-Dec-2025\n",
      "  âœ“ Appended ASTRAL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ASTRAL 30-Dec-2025\n",
      "  âœ“ Appended AUBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AUBANK 30-Dec-2025\n",
      "  âœ“ Appended AUROPHARMA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AUROPHARMA 30-Dec-2025\n",
      "  âœ“ Appended AXISBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for AXISBANK 30-Dec-2025\n",
      "  âœ“ Appended BAJAJ-AUTO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJAJ-AUTO 30-Dec-2025\n",
      "  âœ“ Appended BAJAJFINSV 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJAJFINSV 30-Dec-2025\n",
      "  âœ“ Appended BAJFINANCE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BAJFINANCE 30-Dec-2025\n",
      "  âœ“ Appended BANDHANBNK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANDHANBNK 30-Dec-2025\n",
      "  âœ“ Appended BANKBARODA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKBARODA 30-Dec-2025\n",
      "  âœ“ Appended BANKINDIA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKINDIA 30-Dec-2025\n",
      "  âœ“ Appended BDL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BDL 30-Dec-2025\n",
      "  âœ“ Appended BEL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BEL 30-Dec-2025\n",
      "  âœ“ Appended BHARATFORG 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHARATFORG 30-Dec-2025\n",
      "  âœ“ Appended BHARTIARTL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHARTIARTL 30-Dec-2025\n",
      "  âœ“ Appended BHEL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BHEL 30-Dec-2025\n",
      "  âœ“ Appended BIOCON 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BIOCON 30-Dec-2025\n",
      "  âœ“ Appended BLUESTARCO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BLUESTARCO 30-Dec-2025\n",
      "  âœ“ Appended BOSCHLTD 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BOSCHLTD 30-Dec-2025\n",
      "  âœ“ Appended BPCL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BPCL 30-Dec-2025\n",
      "  âœ“ Appended BRITANNIA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BRITANNIA 30-Dec-2025\n",
      "  âœ“ Appended BSE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BSE 30-Dec-2025\n",
      "  âœ“ Appended CAMS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CAMS 30-Dec-2025\n",
      "  âœ“ Appended CANBK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CANBK 30-Dec-2025\n",
      "  âœ“ Appended CDSL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CDSL 30-Dec-2025\n",
      "  âœ“ Appended CGPOWER 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CGPOWER 30-Dec-2025\n",
      "  âœ“ Appended CHOLAFIN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CHOLAFIN 30-Dec-2025\n",
      "  âœ“ Appended CIPLA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CIPLA 30-Dec-2025\n",
      "  âœ“ Appended COALINDIA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for COALINDIA 30-Dec-2025\n",
      "  âœ“ Appended COFORGE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for COFORGE 30-Dec-2025\n",
      "  âœ“ Appended COLPAL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for COLPAL 30-Dec-2025\n",
      "  âœ“ Appended CONCOR 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CONCOR 30-Dec-2025\n",
      "  âœ“ Appended CROMPTON 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CROMPTON 30-Dec-2025\n",
      "  âœ“ Appended CUMMINSIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CUMMINSIND 30-Dec-2025\n",
      "  âœ“ Appended CYIENT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for CYIENT 30-Dec-2025\n",
      "  âœ“ Appended DABUR 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DABUR 30-Dec-2025\n",
      "  âœ“ Appended DALBHARAT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DALBHARAT 30-Dec-2025\n",
      "  âœ“ Appended DELHIVERY 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DELHIVERY 30-Dec-2025\n",
      "  âœ“ Appended DIVISLAB 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DIVISLAB 30-Dec-2025\n",
      "  âœ“ Appended DIXON 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DIXON 30-Dec-2025\n",
      "  âœ“ Appended DLF 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DLF 30-Dec-2025\n",
      "  âœ“ Appended DMART 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DMART 30-Dec-2025\n",
      "  âœ“ Appended DRREDDY 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for DRREDDY 30-Dec-2025\n",
      "  âœ“ Appended EICHERMOT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for EICHERMOT 30-Dec-2025\n",
      "  âœ“ Appended ETERNAL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ETERNAL 30-Dec-2025\n",
      "  âœ“ Appended EXIDEIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for EXIDEIND 30-Dec-2025\n",
      "  âœ“ Appended FEDERALBNK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for FEDERALBNK 30-Dec-2025\n",
      "  âœ“ Appended FORTIS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for FORTIS 30-Dec-2025\n",
      "  âœ“ Appended GAIL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GAIL 30-Dec-2025\n",
      "  âœ“ Appended GLENMARK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GLENMARK 30-Dec-2025\n",
      "  âœ“ Appended GMRAIRPORT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GMRAIRPORT 30-Dec-2025\n",
      "  âœ“ Appended GODREJCP 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GODREJCP 30-Dec-2025\n",
      "  âœ“ Appended GODREJPROP 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GODREJPROP 30-Dec-2025\n",
      "  âœ“ Appended GRASIM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for GRASIM 30-Dec-2025\n",
      "  âœ“ Appended HAL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HAL 30-Dec-2025\n",
      "  âœ“ Appended HAVELLS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HAVELLS 30-Dec-2025\n",
      "  âœ“ Appended HCLTECH 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HCLTECH 30-Dec-2025\n",
      "  âœ“ Appended HDFCAMC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCAMC 30-Dec-2025\n",
      "  âœ“ Appended HDFCBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCBANK 30-Dec-2025\n",
      "  âœ“ Appended HDFCLIFE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HDFCLIFE 30-Dec-2025\n",
      "  âœ“ Appended HEROMOTOCO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HEROMOTOCO 30-Dec-2025\n",
      "  âœ“ Appended HFCL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HFCL 30-Dec-2025\n",
      "  âœ“ Appended HINDALCO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDALCO 30-Dec-2025\n",
      "  âœ“ Appended HINDPETRO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDPETRO 30-Dec-2025\n",
      "  âœ“ Appended HINDUNILVR 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDUNILVR 30-Dec-2025\n",
      "  âœ“ Appended HINDZINC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HINDZINC 30-Dec-2025\n",
      "  âœ“ Appended HUDCO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for HUDCO 30-Dec-2025\n",
      "  âœ“ Appended ICICIBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIBANK 30-Dec-2025\n",
      "  âœ“ Appended ICICIGI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIGI 30-Dec-2025\n",
      "  âœ“ Appended ICICIPRULI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ICICIPRULI 30-Dec-2025\n",
      "  âœ“ Appended IDEA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IDEA 30-Dec-2025\n",
      "  âœ“ Appended IDFCFIRSTB 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IDFCFIRSTB 30-Dec-2025\n",
      "  âœ“ Appended IEX 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IEX 30-Dec-2025\n",
      "  âœ“ Appended IIFL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IIFL 30-Dec-2025\n",
      "  âœ“ Appended INDHOTEL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDHOTEL 30-Dec-2025\n",
      "  âœ“ Appended INDIANB 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDIANB 30-Dec-2025\n",
      "  âœ“ Appended INDIGO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDIGO 30-Dec-2025\n",
      "  âœ“ Appended INDUSINDBK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDUSINDBK 30-Dec-2025\n",
      "  âœ“ Appended INDUSTOWER 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INDUSTOWER 30-Dec-2025\n",
      "  âœ“ Appended INFY 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INFY 30-Dec-2025\n",
      "  âœ“ Appended INOXWIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for INOXWIND 30-Dec-2025\n",
      "  âœ“ Appended IOC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IOC 30-Dec-2025\n",
      "  âœ“ Appended IRCTC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IRCTC 30-Dec-2025\n",
      "  âœ“ Appended IREDA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IREDA 30-Dec-2025\n",
      "  âœ“ Appended IRFC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for IRFC 30-Dec-2025\n",
      "  âœ“ Appended ITC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ITC 30-Dec-2025\n",
      "  âœ“ Appended JINDALSTEL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JINDALSTEL 30-Dec-2025\n",
      "  âœ“ Appended JIOFIN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JIOFIN 30-Dec-2025\n",
      "  âœ“ Appended JSWENERGY 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JSWENERGY 30-Dec-2025\n",
      "  âœ“ Appended JSWSTEEL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JSWSTEEL 30-Dec-2025\n",
      "  âœ“ Appended JUBLFOOD 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for JUBLFOOD 30-Dec-2025\n",
      "  âœ“ Appended KALYANKJIL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KALYANKJIL 30-Dec-2025\n",
      "  âœ“ Appended KAYNES 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KAYNES 30-Dec-2025\n",
      "  âœ“ Appended KEI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KEI 30-Dec-2025\n",
      "  âœ“ Appended KFINTECH 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KFINTECH 30-Dec-2025\n",
      "  âœ“ Appended KOTAKBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KOTAKBANK 30-Dec-2025\n",
      "  âœ“ Appended KPITTECH 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for KPITTECH 30-Dec-2025\n",
      "  âœ“ Appended LAURUSLABS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LAURUSLABS 30-Dec-2025\n",
      "  âœ“ Appended LICHSGFIN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LICHSGFIN 30-Dec-2025\n",
      "  âœ“ Appended LICI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LICI 30-Dec-2025\n",
      "  âœ“ Appended LODHA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LODHA 30-Dec-2025\n",
      "  âœ“ Appended LT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LT 30-Dec-2025\n",
      "  âœ“ Appended LTF 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LTF 30-Dec-2025\n",
      "  âœ“ Appended LTIM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LTIM 30-Dec-2025\n",
      "  âœ“ Appended LUPIN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for LUPIN 30-Dec-2025\n",
      "  âœ“ Appended MANAPPURAM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MANAPPURAM 30-Dec-2025\n",
      "  âœ“ Appended MANKIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MANKIND 30-Dec-2025\n",
      "  âœ“ Appended MARICO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MARICO 30-Dec-2025\n",
      "  âœ“ Appended MARUTI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MARUTI 30-Dec-2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Appended MAXHEALTH 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MAXHEALTH 30-Dec-2025\n",
      "  âœ“ Appended MAZDOCK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MAZDOCK 30-Dec-2025\n",
      "  âœ“ Appended MCX 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MCX 30-Dec-2025\n",
      "  âœ“ Appended MFSL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MFSL 30-Dec-2025\n",
      "  âœ“ Appended MOTHERSON 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MOTHERSON 30-Dec-2025\n",
      "  âœ“ Appended MPHASIS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MPHASIS 30-Dec-2025\n",
      "  âœ“ Appended MUTHOOTFIN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for MUTHOOTFIN 30-Dec-2025\n",
      "  âœ“ Appended NATIONALUM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NATIONALUM 30-Dec-2025\n",
      "  âœ“ Appended NAUKRI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NAUKRI 30-Dec-2025\n",
      "  âœ“ Appended NBCC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NBCC 30-Dec-2025\n",
      "  âœ“ Appended NCC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NCC 30-Dec-2025\n",
      "  âœ“ Appended NESTLEIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NESTLEIND 30-Dec-2025\n",
      "  âœ“ Appended NHPC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NHPC 30-Dec-2025\n",
      "  âœ“ Appended NMDC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NMDC 30-Dec-2025\n",
      "  âœ“ Appended NTPC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NTPC 30-Dec-2025\n",
      "  âœ“ Appended NUVAMA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NUVAMA 30-Dec-2025\n",
      "  âœ“ Appended NYKAA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NYKAA 30-Dec-2025\n",
      "  âœ“ Appended OBEROIRLTY 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for OBEROIRLTY 30-Dec-2025\n",
      "  âœ“ Appended OFSS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for OFSS 30-Dec-2025\n",
      "  âœ“ Appended OIL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for OIL 30-Dec-2025\n",
      "  âœ“ Appended ONGC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ONGC 30-Dec-2025\n",
      "  âœ“ Appended PAGEIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PAGEIND 30-Dec-2025\n",
      "  âœ“ Appended PATANJALI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PATANJALI 30-Dec-2025\n",
      "  âœ“ Appended PAYTM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PAYTM 30-Dec-2025\n",
      "  âœ“ Appended PERSISTENT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PERSISTENT 30-Dec-2025\n",
      "  âœ“ Appended PETRONET 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PETRONET 30-Dec-2025\n",
      "  âœ“ Appended PFC 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PFC 30-Dec-2025\n",
      "  âœ“ Appended PGEL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PGEL 30-Dec-2025\n",
      "  âœ“ Appended PHOENIXLTD 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PHOENIXLTD 30-Dec-2025\n",
      "  âœ“ Appended PIDILITIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PIDILITIND 30-Dec-2025\n",
      "  âœ“ Appended PIIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PIIND 30-Dec-2025\n",
      "  âœ“ Appended PNB 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PNB 30-Dec-2025\n",
      "  âœ“ Appended PNBHOUSING 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PNBHOUSING 30-Dec-2025\n",
      "  âœ“ Appended POLICYBZR 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for POLICYBZR 30-Dec-2025\n",
      "  âœ“ Appended POLYCAB 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for POLYCAB 30-Dec-2025\n",
      "  âœ“ Appended POWERGRID 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for POWERGRID 30-Dec-2025\n",
      "  âœ“ Appended PPLPHARMA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PPLPHARMA 30-Dec-2025\n",
      "  âœ“ Appended PRESTIGE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for PRESTIGE 30-Dec-2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\221194803.py:282: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined = pd.concat([existing, metrics_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ“ Appended RBLBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RBLBANK 30-Dec-2025\n",
      "  âœ“ Appended RECLTD 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RECLTD 30-Dec-2025\n",
      "  âœ“ Appended RELIANCE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RELIANCE 30-Dec-2025\n",
      "  âœ“ Appended RVNL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for RVNL 30-Dec-2025\n",
      "  âœ“ Appended SAIL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SAIL 30-Dec-2025\n",
      "  âœ“ Appended SBICARD 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBICARD 30-Dec-2025\n",
      "  âœ“ Appended SBILIFE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBILIFE 30-Dec-2025\n",
      "  âœ“ Appended SBIN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SBIN 30-Dec-2025\n",
      "  âœ“ Appended SHREECEM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SHREECEM 30-Dec-2025\n",
      "  âœ“ Appended SHRIRAMFIN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SHRIRAMFIN 30-Dec-2025\n",
      "  âœ“ Appended SIEMENS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SIEMENS 30-Dec-2025\n",
      "  âœ“ Appended SOLARINDS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SOLARINDS 30-Dec-2025\n",
      "  âœ“ Appended SONACOMS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SONACOMS 30-Dec-2025\n",
      "  âœ“ Appended SRF 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SRF 30-Dec-2025\n",
      "  âœ“ Appended SUNPHARMA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUNPHARMA 30-Dec-2025\n",
      "  âœ“ Appended SUPREMEIND 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUPREMEIND 30-Dec-2025\n",
      "  âœ“ Appended SUZLON 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SUZLON 30-Dec-2025\n",
      "  âœ“ Appended SYNGENE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for SYNGENE 30-Dec-2025\n",
      "  âœ“ Appended TATACONSUM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATACONSUM 30-Dec-2025\n",
      "  âœ“ Appended TATAELXSI 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATAELXSI 30-Dec-2025\n",
      "  âœ“ Appended TATAPOWER 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATAPOWER 30-Dec-2025\n",
      "  âœ“ Appended TATASTEEL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATASTEEL 30-Dec-2025\n",
      "  âœ“ Appended TATATECH 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TATATECH 30-Dec-2025\n",
      "  âœ“ Appended TCS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TCS 30-Dec-2025\n",
      "  âœ“ Appended TECHM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TECHM 30-Dec-2025\n",
      "  âœ“ Appended TIINDIA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TIINDIA 30-Dec-2025\n",
      "  âœ“ Appended TITAGARH 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TITAGARH 30-Dec-2025\n",
      "  âœ“ Appended TITAN 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TITAN 30-Dec-2025\n",
      "  âœ“ Appended TORNTPHARM 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TORNTPHARM 30-Dec-2025\n",
      "  âœ“ Appended TORNTPOWER 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TORNTPOWER 30-Dec-2025\n",
      "  âœ“ Appended TRENT 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TRENT 30-Dec-2025\n",
      "  âœ“ Appended TVSMOTOR 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for TVSMOTOR 30-Dec-2025\n",
      "  âœ“ Appended ULTRACEMCO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ULTRACEMCO 30-Dec-2025\n",
      "  âœ“ Appended UNIONBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNIONBANK 30-Dec-2025\n",
      "  âœ“ Appended UNITDSPR 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNITDSPR 30-Dec-2025\n",
      "  âœ“ Appended UNOMINDA 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UNOMINDA 30-Dec-2025\n",
      "  âœ“ Appended UPL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for UPL 30-Dec-2025\n",
      "  âœ“ Appended VBL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for VBL 30-Dec-2025\n",
      "  âœ“ Appended VEDL 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for VEDL 30-Dec-2025\n",
      "  âœ“ Appended VOLTAS 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for VOLTAS 30-Dec-2025\n",
      "  âœ“ Appended WIPRO 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for WIPRO 30-Dec-2025\n",
      "  âœ“ Appended YESBANK 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for YESBANK 30-Dec-2025\n",
      "  âœ“ Appended ZYDUSLIFE 30-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for ZYDUSLIFE 30-Dec-2025\n",
      "\n",
      "\n",
      "ğŸ“Š Processing indices...\n",
      "  âœ“ Appended NIFTY 02-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NIFTY 02-Dec-2025\n",
      "  âœ“ Processed NIFTY 02-Dec-2025\n",
      "  âœ“ Appended NIFTY 09-Dec-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NIFTY 09-Dec-2025\n",
      "  âœ“ Processed NIFTY 09-Dec-2025\n",
      "  âœ“ Appended NIFTY 11-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for NIFTY 11-Nov-2025\n",
      "  âœ“ Processed NIFTY 11-Nov-2025\n",
      "  âœ“ Appended BANKNIFTY 25-Nov-2025 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKNIFTY 25-Nov-2025\n",
      "  âœ“ Processed BANKNIFTY 25-Nov-2025\n",
      "  âœ“ Appended BANKNIFTY 27-Jan-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKNIFTY 27-Jan-2026\n",
      "  âœ“ Processed BANKNIFTY 27-Jan-2026\n",
      "  âœ“ Appended BANKNIFTY 29-Sep-2026 to master DB\n",
      "  âœ“ Saved aggregate metrics for BANKNIFTY 29-Sep-2026\n",
      "  âœ“ Processed BANKNIFTY 29-Sep-2026\n",
      "\n",
      "======================================================================\n",
      "âœ… DAILY COLLECTION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "  â€¢ Successfully processed: 612\n",
      "  â€¢ Errors: 0\n",
      "  â€¢ Database location: C:\\Users\\sarda\\Desktop\\nse_options_historical_db\n",
      "  â€¢ Total historical files: 612\n",
      "  â€¢ Days of historical data: 3\n",
      "\n",
      "ğŸ’¡ Keep collecting! You need at least 5 days for ML features.\n",
      "   Currently: 3/5 days\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ğŸ’¾ Next Steps:\n",
      "1. Run this cell daily to build your database\n",
      "2. After 5+ days, run Cell 7 (Advanced Analysis)\n",
      "3. After 10+ days, run Cell 8 (ML Predictions)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CELL 6: Daily Historical Data Builder & Appender\n",
    "\n",
    "Run this EVERY DAY after market close (post 3:30 PM IST)\n",
    "\n",
    "Features:\n",
    "1. Collects today's data (PCR, Max Pain, OI, IV, Greeks)\n",
    "2. Appends to master historical database\n",
    "3. Calculates day-over-day changes\n",
    "4. Builds patterns database for ML\n",
    "5. Tracks actual vs predicted outcomes\n",
    "\n",
    "This is your GOLD MINE - the longer you run this, the better your predictions!\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "BASE_DIR = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"nse_options_analysis\")\n",
    "HISTORICAL_DB = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"nse_options_historical_db\")\n",
    "MASTER_DB = os.path.join(HISTORICAL_DB, \"master_database\")\n",
    "PATTERNS_DB = os.path.join(HISTORICAL_DB, \"patterns\")\n",
    "OUTCOMES_DB = os.path.join(HISTORICAL_DB, \"outcomes\")\n",
    "\n",
    "# Create directories\n",
    "for path in [HISTORICAL_DB, MASTER_DB, PATTERNS_DB, OUTCOMES_DB]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "TODAY = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "YESTERDAY = (datetime.now() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“Š DAILY HISTORICAL DATA BUILDER\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nğŸ“… Date: {TODAY}\")\n",
    "print(f\"ğŸ’¾ Database: {HISTORICAL_DB}\\n\")\n",
    "\n",
    "# ==========================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================\n",
    "def safe_float(val, default=0):\n",
    "    \"\"\"Safely convert to float\"\"\"\n",
    "    try:\n",
    "        if val is None or pd.isna(val):\n",
    "            return default\n",
    "        return float(val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def calculate_greeks_proxy(row, df, cmp):\n",
    "    \"\"\"\n",
    "    Calculate approximate Greeks using Black-Scholes approximations\n",
    "    Note: These are PROXIES - not exact, but good enough for analysis\n",
    "    \"\"\"\n",
    "    strike = row['Strike']\n",
    "    call_ltp = safe_float(row.get('Call_LTP'))\n",
    "    put_ltp = safe_float(row.get('Put_LTP'))\n",
    "    \n",
    "    # Delta proxy (simplified)\n",
    "    # Call delta: 0 (deep OTM) to 1 (deep ITM)\n",
    "    # Put delta: -1 (deep ITM) to 0 (deep OTM)\n",
    "    moneyness = (cmp - strike) / cmp\n",
    "    \n",
    "    if call_ltp > 0:\n",
    "        # Approximate call delta using simple formula\n",
    "        call_delta = 0.5 + (moneyness * 2)  # Rough approximation\n",
    "        call_delta = max(0, min(1, call_delta))  # Bound between 0 and 1\n",
    "    else:\n",
    "        call_delta = 0\n",
    "    \n",
    "    if put_ltp > 0:\n",
    "        put_delta = -0.5 - (moneyness * 2)\n",
    "        put_delta = max(-1, min(0, put_delta))\n",
    "    else:\n",
    "        put_delta = 0\n",
    "    \n",
    "    # Gamma proxy (highest at ATM)\n",
    "    # Peaks at ATM, decays as you move away\n",
    "    distance_from_atm = abs(cmp - strike) / cmp\n",
    "    gamma = max(0, 0.1 * (1 - distance_from_atm * 10))  # Simplified\n",
    "    \n",
    "    # Theta proxy (time decay) - negative for option buyers\n",
    "    # Higher for ATM options\n",
    "    theta = -gamma * 0.5  # Simplified relationship\n",
    "    \n",
    "    # Vega proxy (IV sensitivity)\n",
    "    vega = gamma * 10  # Simplified\n",
    "    \n",
    "    return {\n",
    "        'call_delta': call_delta,\n",
    "        'put_delta': put_delta,\n",
    "        'gamma': gamma,\n",
    "        'theta': theta,\n",
    "        'vega': vega\n",
    "    }\n",
    "\n",
    "def calculate_oi_changes(symbol, expiry, current_df):\n",
    "    \"\"\"\n",
    "    Calculate OI changes from yesterday\n",
    "    This is CRITICAL for detecting fresh positioning\n",
    "    \"\"\"\n",
    "    hist_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_history.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    if not os.path.exists(hist_file):\n",
    "        # First day - no changes\n",
    "        current_df['Call_OI_Change'] = 0\n",
    "        current_df['Put_OI_Change'] = 0\n",
    "        current_df['Call_OI_Pct_Change'] = 0\n",
    "        current_df['Put_OI_Pct_Change'] = 0\n",
    "        return current_df\n",
    "    \n",
    "    # Load yesterday's data\n",
    "    hist_df = pd.read_csv(hist_file)\n",
    "    yesterday_data = hist_df[hist_df['Date'] == YESTERDAY]\n",
    "    \n",
    "    if yesterday_data.empty:\n",
    "        current_df['Call_OI_Change'] = 0\n",
    "        current_df['Put_OI_Change'] = 0\n",
    "        current_df['Call_OI_Pct_Change'] = 0\n",
    "        current_df['Put_OI_Pct_Change'] = 0\n",
    "        return current_df\n",
    "    \n",
    "    # Merge and calculate changes\n",
    "    merged = current_df.merge(\n",
    "        yesterday_data[['Strike', 'Call_OI', 'Put_OI']],\n",
    "        on='Strike',\n",
    "        how='left',\n",
    "        suffixes=('', '_prev')\n",
    "    )\n",
    "    \n",
    "    merged['Call_OI_Change'] = merged['Call_OI'] - merged['Call_OI_prev'].fillna(0)\n",
    "    merged['Put_OI_Change'] = merged['Put_OI'] - merged['Put_OI_prev'].fillna(0)\n",
    "    \n",
    "    merged['Call_OI_Pct_Change'] = (\n",
    "        (merged['Call_OI_Change'] / merged['Call_OI_prev'].replace(0, 1)) * 100\n",
    "    ).fillna(0)\n",
    "    \n",
    "    merged['Put_OI_Pct_Change'] = (\n",
    "        (merged['Put_OI_Change'] / merged['Put_OI_prev'].replace(0, 1)) * 100\n",
    "    ).fillna(0)\n",
    "    \n",
    "    return merged.drop(columns=['Call_OI_prev', 'Put_OI_prev'])\n",
    "\n",
    "def enrich_option_chain_data(symbol, expiry, is_index=False):\n",
    "    \"\"\"\n",
    "    Load option chain and enrich with:\n",
    "    1. Greeks (proxy)\n",
    "    2. OI changes\n",
    "    3. Volume/OI ratio\n",
    "    4. Skew metrics\n",
    "    \"\"\"\n",
    "    # Load raw data\n",
    "    df = load_option_chain(symbol, expiry, is_index)\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    \n",
    "    cmp = get_underlying_price(df)\n",
    "    if not cmp:\n",
    "        return None\n",
    "    \n",
    "    # Calculate OI changes\n",
    "    df = calculate_oi_changes(symbol, expiry, df)\n",
    "    \n",
    "    # Calculate Greeks for each strike\n",
    "    greeks_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        greeks = calculate_greeks_proxy(row, df, cmp)\n",
    "        greeks_list.append(greeks)\n",
    "    \n",
    "    greeks_df = pd.DataFrame(greeks_list)\n",
    "    df = pd.concat([df, greeks_df], axis=1)\n",
    "    \n",
    "    # Calculate advanced metrics\n",
    "    df['moneyness'] = (cmp - df['Strike']) / cmp\n",
    "    df['is_atm'] = (abs(df['Strike'] - cmp) / cmp) < 0.02  # Within 2% = ATM\n",
    "    \n",
    "    # Volume to OI ratio (if volume data available)\n",
    "    # Note: NSE doesn't give volume in API, so we skip this for now\n",
    "    \n",
    "    # Add timestamp\n",
    "    df['Date'] = TODAY\n",
    "    df['CMP'] = cmp\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_aggregate_metrics(df, symbol, expiry):\n",
    "    \"\"\"\n",
    "    Calculate portfolio-level metrics:\n",
    "    1. Total Greeks exposure\n",
    "    2. Net delta\n",
    "    3. Gamma exposure (GEX)\n",
    "    4. Put/Call OI ratio by Greeks\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Delta-weighted OI\n",
    "    df['call_delta_oi'] = df['Call_OI'] * df['call_delta']\n",
    "    df['put_delta_oi'] = df['Put_OI'] * abs(df['put_delta'])\n",
    "    \n",
    "    # Gamma exposure\n",
    "    df['gamma_exposure'] = (df['Call_OI'] - df['Put_OI']) * df['gamma'] * df['Strike']\n",
    "    \n",
    "    # ATM metrics (most important)\n",
    "    atm_df = df[df['is_atm']]\n",
    "    \n",
    "    metrics = {\n",
    "        'symbol': symbol,\n",
    "        'expiry': expiry,\n",
    "        'date': TODAY,\n",
    "        'total_call_oi': df['Call_OI'].sum(),\n",
    "        'total_put_oi': df['Put_OI'].sum(),\n",
    "        'pcr': df['Put_OI'].sum() / df['Call_OI'].sum() if df['Call_OI'].sum() > 0 else None,\n",
    "        \n",
    "        # OI Changes\n",
    "        'call_oi_change': df['Call_OI_Change'].sum(),\n",
    "        'put_oi_change': df['Put_OI_Change'].sum(),\n",
    "        'net_oi_change': df['Put_OI_Change'].sum() - df['Call_OI_Change'].sum(),\n",
    "        \n",
    "        # Greeks\n",
    "        'net_delta': df['call_delta_oi'].sum() + df['put_delta_oi'].sum(),\n",
    "        'total_gamma_exposure': df['gamma_exposure'].sum(),\n",
    "        'max_gamma_strike': df.loc[df['gamma'].idxmax(), 'Strike'] if not df.empty else None,\n",
    "        \n",
    "        # ATM metrics\n",
    "        'atm_call_oi': atm_df['Call_OI'].sum() if not atm_df.empty else 0,\n",
    "        'atm_put_oi': atm_df['Put_OI'].sum() if not atm_df.empty else 0,\n",
    "        'atm_pcr': (atm_df['Put_OI'].sum() / atm_df['Call_OI'].sum()) if not atm_df.empty and atm_df['Call_OI'].sum() > 0 else None,\n",
    "        \n",
    "        # IV metrics\n",
    "        'avg_call_iv': df['CE_IV'].mean() if 'CE_IV' in df.columns else None,\n",
    "        'avg_put_iv': df['PE_IV'].mean() if 'PE_IV' in df.columns else None,\n",
    "        'iv_skew': (df['PE_IV'].mean() - df['CE_IV'].mean()) if 'CE_IV' in df.columns else None,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def append_to_master_database(df, symbol, expiry):\n",
    "    \"\"\"\n",
    "    Append today's enriched data to master historical database\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    \n",
    "    master_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_history.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    if os.path.exists(master_file):\n",
    "        # Load existing and append\n",
    "        existing = pd.read_csv(master_file)\n",
    "        # Remove today's data if exists (in case of re-run)\n",
    "        existing = existing[existing['Date'] != TODAY]\n",
    "        combined = pd.concat([existing, df], ignore_index=True)\n",
    "        combined.to_csv(master_file, index=False)\n",
    "    else:\n",
    "        # First time\n",
    "        df.to_csv(master_file, index=False)\n",
    "    \n",
    "    print(f\"  âœ“ Appended {symbol} {expiry} to master DB\")\n",
    "\n",
    "def save_aggregate_metrics(metrics, symbol, expiry):\n",
    "    \"\"\"\n",
    "    Save daily aggregate metrics (PCR, Greeks, OI changes)\n",
    "    \"\"\"\n",
    "    if not metrics:\n",
    "        return\n",
    "    \n",
    "    agg_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_aggregates.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    \n",
    "    if os.path.exists(agg_file):\n",
    "        existing = pd.read_csv(agg_file)\n",
    "        # Remove today if exists\n",
    "        existing = existing[existing['date'] != TODAY]\n",
    "        combined = pd.concat([existing, metrics_df], ignore_index=True)\n",
    "        combined.to_csv(agg_file, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(agg_file, index=False)\n",
    "    \n",
    "    print(f\"  âœ“ Saved aggregate metrics for {symbol} {expiry}\")\n",
    "\n",
    "def detect_patterns(symbol, expiry):\n",
    "    \"\"\"\n",
    "    Detect trading patterns from historical data:\n",
    "    1. PCR reversals\n",
    "    2. OI buildup/unwinding\n",
    "    3. Max Pain movement\n",
    "    \"\"\"\n",
    "    agg_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_aggregates.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    if not os.path.exists(agg_file):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(agg_file)\n",
    "    if len(df) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Sort by date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Calculate changes\n",
    "    df['pcr_change'] = df['pcr'].diff()\n",
    "    df['oi_change_trend'] = df['net_oi_change'].diff()\n",
    "    \n",
    "    patterns = {\n",
    "        'symbol': symbol,\n",
    "        'expiry': expiry,\n",
    "        'date': TODAY,\n",
    "        'pcr_trend': 'rising' if df['pcr_change'].iloc[-1] > 0 else 'falling',\n",
    "        'pcr_velocity': df['pcr_change'].iloc[-1],\n",
    "        'oi_trend': 'buildup' if df['net_oi_change'].iloc[-1] > 0 else 'unwinding',\n",
    "        'days_to_expiry': (pd.to_datetime(expiry) - pd.to_datetime(TODAY)).days,\n",
    "    }\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def save_patterns(patterns):\n",
    "    \"\"\"Save detected patterns for ML training\"\"\"\n",
    "    if not patterns:\n",
    "        return\n",
    "    \n",
    "    patterns_file = os.path.join(PATTERNS_DB, f\"patterns_{TODAY}.csv\")\n",
    "    \n",
    "    patterns_df = pd.DataFrame([patterns])\n",
    "    \n",
    "    if os.path.exists(patterns_file):\n",
    "        existing = pd.read_csv(patterns_file)\n",
    "        combined = pd.concat([existing, patterns_df], ignore_index=True)\n",
    "        combined.to_csv(patterns_file, index=False)\n",
    "    else:\n",
    "        patterns_df.to_csv(patterns_file, index=False)\n",
    "\n",
    "# ==========================\n",
    "# MAIN COLLECTION PROCESS\n",
    "# ==========================\n",
    "def collect_and_store_daily_data():\n",
    "    \"\"\"\n",
    "    Main function: Collect today's data and append to historical DB\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”„ Starting daily data collection...\\n\")\n",
    "    \n",
    "    # Get list of symbols and expiries from today's run\n",
    "    equity_files = [f for f in os.listdir(BASE_DIR) if f.startswith('Equities_Analysis_')]\n",
    "    if not equity_files:\n",
    "        print(\"âŒ No data files found. Run Cell 1 first!\")\n",
    "        return\n",
    "    \n",
    "    expiries = [f.replace('Equities_Analysis_', '').replace('.csv', '') for f in equity_files]\n",
    "    \n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Process equities\n",
    "    for expiry in expiries:\n",
    "        print(f\"\\nğŸ“… Processing expiry: {expiry}\")\n",
    "        \n",
    "        summary = load_equity_analysis(expiry)\n",
    "        if summary is None:\n",
    "            continue\n",
    "        \n",
    "        for idx, row in summary.iterrows():\n",
    "            symbol = row['Symbol']\n",
    "            \n",
    "            try:\n",
    "                # Enrich option chain with Greeks, OI changes, etc.\n",
    "                enriched_df = enrich_option_chain_data(symbol, expiry, is_index=False)\n",
    "                \n",
    "                if enriched_df is not None:\n",
    "                    # Append to master database\n",
    "                    append_to_master_database(enriched_df, symbol, expiry)\n",
    "                    \n",
    "                    # Calculate and save aggregate metrics\n",
    "                    metrics = calculate_aggregate_metrics(enriched_df, symbol, expiry)\n",
    "                    save_aggregate_metrics(metrics, symbol, expiry)\n",
    "                    \n",
    "                    # Detect patterns\n",
    "                    patterns = detect_patterns(symbol, expiry)\n",
    "                    if patterns:\n",
    "                        save_patterns(patterns)\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                if error_count <= 5:  # Only show first 5 errors\n",
    "                    print(f\"  âš  Error processing {symbol}: {e}\")\n",
    "            \n",
    "            # Progress\n",
    "            if (idx + 1) % 20 == 0:\n",
    "                print(f\"  Progress: {idx + 1}/{len(summary)}\", end='\\r')\n",
    "    \n",
    "    # Process indices\n",
    "    print(f\"\\n\\nğŸ“Š Processing indices...\")\n",
    "    for index in ['NIFTY', 'BANKNIFTY']:\n",
    "        summary = load_index_analysis(index)\n",
    "        if summary is None:\n",
    "            continue\n",
    "        \n",
    "        for _, row in summary.head(3).iterrows():  # First 3 expiries\n",
    "            expiry = row['Expiry']\n",
    "            \n",
    "            try:\n",
    "                enriched_df = enrich_option_chain_data(index, expiry, is_index=True)\n",
    "                \n",
    "                if enriched_df is not None:\n",
    "                    append_to_master_database(enriched_df, index, expiry)\n",
    "                    metrics = calculate_aggregate_metrics(enriched_df, index, expiry)\n",
    "                    save_aggregate_metrics(metrics, index, expiry)\n",
    "                    patterns = detect_patterns(index, expiry)\n",
    "                    if patterns:\n",
    "                        save_patterns(patterns)\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                    print(f\"  âœ“ Processed {index} {expiry}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âš  Error processing {index}: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… DAILY COLLECTION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nğŸ“Š Summary:\")\n",
    "    print(f\"  â€¢ Successfully processed: {processed_count}\")\n",
    "    print(f\"  â€¢ Errors: {error_count}\")\n",
    "    print(f\"  â€¢ Database location: {HISTORICAL_DB}\")\n",
    "    \n",
    "    # Database statistics\n",
    "    master_files = [f for f in os.listdir(MASTER_DB) if f.endswith('_history.csv')]\n",
    "    print(f\"  â€¢ Total historical files: {len(master_files)}\")\n",
    "    \n",
    "    # Calculate total days of data\n",
    "    if master_files:\n",
    "        sample_file = os.path.join(MASTER_DB, master_files[0])\n",
    "        sample_df = pd.read_csv(sample_file)\n",
    "        unique_dates = sample_df['Date'].nunique()\n",
    "        print(f\"  â€¢ Days of historical data: {unique_dates}\")\n",
    "        \n",
    "        if unique_dates >= 5:\n",
    "            print(f\"\\nğŸ‰ You now have {unique_dates} days of data!\")\n",
    "            print(\"   Advanced analysis features will become available:\")\n",
    "            print(\"   â€¢ Historical PCR correlation\")\n",
    "            print(\"   â€¢ Pattern recognition\")\n",
    "            print(\"   â€¢ Backtesting capability\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ’¡ Keep collecting! You need at least 5 days for ML features.\")\n",
    "            print(f\"   Currently: {unique_dates}/5 days\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'processed': processed_count,\n",
    "        'errors': error_count,\n",
    "        'database': HISTORICAL_DB\n",
    "    }\n",
    "\n",
    "# ==========================\n",
    "# RUN COLLECTION\n",
    "# ==========================\n",
    "print(\"\\nâ° IMPORTANT: Run this cell EVERY DAY after market close!\")\n",
    "print(\"   Recommended time: After 4:00 PM IST\\n\")\n",
    "\n",
    "# Auto-run\n",
    "result = collect_and_store_daily_data()\n",
    "\n",
    "print(\"\\nğŸ’¾ Next Steps:\")\n",
    "print(\"1. Run this cell daily to build your database\")\n",
    "print(\"2. After 5+ days, run Cell 7 (Advanced Analysis)\")\n",
    "print(\"3. After 10+ days, run Cell 8 (ML Predictions)\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72f6fb0b-4b20-4ce7-818f-35fcf5fe7e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”¬ ADVANCED HISTORICAL ANALYSIS ENGINE\n",
      "======================================================================\n",
      "\n",
      "ğŸ“… Analysis Date: 2025-11-11\n",
      "\n",
      "ğŸ’¡ This cell analyzes your historical data to find what ACTUALLY works!\n",
      "\n",
      "â° Run this weekly to track performance over time.\n",
      "\n",
      "ğŸ“Š Data Status:\n",
      "   â€¢ Files: 612\n",
      "   â€¢ Days of data: 3\n",
      "\n",
      "ğŸ’¡ You have 3 days - basic analysis available\n",
      "   Collect 5+ days for full ML features!\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ RUNNING FULL ADVANCED ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ˆ PCR Correlation Study\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Analyzing: Does PCR predict next-day returns?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
      "C:\\Users\\sarda\\AppData\\Local\\Temp\\ipykernel_24240\\783482510.py:127: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Stocks by PCR Predictive Power:\n",
      "    Symbol      Expiry  Correlation Strength\n",
      "ADANIENSOL 25-Nov-2025          1.0   Strong\n",
      "  ADANIENT 25-Nov-2025         -1.0   Strong\n",
      "    360ONE 30-Dec-2025          1.0   Strong\n",
      "       ABB 25-Nov-2025          1.0   Strong\n",
      " ABCAPITAL 30-Dec-2025         -1.0   Strong\n",
      "ADANIENSOL 30-Dec-2025         -1.0   Strong\n",
      "  ADANIENT 30-Dec-2025         -1.0   Strong\n",
      "ADANIGREEN 25-Nov-2025         -1.0   Strong\n",
      "ADANIGREEN 30-Dec-2025         -1.0   Strong\n",
      "ADANIPORTS 25-Nov-2025         -1.0   Strong\n",
      "\n",
      "ğŸ’¡ Interpretation:\n",
      "  â€¢ Negative correlation: High PCR â†’ Price goes UP (contrarian works!)\n",
      "  â€¢ Positive correlation: High PCR â†’ Price goes DOWN (sentiment follows)\n",
      "  â€¢ |Correlation| > 0.5: Strong predictive power\n",
      "\n",
      "ğŸ’¾ Saved to: pcr_correlation_2025-11-11.csv\n",
      "\n",
      "ğŸ¯ Max Pain Accuracy Study\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Analyzing: Does price converge to Max Pain?\n",
      "\n",
      "Top 10 Stocks by Max Pain Accuracy (Lowest Distance):\n",
      "    Symbol      Expiry  Avg_Distance_% Convergence  Convergence_Score\n",
      "       ABB 30-Dec-2025        0.270629          No          -0.650306\n",
      "    360ONE 30-Dec-2025        0.595521         Yes           0.962422\n",
      "ADANIPORTS 25-Nov-2025        0.615928          No          -0.503679\n",
      "ADANIENSOL 25-Nov-2025        1.258999          No          -0.762686\n",
      "  ADANIENT 30-Dec-2025        1.266112          No          -0.468046\n",
      "ADANIPORTS 27-Jan-2026        1.679925         Yes           0.778373\n",
      "    360ONE 25-Nov-2025        2.395588         Yes           0.986261\n",
      " ABCAPITAL 25-Nov-2025        2.520143         Yes           0.995179\n",
      "ADANIENSOL 30-Dec-2025        2.927623         Yes           0.724956\n",
      "ADANIGREEN 25-Nov-2025        3.073739          No          -0.554567\n",
      "\n",
      "ğŸ’¡ Interpretation:\n",
      "  â€¢ Lower Avg Distance = More accurate Max Pain\n",
      "  â€¢ Convergence 'Yes' = Price moves toward Max Pain as expiry nears\n",
      "  â€¢ Use Max Pain more confidently for these stocks!\n",
      "\n",
      "ğŸ’¾ Saved to: max_pain_accuracy_2025-11-11.csv\n",
      "\n",
      "ğŸ”„ OI Change Pattern Analysis\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Detecting: Fresh positioning vs. unwinding\n",
      "\n",
      "Top 10 Stocks with Significant OI Changes:\n",
      "    Symbol  Signal  Net_OI_Change  Call_Buildup  Put_Buildup\n",
      "  ADANIENT NEUTRAL         1385.0        2750.0       4135.0\n",
      "ADANIENSOL NEUTRAL         -663.0        1245.0        582.0\n",
      "       ABB NEUTRAL         -610.0         701.0         91.0\n",
      "ADANIGREEN NEUTRAL         -557.0         832.0        275.0\n",
      "  ANGELONE NEUTRAL          295.0         130.0        425.0\n",
      " AMBUJACEM NEUTRAL         -261.0         370.0        109.0\n",
      " ABCAPITAL NEUTRAL         -233.0         304.0         71.0\n",
      "     AMBER NEUTRAL          219.0         183.0        402.0\n",
      "    360ONE NEUTRAL         -211.0         291.0         80.0\n",
      "ADANIPORTS NEUTRAL          156.0           7.0        163.0\n",
      "\n",
      "ğŸ’¡ Interpretation:\n",
      "  â€¢ Put Buildup (Positive) = Sellers expect support = BULLISH\n",
      "  â€¢ Call Buildup (Negative) = Sellers expect resistance = BEARISH\n",
      "  â€¢ Large changes indicate strong institutional positioning\n",
      "\n",
      "ğŸ’¾ Saved to: oi_patterns_2025-11-11.csv\n",
      "\n",
      "ğŸ¯ Signal Win Rate Analysis\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Backtesting: How accurate were our signals?\n",
      "\n",
      "Top 10 Best Performing Signals:\n",
      "    Symbol  Signals  Win_Rate_%  Bullish_WR_%  Bearish_WR_%  High_Conf_WR_%\n",
      " ABCAPITAL        3   66.666667           0.0     50.000000        0.000000\n",
      "ADANIENSOL        3   66.666667         100.0     33.333333        0.000000\n",
      "APOLLOHOSP        3   66.666667           0.0     50.000000        0.000000\n",
      " APLAPOLLO        3   66.666667           0.0     50.000000       50.000000\n",
      "  ANGELONE        3   33.333333           0.0     25.000000       33.333333\n",
      " AMBUJACEM        3   33.333333          25.0      0.000000        0.000000\n",
      "  AXISBANK        3   33.333333           0.0     25.000000        0.000000\n",
      "    ASTRAL        3   33.333333           0.0     25.000000       25.000000\n",
      "       ABB        3   33.333333           0.0     50.000000        0.000000\n",
      "BAJAJ-AUTO        3   33.333333         100.0      0.000000        0.000000\n",
      "\n",
      "ğŸ’¡ Key Insights:\n",
      "  â€¢ Average Win Rate: 28.3%\n",
      "  â€¢ High Confidence Win Rate: 7.9%\n",
      "  â€¢ Total Signals Tracked: 60\n",
      "\n",
      "âš ï¸  Signals need improvement (28.3% win rate)\n",
      "\n",
      "ğŸ’¾ Saved to: win_rates_2025-11-11.csv\n",
      "\n",
      "\n",
      "======================================================================\n",
      "âœ… ADVANCED ANALYSIS COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‚ All reports saved to: C:\\Users\\sarda\\Desktop\\nse_options_historical_db\\advanced_analysis\n",
      "\n",
      "ğŸ¯ Next Steps:\n",
      "1. Review the CSV files in advanced_analysis folder\n",
      "2. Focus on stocks with:\n",
      "   â€¢ High PCR correlation (predictive)\n",
      "   â€¢ Low Max Pain distance (accurate)\n",
      "   â€¢ High win rates (profitable)\n",
      "3. After 10+ days, run Cell 8 for ML predictions\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CELL 7: Advanced Historical Analysis Engine\n",
    "\n",
    "Prerequisites: Need at least 5 days of data (run Cell 6 daily)\n",
    "\n",
    "Features:\n",
    "1. Historical PCR correlation with price movements\n",
    "2. Max Pain accuracy tracker\n",
    "3. OI change pattern analysis\n",
    "4. Greeks-based positioning\n",
    "5. Win rate calculator for signals\n",
    "6. Best time-to-expiry analysis\n",
    "\n",
    "This shows you WHAT ACTUALLY WORKS!\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# ==========================\n",
    "# CONFIGURATION\n",
    "# ==========================\n",
    "HISTORICAL_DB = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"nse_options_historical_db\")\n",
    "MASTER_DB = os.path.join(HISTORICAL_DB, \"master_database\")\n",
    "ANALYSIS_OUTPUT = os.path.join(HISTORICAL_DB, \"advanced_analysis\")\n",
    "\n",
    "os.makedirs(ANALYSIS_OUTPUT, exist_ok=True)\n",
    "\n",
    "TODAY = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”¬ ADVANCED HISTORICAL ANALYSIS ENGINE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nğŸ“… Analysis Date: {TODAY}\\n\")\n",
    "\n",
    "# ==========================\n",
    "# DATA VALIDATION\n",
    "# ==========================\n",
    "def check_data_availability():\n",
    "    \"\"\"Check if we have enough historical data\"\"\"\n",
    "    if not os.path.exists(MASTER_DB):\n",
    "        print(\"âŒ No historical database found!\")\n",
    "        print(\"   Please run Cell 6 to start collecting data.\\n\")\n",
    "        return False\n",
    "    \n",
    "    agg_files = [f for f in os.listdir(MASTER_DB) if f.endswith('_aggregates.csv')]\n",
    "    \n",
    "    if not agg_files:\n",
    "        print(\"âŒ No aggregate data found!\")\n",
    "        print(\"   Please run Cell 6 first.\\n\")\n",
    "        return False\n",
    "    \n",
    "    # Check number of days\n",
    "    sample_file = os.path.join(MASTER_DB, agg_files[0])\n",
    "    df = pd.read_csv(sample_file)\n",
    "    num_days = df['date'].nunique()\n",
    "    \n",
    "    print(f\"ğŸ“Š Data Status:\")\n",
    "    print(f\"   â€¢ Files: {len(agg_files)}\")\n",
    "    print(f\"   â€¢ Days of data: {num_days}\\n\")\n",
    "    \n",
    "    if num_days < 2:\n",
    "        print(\"âš ï¸  Need at least 2 days for basic analysis\")\n",
    "        print(f\"   Current: {num_days}/2 days\\n\")\n",
    "        return False\n",
    "    \n",
    "    if num_days < 5:\n",
    "        print(f\"ğŸ’¡ You have {num_days} days - basic analysis available\")\n",
    "        print(\"   Collect 5+ days for full ML features!\\n\")\n",
    "    else:\n",
    "        print(f\"ğŸ‰ You have {num_days} days - full analysis available!\\n\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ==========================\n",
    "# 1. PCR CORRELATION ANALYSIS\n",
    "# ==========================\n",
    "def analyze_pcr_correlation(symbol, expiry):\n",
    "    \"\"\"\n",
    "    Analyze correlation between PCR and next-day price movement\n",
    "    \n",
    "    This tells you: Does high PCR actually lead to bullish moves?\n",
    "    \"\"\"\n",
    "    agg_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_aggregates.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    if not os.path.exists(agg_file):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(agg_file)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    if len(df) < 3:\n",
    "        return None\n",
    "    \n",
    "    # Get CMP from detailed data\n",
    "    hist_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_history.csv\".replace(\"/\", \"-\"))\n",
    "    if os.path.exists(hist_file):\n",
    "        hist_df = pd.read_csv(hist_file)\n",
    "        # Get daily CMP (first strike's CMP since it's same for all)\n",
    "        daily_cmp = hist_df.groupby('Date')['CMP'].first().reset_index()\n",
    "        daily_cmp.columns = ['date', 'cmp']\n",
    "        daily_cmp['date'] = pd.to_datetime(daily_cmp['date'])\n",
    "        \n",
    "        # Merge with aggregates\n",
    "        df = df.merge(daily_cmp, on='date', how='left')\n",
    "        \n",
    "        # Calculate next-day price change\n",
    "        df['next_day_return'] = df['cmp'].shift(-1) / df['cmp'] - 1\n",
    "        df['next_day_return_pct'] = df['next_day_return'] * 100\n",
    "        \n",
    "        # Correlation\n",
    "        correlation = df[['pcr', 'next_day_return_pct']].corr().iloc[0, 1]\n",
    "        \n",
    "        # PCR categories\n",
    "        df['pcr_category'] = pd.cut(df['pcr'], \n",
    "                                      bins=[0, 0.7, 0.9, 1.1, 1.5, 10],\n",
    "                                      labels=['Extreme Bullish', 'Bullish', 'Neutral', 'Bearish', 'Extreme Bearish'])\n",
    "        \n",
    "        # Average return by PCR category\n",
    "        category_returns = df.groupby('pcr_category')['next_day_return_pct'].agg(['mean', 'count', 'std'])\n",
    "        \n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'expiry': expiry,\n",
    "            'correlation': correlation,\n",
    "            'category_returns': category_returns,\n",
    "            'data': df\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def run_pcr_correlation_study():\n",
    "    \"\"\"\n",
    "    Run PCR correlation for all symbols\n",
    "    Find which stocks' PCR is most predictive\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“ˆ PCR Correlation Study\")\n",
    "    print(\"â”€\" * 70)\n",
    "    print(\"Analyzing: Does PCR predict next-day returns?\\n\")\n",
    "    \n",
    "    agg_files = [f for f in os.listdir(MASTER_DB) if f.endswith('_aggregates.csv')]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file in agg_files[:20]:  # Analyze top 20\n",
    "        parts = file.replace('_aggregates.csv', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            symbol = parts[0]\n",
    "            expiry = '_'.join(parts[1:])\n",
    "            \n",
    "            result = analyze_pcr_correlation(symbol, expiry)\n",
    "            if result and not pd.isna(result['correlation']):\n",
    "                results.append({\n",
    "                    'Symbol': symbol,\n",
    "                    'Expiry': expiry,\n",
    "                    'Correlation': result['correlation'],\n",
    "                    'Strength': 'Strong' if abs(result['correlation']) > 0.5 else 'Moderate' if abs(result['correlation']) > 0.3 else 'Weak'\n",
    "                })\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âš ï¸  Not enough data yet. Need at least 3 days.\\n\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('Correlation', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Stocks by PCR Predictive Power:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Interpretation:\")\n",
    "    print(\"  â€¢ Negative correlation: High PCR â†’ Price goes UP (contrarian works!)\")\n",
    "    print(\"  â€¢ Positive correlation: High PCR â†’ Price goes DOWN (sentiment follows)\")\n",
    "    print(\"  â€¢ |Correlation| > 0.5: Strong predictive power\")\n",
    "    \n",
    "    # Save\n",
    "    results_df.to_csv(os.path.join(ANALYSIS_OUTPUT, f\"pcr_correlation_{TODAY}.csv\"), index=False)\n",
    "    print(f\"\\nğŸ’¾ Saved to: pcr_correlation_{TODAY}.csv\\n\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ==========================\n",
    "# 2. MAX PAIN ACCURACY TRACKER\n",
    "# ==========================\n",
    "def analyze_max_pain_accuracy(symbol, expiry):\n",
    "    \"\"\"\n",
    "    Check: How often does price actually gravitate toward Max Pain?\n",
    "    \n",
    "    Tracks:\n",
    "    1. Daily distance from Max Pain\n",
    "    2. Convergence as expiry approaches\n",
    "    3. Accuracy on expiry day\n",
    "    \"\"\"\n",
    "    agg_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_aggregates.csv\".replace(\"/\", \"-\"))\n",
    "    hist_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_history.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    if not os.path.exists(agg_file) or not os.path.exists(hist_file):\n",
    "        return None\n",
    "    \n",
    "    # Load data\n",
    "    hist_df = pd.read_csv(hist_file)\n",
    "    hist_df['Date'] = pd.to_datetime(hist_df['Date'])\n",
    "    \n",
    "    # Get CMP and Max Pain per day\n",
    "    daily_data = []\n",
    "    for date in hist_df['Date'].unique():\n",
    "        day_data = hist_df[hist_df['Date'] == date]\n",
    "        cmp = day_data['CMP'].iloc[0]\n",
    "        \n",
    "        # Calculate max pain for that day\n",
    "        strikes = day_data['Strike'].values\n",
    "        call_oi = day_data['Call_OI'].values\n",
    "        put_oi = day_data['Put_OI'].values\n",
    "        \n",
    "        total_losses = []\n",
    "        for expiry_strike in strikes:\n",
    "            call_loss = ((expiry_strike - strikes).clip(min=0) * call_oi).sum()\n",
    "            put_loss = ((strikes - expiry_strike).clip(min=0) * put_oi).sum()\n",
    "            total_losses.append(call_loss + put_loss)\n",
    "        \n",
    "        if total_losses:\n",
    "            max_pain = int(strikes[total_losses.index(min(total_losses))])\n",
    "            distance_pct = ((max_pain - cmp) / cmp) * 100\n",
    "            \n",
    "            daily_data.append({\n",
    "                'date': date,\n",
    "                'cmp': cmp,\n",
    "                'max_pain': max_pain,\n",
    "                'distance_pct': distance_pct,\n",
    "                'distance_abs': abs(distance_pct)\n",
    "            })\n",
    "    \n",
    "    if not daily_data:\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(daily_data)\n",
    "    df = df.sort_values('date')\n",
    "    \n",
    "    # Calculate days to expiry\n",
    "    expiry_date = pd.to_datetime(expiry)\n",
    "    df['days_to_expiry'] = (expiry_date - df['date']).dt.days\n",
    "    \n",
    "    # Convergence analysis\n",
    "    if len(df) >= 3:\n",
    "        # Does distance decrease as expiry approaches?\n",
    "        convergence_corr = df[['days_to_expiry', 'distance_abs']].corr().iloc[0, 1]\n",
    "        \n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'expiry': expiry,\n",
    "            'avg_distance': df['distance_abs'].mean(),\n",
    "            'final_distance': df['distance_abs'].iloc[-1] if len(df) > 0 else None,\n",
    "            'convergence_correlation': convergence_corr,\n",
    "            'data': df\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def run_max_pain_accuracy_study():\n",
    "    \"\"\"\n",
    "    Check Max Pain accuracy across all symbols\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ Max Pain Accuracy Study\")\n",
    "    print(\"â”€\" * 70)\n",
    "    print(\"Analyzing: Does price converge to Max Pain?\\n\")\n",
    "    \n",
    "    agg_files = [f for f in os.listdir(MASTER_DB) if f.endswith('_aggregates.csv')]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file in agg_files[:20]:\n",
    "        parts = file.replace('_aggregates.csv', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            symbol = parts[0]\n",
    "            expiry = '_'.join(parts[1:])\n",
    "            \n",
    "            result = analyze_max_pain_accuracy(symbol, expiry)\n",
    "            if result:\n",
    "                results.append({\n",
    "                    'Symbol': symbol,\n",
    "                    'Expiry': expiry,\n",
    "                    'Avg_Distance_%': result['avg_distance'],\n",
    "                    'Convergence': 'Yes' if result['convergence_correlation'] > 0.3 else 'No',\n",
    "                    'Convergence_Score': result['convergence_correlation']\n",
    "                })\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âš ï¸  Not enough data yet.\\n\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('Avg_Distance_%')\n",
    "    \n",
    "    print(\"Top 10 Stocks by Max Pain Accuracy (Lowest Distance):\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Interpretation:\")\n",
    "    print(\"  â€¢ Lower Avg Distance = More accurate Max Pain\")\n",
    "    print(\"  â€¢ Convergence 'Yes' = Price moves toward Max Pain as expiry nears\")\n",
    "    print(\"  â€¢ Use Max Pain more confidently for these stocks!\")\n",
    "    \n",
    "    # Save\n",
    "    results_df.to_csv(os.path.join(ANALYSIS_OUTPUT, f\"max_pain_accuracy_{TODAY}.csv\"), index=False)\n",
    "    print(f\"\\nğŸ’¾ Saved to: max_pain_accuracy_{TODAY}.csv\\n\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ==========================\n",
    "# 3. OI CHANGE PATTERNS\n",
    "# ==========================\n",
    "def analyze_oi_change_patterns(symbol, expiry):\n",
    "    \"\"\"\n",
    "    Analyze OI buildup vs unwinding patterns\n",
    "    \n",
    "    Fresh OI buildup = New positions (bullish if puts, bearish if calls)\n",
    "    OI unwinding = Closing positions (reversal signal)\n",
    "    \"\"\"\n",
    "    hist_file = os.path.join(MASTER_DB, f\"{symbol}_{expiry}_history.csv\".replace(\"/\", \"-\"))\n",
    "    \n",
    "    if not os.path.exists(hist_file):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(hist_file)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Get latest 2 days\n",
    "    dates = sorted(df['Date'].unique())\n",
    "    if len(dates) < 2:\n",
    "        return None\n",
    "    \n",
    "    latest = dates[-1]\n",
    "    previous = dates[-2]\n",
    "    \n",
    "    latest_data = df[df['Date'] == latest]\n",
    "    \n",
    "    # Identify major OI changes\n",
    "    significant_call_build = latest_data[latest_data['Call_OI_Change'] > latest_data['Call_OI'] * 0.1]\n",
    "    significant_put_build = latest_data[latest_data['Put_OI_Change'] > latest_data['Put_OI'] * 0.1]\n",
    "    \n",
    "    significant_call_unwind = latest_data[latest_data['Call_OI_Change'] < -latest_data['Call_OI'] * 0.1]\n",
    "    significant_put_unwind = latest_data[latest_data['Put_OI_Change'] < -latest_data['Put_OI'] * 0.1]\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'expiry': expiry,\n",
    "        'date': latest,\n",
    "        'call_buildup_strikes': significant_call_build['Strike'].tolist(),\n",
    "        'put_buildup_strikes': significant_put_build['Strike'].tolist(),\n",
    "        'call_unwind_strikes': significant_call_unwind['Strike'].tolist(),\n",
    "        'put_unwind_strikes': significant_put_unwind['Strike'].tolist(),\n",
    "        'total_call_buildup': significant_call_build['Call_OI_Change'].sum(),\n",
    "        'total_put_buildup': significant_put_build['Put_OI_Change'].sum(),\n",
    "    }\n",
    "\n",
    "def run_oi_pattern_study():\n",
    "    \"\"\"\n",
    "    Identify stocks with significant OI changes\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ OI Change Pattern Analysis\")\n",
    "    print(\"â”€\" * 70)\n",
    "    print(\"Detecting: Fresh positioning vs. unwinding\\n\")\n",
    "    \n",
    "    agg_files = [f for f in os.listdir(MASTER_DB) if f.endswith('_aggregates.csv')]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file in agg_files[:30]:\n",
    "        parts = file.replace('_aggregates.csv', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            symbol = parts[0]\n",
    "            expiry = '_'.join(parts[1:])\n",
    "            \n",
    "            result = analyze_oi_change_patterns(symbol, expiry)\n",
    "            if result:\n",
    "                # Determine signal\n",
    "                net_buildup = result['total_put_buildup'] - result['total_call_buildup']\n",
    "                \n",
    "                if net_buildup > 10000:\n",
    "                    signal = \"BULLISH (Put buildup)\"\n",
    "                elif net_buildup < -10000:\n",
    "                    signal = \"BEARISH (Call buildup)\"\n",
    "                else:\n",
    "                    signal = \"NEUTRAL\"\n",
    "                \n",
    "                results.append({\n",
    "                    'Symbol': symbol,\n",
    "                    'Signal': signal,\n",
    "                    'Net_OI_Change': net_buildup,\n",
    "                    'Call_Buildup': result['total_call_buildup'],\n",
    "                    'Put_Buildup': result['total_put_buildup']\n",
    "                })\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âš ï¸  Not enough data yet.\\n\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('Net_OI_Change', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Stocks with Significant OI Changes:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Interpretation:\")\n",
    "    print(\"  â€¢ Put Buildup (Positive) = Sellers expect support = BULLISH\")\n",
    "    print(\"  â€¢ Call Buildup (Negative) = Sellers expect resistance = BEARISH\")\n",
    "    print(\"  â€¢ Large changes indicate strong institutional positioning\")\n",
    "    \n",
    "    results_df.to_csv(os.path.join(ANALYSIS_OUTPUT, f\"oi_patterns_{TODAY}.csv\"), index=False)\n",
    "    print(f\"\\nğŸ’¾ Saved to: oi_patterns_{TODAY}.csv\\n\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ==========================\n",
    "# 4. SIGNAL WIN RATE CALCULATOR\n",
    "# ==========================\n",
    "def calculate_signal_win_rate(symbol, expiry):\n",
    "    \"\"\"\n",
    "    Backtest: When we gave a BULLISH/BEARISH signal, did it work?\n",
    "    \n",
    "    This is your MONEY METRIC!\n",
    "    \"\"\"\n",
    "    # Load historical signals (from Cell 2's historical tracking)\n",
    "    hist_signal_file = os.path.join(OUTPUT_DIR, \"historical_tracking\", f\"{symbol}_history.csv\")\n",
    "    \n",
    "    if not os.path.exists(hist_signal_file):\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(hist_signal_file)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Expiry'] == expiry].sort_values('Date')\n",
    "    \n",
    "    if len(df) < 3:\n",
    "        return None\n",
    "    \n",
    "    # Calculate actual next-day returns\n",
    "    df['next_day_cmp'] = df['CMP'].shift(-1)\n",
    "    df['actual_return'] = (df['next_day_cmp'] / df['CMP'] - 1) * 100\n",
    "    \n",
    "    # Check if signal was correct\n",
    "    df['signal_correct'] = False\n",
    "    df.loc[(df['Final_Signal'].str.contains('BULLISH')) & (df['actual_return'] > 0), 'signal_correct'] = True\n",
    "    df.loc[(df['Final_Signal'].str.contains('BEARISH')) & (df['actual_return'] < 0), 'signal_correct'] = True\n",
    "    \n",
    "    # Win rate by signal type\n",
    "    bullish_trades = df[df['Final_Signal'].str.contains('BULLISH')]\n",
    "    bearish_trades = df[df['Final_Signal'].str.contains('BEARISH')]\n",
    "    \n",
    "    bullish_win_rate = (bullish_trades['signal_correct'].sum() / len(bullish_trades) * 100) if len(bullish_trades) > 0 else 0\n",
    "    bearish_win_rate = (bearish_trades['signal_correct'].sum() / len(bearish_trades) * 100) if len(bearish_trades) > 0 else 0\n",
    "    \n",
    "    # Win rate by confidence\n",
    "    high_conf = df[df['Confidence_pct'] > 60]\n",
    "    low_conf = df[df['Confidence_pct'] <= 60]\n",
    "    \n",
    "    high_conf_win_rate = (high_conf['signal_correct'].sum() / len(high_conf) * 100) if len(high_conf) > 0 else 0\n",
    "    low_conf_win_rate = (low_conf['signal_correct'].sum() / len(low_conf) * 100) if len(low_conf) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'expiry': expiry,\n",
    "        'total_signals': len(df) - 1,  # Exclude last day (no next day yet)\n",
    "        'bullish_win_rate': bullish_win_rate,\n",
    "        'bearish_win_rate': bearish_win_rate,\n",
    "        'high_conf_win_rate': high_conf_win_rate,\n",
    "        'low_conf_win_rate': low_conf_win_rate,\n",
    "        'overall_win_rate': (df['signal_correct'].sum() / (len(df) - 1) * 100) if len(df) > 1 else 0\n",
    "    }\n",
    "\n",
    "def run_win_rate_study():\n",
    "    \"\"\"\n",
    "    Calculate win rates for all tracked symbols\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ Signal Win Rate Analysis\")\n",
    "    print(\"â”€\" * 70)\n",
    "    print(\"Backtesting: How accurate were our signals?\\n\")\n",
    "    \n",
    "    hist_files = []\n",
    "    hist_dir = os.path.join(OUTPUT_DIR, \"historical_tracking\")\n",
    "    \n",
    "    if os.path.exists(hist_dir):\n",
    "        hist_files = [f for f in os.listdir(hist_dir) if f.endswith('_history.csv')]\n",
    "    \n",
    "    if not hist_files:\n",
    "        print(\"âš ï¸  No historical signals yet.\")\n",
    "        print(\"   Signals are tracked when you run Cell 3-5.\")\n",
    "        print(\"   Run analysis for a few days to see win rates!\\n\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for file in hist_files[:20]:\n",
    "        symbol = file.replace('_history.csv', '')\n",
    "        \n",
    "        # Get expiries for this symbol\n",
    "        hist_df = pd.read_csv(os.path.join(hist_dir, file))\n",
    "        expiries = hist_df['Expiry'].unique()\n",
    "        \n",
    "        for expiry in expiries:\n",
    "            result = calculate_signal_win_rate(symbol, expiry)\n",
    "            if result and result['total_signals'] >= 2:\n",
    "                results.append({\n",
    "                    'Symbol': symbol,\n",
    "                    'Signals': result['total_signals'],\n",
    "                    'Win_Rate_%': result['overall_win_rate'],\n",
    "                    'Bullish_WR_%': result['bullish_win_rate'],\n",
    "                    'Bearish_WR_%': result['bearish_win_rate'],\n",
    "                    'High_Conf_WR_%': result['high_conf_win_rate']\n",
    "                })\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âš ï¸  Need at least 3 days of signals.\\n\")\n",
    "        return None\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('Win_Rate_%', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Best Performing Signals:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Key Insights:\")\n",
    "    avg_wr = results_df['Win_Rate_%'].mean()\n",
    "    print(f\"  â€¢ Average Win Rate: {avg_wr:.1f}%\")\n",
    "    print(f\"  â€¢ High Confidence Win Rate: {results_df['High_Conf_WR_%'].mean():.1f}%\")\n",
    "    print(f\"  â€¢ Total Signals Tracked: {results_df['Signals'].sum()}\")\n",
    "    \n",
    "    if avg_wr > 55:\n",
    "        print(f\"\\nğŸ‰ Your signals are profitable! (>{avg_wr:.1f}% win rate)\")\n",
    "    elif avg_wr > 50:\n",
    "        print(f\"\\nğŸ‘ Your signals are decent ({avg_wr:.1f}% win rate)\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Signals need improvement ({avg_wr:.1f}% win rate)\")\n",
    "    \n",
    "    results_df.to_csv(os.path.join(ANALYSIS_OUTPUT, f\"win_rates_{TODAY}.csv\"), index=False)\n",
    "    print(f\"\\nğŸ’¾ Saved to: win_rates_{TODAY}.csv\\n\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ==========================\n",
    "# MASTER ANALYSIS RUNNER\n",
    "# ==========================\n",
    "def run_full_advanced_analysis():\n",
    "    \"\"\"\n",
    "    Run all advanced analyses\n",
    "    \"\"\"\n",
    "    if not check_data_availability():\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸš€ RUNNING FULL ADVANCED ANALYSIS\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # 1. PCR Correlation\n",
    "    pcr_results = run_pcr_correlation_study()\n",
    "    \n",
    "    # 2. Max Pain Accuracy\n",
    "    max_pain_results = run_max_pain_accuracy_study()\n",
    "    \n",
    "    # 3. OI Patterns\n",
    "    oi_results = run_oi_pattern_study()\n",
    "    \n",
    "    # 4. Win Rates\n",
    "    win_rate_results = run_win_rate_study()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… ADVANCED ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nğŸ“‚ All reports saved to: {ANALYSIS_OUTPUT}\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ Next Steps:\")\n",
    "    print(\"1. Review the CSV files in advanced_analysis folder\")\n",
    "    print(\"2. Focus on stocks with:\")\n",
    "    print(\"   â€¢ High PCR correlation (predictive)\")\n",
    "    print(\"   â€¢ Low Max Pain distance (accurate)\")\n",
    "    print(\"   â€¢ High win rates (profitable)\")\n",
    "    print(\"3. After 10+ days, run Cell 8 for ML predictions\")\n",
    "    \n",
    "    return {\n",
    "        'pcr': pcr_results,\n",
    "        'max_pain': max_pain_results,\n",
    "        'oi': oi_results,\n",
    "        'win_rates': win_rate_results\n",
    "    }\n",
    "\n",
    "# ==========================\n",
    "# RUN ANALYSIS\n",
    "# ==========================\n",
    "print(\"ğŸ’¡ This cell analyzes your historical data to find what ACTUALLY works!\\n\")\n",
    "print(\"â° Run this weekly to track performance over time.\\n\")\n",
    "\n",
    "analysis_results = run_full_advanced_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa35169-580e-4d16-9f6a-6db775462147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
